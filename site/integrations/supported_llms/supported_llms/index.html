<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://cerebralblue.github.io/ns-doc-html/integrations/supported_llms/supported_llms/">
    <link rel="shortcut icon" href="../../../images/ns-white.svg">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&amp;display=swap" rel="stylesheet">
    <title>Supported LLMs - NeuralSeek Documentation</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/colors.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight-github-theme.css">
    <link rel="stylesheet" href="../../../css/highlightjs-copy.css">
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/override.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.min.js"></script>
    <script src="../../../js/highlightjs-copy.min.js"></script>
    
    <base target="_top">
    <script>
      hljs.addPlugin(new CopyButtonPlugin());
      hljs.configure({languages:[]});
      hljs.highlightAll();
    </script>
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Overview", url: "#overview", children: [
          ]},
          {title: "Configuring an LLM", url: "#configuring-an-llm", children: [
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LE5XX6X6Z7"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'G-LE5XX6X6Z7');
      </script> 
 <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    <h1 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link"></a></h1>
<p>NeuralSeek supports the following Large Language Models (LLMs):</p>
<ul>
<li><a href="#Amazon Bedrock - Claude Instant v1.1">Amazon Bedrock - Claude Instant v1.1</a></li>
<li><a href="#Amazon Bedrock - Claude v1.3">Amazon Bedrock - Claude v1.3</a></li>
<li><a href="#Amazon Bedrock - Claude v2">Amazon Bedrock - Claude v2</a></li>
<li><a href="#Amazon Bedrock - Jurassic-2 Mid">Amazon Bedrock - Jurassic-2 Mid</a></li>
<li><a href="#Amazon Bedrock - Jurassic-2 Ultra">Amazon Bedrock - Jurassic-2 Ultra</a></li>
<li><a href="#Amazon Bedrock - Llama-2-chat 13B">Amazon Bedrock - Llama-2-chat 13B</a></li>
<li><a href="#Amazon Bedrock - Titan">Amazon Bedrock - Titan</a></li>
<li><a href="#Azure Cognitive Services - GPT3.5">Azure Cognitive Services - GPT3.5</a></li>
<li><a href="#Azure Cognitive Services - GPT4">Azure Cognitive Services - GPT4</a></li>
<li><a href="#Azure Cognitive Services - GPT4 (32K)">Azure Cognitive Services - GPT4 (32K)</a></li>
<li><a href="#HuggingFace - Flan-t5-xxl">HuggingFace - Flan-t5-xxl</a></li>
<li><a href="#HuggingFace - Flan-ul2">HuggingFace - Flan-ul2</a></li>
<li><a href="#HuggingFace - Llama-2">HuggingFace - Llama-2</a></li>
<li><a href="#HuggingFace - Llama-2-chat">HuggingFace - Llama-2-chat</a></li>
<li><a href="#HuggingFace - Mistral-7B-Instruct">HuggingFace - Mistral-7B-Instruct</a></li>
<li><a href="#HuggingFace - Mixtral-8x7B-Instruct">HuggingFace - Mixtral-8x7B-Instruct</a></li>
<li><a href="#HuggingFace - MPT-7B-instruct">HuggingFace - MPT-7B-instruct</a></li>
<li><a href="#OpenAI - GPT3.5">OpenAI - GPT3.5</a></li>
<li><a href="#OpenAI - GPT3.5 (16K)">OpenAI - GPT3.5 (16K)</a></li>
<li><a href="#OpenAI - GPT4">OpenAI - GPT4</a></li>
<li><a href="#OpenAI - GPT4 (32K)">OpenAI - GPT4 (32K)</a></li>
<li><a href="#OpenAI - GPT4 Turbo (Preview)">OpenAI - GPT4 Turbo (Preview)</a></li>
<li><a href="#Self-Hosted - Flan-t5-xxl">Self-Hosted - Flan-t5-xxl</a></li>
<li><a href="#Self-Hosted - Flan-ul2">Self-Hosted - Flan-ul2</a></li>
<li><a href="#Self-Hosted - Llama-2">Self-Hosted - Llama-2</a></li>
<li><a href="#Self-Hosted - Llama-2-chat">Self-Hosted - Llama-2-chat</a></li>
<li><a href="#Self-Hosted - Mistral-7B-Instruct">Self-Hosted - Mistral-7B-Instruct</a></li>
<li><a href="#Self-Hosted - MPT-7B-instruct">Self-Hosted - MPT-7B-instruct</a></li>
<li><a href="#together.ai - Llama-2 Chat 13B">together.ai - Llama-2 Chat 13B</a></li>
<li><a href="#together.ai - Llama-2 Chat 70B">together.ai - Llama-2 Chat 70B</a></li>
<li><a href="#together.ai - Llama-2 Chat 7B">together.ai - Llama-2 Chat 7B</a></li>
<li><a href="#together.ai - llama-2-13b">together.ai - llama-2-13b</a></li>
<li><a href="#together.ai - llama-2-70b">together.ai - llama-2-70b</a></li>
<li><a href="#together.ai - LLaMA-2-7B-32K-Instruct">together.ai - LLaMA-2-7B-32K-Instruct</a></li>
<li><a href="#together.ai - Mistral-7B-Instruct">together.ai - Mistral-7B-Instruct</a></li>
<li><a href="#together.ai - Mixtral-8x7B-Instruct">together.ai - Mixtral-8x7B-Instruct</a></li>
<li><a href="#watsonx - Flan-t5-xxl">watsonx - Flan-t5-xxl</a></li>
<li><a href="#watsonx - Flan-ul2">watsonx - Flan-ul2</a></li>
<li><a href="#watsonx - granite-13b-chat-v1">watsonx - granite-13b-chat-v1</a></li>
<li><a href="#watsonx - granite-13b-chat-v2">watsonx - granite-13b-chat-v2</a></li>
<li><a href="#watsonx - granite-13b-instruct-v1">watsonx - granite-13b-instruct-v1</a></li>
<li><a href="#watsonx - granite-13b-instruct-v2">watsonx - granite-13b-instruct-v2</a></li>
<li><a href="#watsonx - Llama-2-chat 13B">watsonx - Llama-2-chat 13B</a></li>
<li><a href="#watsonx - Llama-2-chat 70B">watsonx - Llama-2-chat 70B</a></li>
<li><a href="#watsonx - MPT-7B-instruct2">watsonx - MPT-7B-instruct2</a></li>
<li><a href="#watsonx (Tech Preview - Deprecated) - Flan-t5-xxl">watsonx (Tech Preview - Deprecated) - Flan-t5-xxl</a></li>
<li><a href="#watsonx (Tech Preview - Deprecated) - Flan-ul2">watsonx (Tech Preview - Deprecated) - Flan-ul2</a></li>
<li><a href="#watsonx (Tech Preview - Deprecated) - Llama-2-chat portuguese 13B">watsonx (Tech Preview - Deprecated) - Llama-2-chat portuguese 13B</a></li>
<li><a href="#watsonx (Tech Preview - Deprecated) - MPT-7B-instruct">watsonx (Tech Preview - Deprecated) - MPT-7B-instruct</a></li>
</ul>
<table>
<thead>
<tr>
<th>Platform</th>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amazon Bedrock</td>
<td>Claude Instant v1.1</td>
<td><span id="Amazon Bedrock - Claude Instant v1.1">A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Claude v1.3</td>
<td><span id="Amazon Bedrock - Claude v1.3">Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Claude v2</td>
<td><span id="Amazon Bedrock - Claude v2">Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Jurassic-2 Mid</td>
<td><span id="Amazon Bedrock - Jurassic-2 Mid">Jurassic-2 Mid is AI21’s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Jurassic-2 Ultra</td>
<td><span id="Amazon Bedrock - Jurassic-2 Ultra">Jurassic-2 Ultra is AI21’s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Llama-2-chat 13B</td>
<td><span id="Amazon Bedrock - Llama-2-chat 13B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>Amazon Bedrock</td>
<td>Titan</td>
<td><span id="Amazon Bedrock - Titan">Amazon Titan Foundation Models are pretrained on large datasets, making them powerful, general-purpose models. Use them as is or customize them by fine tuning the models with your own data for a particular task without annotating large volumes of data</span></td>
</tr>
<tr>
<td>Azure Cognitive Services</td>
<td>GPT3.5</td>
<td><span id="Azure Cognitive Services - GPT3.5">GPT-3.5 provides a good balance of speed and capability.</span></td>
</tr>
<tr>
<td>Azure Cognitive Services</td>
<td>GPT4</td>
<td><span id="Azure Cognitive Services - GPT4">GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</span></td>
</tr>
<tr>
<td>Azure Cognitive Services</td>
<td>GPT4 (32K)</td>
<td><span id="Azure Cognitive Services - GPT4 (32K)">GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 32K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Flan-t5-xxl</td>
<td><span id="HuggingFace - Flan-t5-xxl">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Flan-ul2</td>
<td><span id="HuggingFace - Flan-ul2">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Llama-2</td>
<td><span id="HuggingFace - Llama-2">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) </span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Llama-2-chat</td>
<td><span id="HuggingFace - Llama-2-chat">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Mistral-7B-Instruct</td>
<td><span id="HuggingFace - Mistral-7B-Instruct">Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version. </span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>Mixtral-8x7B-Instruct</td>
<td><span id="HuggingFace - Mixtral-8x7B-Instruct">The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version. </span></td>
</tr>
<tr>
<td>HuggingFace</td>
<td>MPT-7B-instruct</td>
<td><span id="HuggingFace - MPT-7B-instruct">The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. </span></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT3.5</td>
<td><span id="OpenAI - GPT3.5">GPT-3.5 provides a good balance of speed and capability.</span></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT3.5 (16K)</td>
<td><span id="OpenAI - GPT3.5 (16K)">GPT-3.5 provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</span></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT4</td>
<td><span id="OpenAI - GPT4">GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</span></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT4 (32K)</td>
<td><span id="OpenAI - GPT4 (32K)">GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</span></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT4 Turbo (Preview)</td>
<td><span id="OpenAI - GPT4 Turbo (Preview)">GPT-4 Turbo provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>Flan-t5-xxl</td>
<td><span id="Self-Hosted - Flan-t5-xxl">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>Flan-ul2</td>
<td><span id="Self-Hosted - Flan-ul2">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>Llama-2</td>
<td><span id="Self-Hosted - Llama-2">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) </span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>Llama-2-chat</td>
<td><span id="Self-Hosted - Llama-2-chat">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>Mistral-7B-Instruct</td>
<td><span id="Self-Hosted - Mistral-7B-Instruct">Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  Mistral operates well on single-GPU instances, and is generally stronger than other models in its class.  This model is the instruct version. </span></td>
</tr>
<tr>
<td>Self-Hosted</td>
<td>MPT-7B-instruct</td>
<td><span id="Self-Hosted - MPT-7B-instruct">The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. </span></td>
</tr>
<tr>
<td>together.ai</td>
<td>Llama-2 Chat 13B</td>
<td><span id="together.ai - Llama-2 Chat 13B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>together.ai</td>
<td>Llama-2 Chat 70B</td>
<td><span id="together.ai - Llama-2 Chat 70B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>together.ai</td>
<td>Llama-2 Chat 7B</td>
<td><span id="together.ai - Llama-2 Chat 7B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>together.ai</td>
<td>llama-2-13b</td>
<td><span id="together.ai - llama-2-13b">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) </span></td>
</tr>
<tr>
<td>together.ai</td>
<td>llama-2-70b</td>
<td><span id="together.ai - llama-2-70b">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) </span></td>
</tr>
<tr>
<td>together.ai</td>
<td>LLaMA-2-7B-32K-Instruct</td>
<td><span id="together.ai - LLaMA-2-7B-32K-Instruct">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) </span></td>
</tr>
<tr>
<td>together.ai</td>
<td>Mistral-7B-Instruct</td>
<td><span id="together.ai - Mistral-7B-Instruct">Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version. </span></td>
</tr>
<tr>
<td>together.ai</td>
<td>Mixtral-8x7B-Instruct</td>
<td><span id="together.ai - Mixtral-8x7B-Instruct">The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version. </span></td>
</tr>
<tr>
<td>watsonx</td>
<td>Flan-t5-xxl</td>
<td><span id="watsonx - Flan-t5-xxl">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>Flan-ul2</td>
<td><span id="watsonx - Flan-ul2">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>granite-13b-chat-v1</td>
<td><span id="watsonx - granite-13b-chat-v1">The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>granite-13b-chat-v2</td>
<td><span id="watsonx - granite-13b-chat-v2">The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>granite-13b-instruct-v1</td>
<td><span id="watsonx - granite-13b-instruct-v1">The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>granite-13b-instruct-v2</td>
<td><span id="watsonx - granite-13b-instruct-v2">The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>Llama-2-chat 13B</td>
<td><span id="watsonx - Llama-2-chat 13B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>Llama-2-chat 70B</td>
<td><span id="watsonx - Llama-2-chat 70B">Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>watsonx</td>
<td>MPT-7B-instruct2</td>
<td><span id="watsonx - MPT-7B-instruct2">The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. </span></td>
</tr>
<tr>
<td>watsonx (Tech Preview - Deprecated)</td>
<td>Flan-t5-xxl</td>
<td><span id="watsonx (Tech Preview - Deprecated) - Flan-t5-xxl">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>watsonx (Tech Preview - Deprecated)</td>
<td>Flan-ul2</td>
<td><span id="watsonx (Tech Preview - Deprecated) - Flan-ul2">The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</span></td>
</tr>
<tr>
<td>watsonx (Tech Preview - Deprecated)</td>
<td>Llama-2-chat portuguese 13B</td>
<td><span id="watsonx (Tech Preview - Deprecated) - Llama-2-chat portuguese 13B">Llama-2 Portuguese brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</span></td>
</tr>
<tr>
<td>watsonx (Tech Preview - Deprecated)</td>
<td>MPT-7B-instruct</td>
<td><span id="watsonx (Tech Preview - Deprecated) - MPT-7B-instruct">The mpt-7b-instruct model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. </span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>💡 LLM choice is available with NeuralSeek’s BYOLLM (bring your own Large Language Model) plan.</p>
<p>💡 LLMs can vary in their capabilities and performances. Some LLM can take up to 30 seconds and longer to generate a full response. Use caution when using in conjunction with a virtual agent platform that imposes a strict timeout.</p>
</blockquote>
<h1 id="configuring-an-llm">Configuring an LLM<a class="headerlink" href="#configuring-an-llm" title="Permanent link"></a></h1>
<blockquote>
<p>⚠️ In order to configure an LLM, make sure that you have subscribed to the Bring Your Own LLM (BYOLLM) plan. All other plans will default to NeuralSeek's curated LLM, and this option will not be available.</p>
</blockquote>
<ol>
<li>In NeuralSeek UI, navigate to <code>Configure &gt; LLM Details</code> page, using the top menu.</li>
<li>Click <code>Add an LLM</code> button.</li>
<li>Select the Platform and LLM Selection. (e.g. Platform: Self-Hosted, LLM: Flan-u2)</li>
<li>Click <code>Add</code>.</li>
<li>Enter the <code>LLM API key</code> in the LLM API Key input field.</li>
<li>Review the Enabled Languages (presented as multi-select)</li>
<li>Review the LLM functions available (presented as checkbox)</li>
<li>Click <code>Test</code> button to test whether the API key works.</li>
</ol>
<blockquote>
<p>💡 You must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled.</p>
</blockquote>

  <br>
</div>

<footer class="container-fluid wm-page-content"><p>Ⓒ 2024 NeuralSeek, all rights reserved.</p>
</footer>

<script src="../../../scripts/watson.js"></script>
<script> if(is_top_frame) loadWatson(); </script>
<script>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});</script></body>
</html>