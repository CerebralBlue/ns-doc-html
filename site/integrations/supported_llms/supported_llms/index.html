
<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://documentation.neuralseek.com/integrations/supported_llms/supported_llms/">
    <link rel="shortcut icon" href="../../../img/ns-white.svg">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&amp;display=swap" rel="stylesheet">
    <title>Supported LLMs - NeuralSeek Documentation</title>

  <meta property="og:title" content="Supported LLMs" />
  <meta property="og:description" content="NeuralSeek Documentation" />
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/colors.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight-github-theme.css">
    <link rel="stylesheet" href="../../../css/highlightjs-copy.css">
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/override.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.min.js"></script>
    <script src="../../../js/highlightjs-copy.min.js"></script>
    <script src="../../../js/elasticlunr.min.js"></script>
      
    <base target="_top">
    <script>
      hljs.addPlugin(new CopyButtonPlugin());
      hljs.configure({languages:[]});
      hljs.highlightAll();
    </script>
    <script>
  var base_url = '../../..';
  var is_top_frame = false;
    
    var pageToc = [
      {title: "Overview", url: "#overview", children: [
      ]},
      {title: "Configuring an LLM", url: "#configuring-an-llm", children: [
      ]},
    ];

</script>
    <script src="../../../js/base.js?v=2"></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LE5XX6X6Z7"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'G-LE5XX6X6Z7');
      </script> 
 <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wm-top-page">
<nav class="navbar wm-page-top-frame">
  <div class="container-fluid wm-top-container">
    
    <div class="wm-top-tool pull-right wm-vcenter">
      <form class="dropdown wm-vcentered" id="wm-search-form" action="../../../search.html">
        
        <button id="wm-search-show" class="btn btn-sm btn-default" type="submit"
          ><i class="fa fa-search" aria-hidden="true"></i></button>

        <div class="input-group input-group-sm wm-top-search">
          <input type="text" name="q" class="form-control" id="mkdocs-search-query" placeholder="Search" autocomplete="off">
          <span class="input-group-btn" role="search">
            
            <button class="btn btn-default dropdown-toggle collapse" data-toggle="dropdown" type="button"><span class="caret"></span></button>
            <ul id="mkdocs-search-results" class="dropdown-menu dropdown-menu-right"></ul>
            <button id="wm-search-go" class="btn btn-default" type="submit"><i class="fa fa-search" aria-hidden="true"></i></button>
          </span>
        </div>
      </form>
    </div>

    
    <div class="wm-top-tool wm-vcenter pull-left wm-small-left">
      <button id="wm-toc-button" type="button" class="btn btn-sm btn-default wm-vcentered"><i class="fa fa-bars" aria-hidden="true"></i></button>
    </div>
    
    
    
      <div class="wm-top-tool wm-vcenter pull-right">
  <div class="wm-select wm-vcentered">
    
    <button class="wm-header__button wm-icon" aria-label="">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg>
    </button>
    <div class="wm-select__inner">
      <ul class="wm-select__list">
        
          <li class="wm-select__item">
            
              <a href="/integrations/supported_llms/supported_llms/" hreflang="English" class="wm-select__link">
              English
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    

    
    

    
    <a href="https://documentation.neuralseek.com/" class="wm-top-brand wm-top-link wm-vcenter">
      
      <img class="wm-top-logo" src="../../../img/ns-white.svg"/>
      
      <div class="wm-top-title">
        NeuralSeek Documentation<br>
        
      </div>
    </a>
  </div>
</nav>

<div id="main-content" class="wm-page-top-frame">
    
<nav class="wm-toc-pane">
  
  <ul class="wm-toctree">
        
      
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev0 wm-toc-opener  "><span class="wm-toc-text">About Cerebral Blue</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev1   "><a href="https://cerebralblue.com/" class="wm-article-link wm-toc-text">Home Page</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="https://cerebralblue.com/about-us/" class="wm-article-link wm-toc-text">About Us</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="https://cerebralblue.com/contact-us/" class="wm-article-link wm-toc-text">Contact Us</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="https://cerebralblue.com/contact-us/" class="wm-article-link wm-toc-text">Partnerships</a>
</li>
  </ul>
</li>

        

<li class="wm-toc-li wm-toc-lev0   "><a href="../../../" class="wm-article-link wm-toc-text">NeuralSeek Overview</a>
</li>
        
      
      
      
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev0 wm-toc-opener  "><span class="wm-toc-text">Main Features</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../main_features/user_interface/user_interface/" class="wm-article-link wm-toc-text">NeuralSeek User Interface</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../main_features/conversational_capabilities/conversational_capabilities/" class="wm-article-link wm-toc-text">Conversational Capabilities</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../main_features/language_capabilities/language_capabilities/" class="wm-article-link wm-toc-text">Language Capabilities</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../main_features/data_management/data_management/" class="wm-article-link wm-toc-text">Data Management</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../main_features/advanced_features/advanced_features/" class="wm-article-link wm-toc-text">Advanced Features</a>
</li>
  </ul>
</li>

        
      
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev0 wm-toc-opener  wm-toc-open"><span class="wm-toc-text">Integrations</span>
</li>
<li class="wm-toc-li-nested collapse in">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../integrations/supported_knowledgebases/supported_knowledgebases/" class="wm-article-link wm-toc-text">Supported KnowledgeBases</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1  wm-current "><a href="../../../integrations/supported_llms/supported_llms/" class="wm-article-link wm-toc-text">Supported LLMs</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../integrations/supported_virtual_agents/supported_virtual_agents/" class="wm-article-link wm-toc-text">Supported Virtual Agents</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../integrations/rest_api/rest_api/" class="wm-article-link wm-toc-text">REST API</a>
</li>
  </ul>
</li>

        
      
      
      
      

<li class="wm-toc-li wm-toc-lev0 wm-toc-opener  "><span class="wm-toc-text">Reference Material</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev1 wm-toc-opener  "><span class="wm-toc-text">mAIstro Features</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/maistro/maistro/" class="wm-article-link wm-toc-text">Visual Editor</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/maistro/ntl_overview/" class="wm-article-link wm-toc-text">NTL Overview</a>
</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev2 wm-toc-opener  "><span class="wm-toc-text">All NTL Functions</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/get_data/" class="wm-article-link wm-toc-text">Get data</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/upload_data/" class="wm-article-link wm-toc-text">Upload data</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/generate_data/" class="wm-article-link wm-toc-text">Generate data</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/extract_data/" class="wm-article-link wm-toc-text">Extract data</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/database_connections/" class="wm-article-link wm-toc-text">Database connections</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/control_flow/" class="wm-article-link wm-toc-text">Control flow</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/guardrails/" class="wm-article-link wm-toc-text">Guardrails</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/system_variables/" class="wm-article-link wm-toc-text">System variables</a>
</li>
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev3 wm-toc-opener  "><span class="wm-toc-text">Modify Data</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev4   "><a href="../../../reference_material/maistro/ntl/modify_data/Jsontools/" class="wm-article-link wm-toc-text">JSON Toolbox</a>
</li>
      

<li class="wm-toc-li wm-toc-lev4   "><a href="../../../reference_material/maistro/ntl/modify_data/Stringtools/" class="wm-article-link wm-toc-text">String Toolbox</a>
</li>
      

<li class="wm-toc-li wm-toc-lev4   "><a href="../../../reference_material/maistro/ntl/modify_data/Transform/" class="wm-article-link wm-toc-text">Transform</a>
</li>
  </ul>
</li>

      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/send_data/" class="wm-article-link wm-toc-text">Send data</a>
</li>
      

<li class="wm-toc-li wm-toc-lev3   "><a href="../../../reference_material/maistro/ntl/parallel/" class="wm-article-link wm-toc-text">Parallel Execution with Nodes and Variables</a>
</li>
  </ul>
</li>

  </ul>
</li>

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

<li class="wm-toc-li wm-toc-lev1 wm-toc-opener  "><span class="wm-toc-text">Guides</span>
</li>
<li class="wm-toc-li-nested collapse ">
  <ul class="wm-toctree">
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/backup_and_restore/backup_and_restore/" class="wm-article-link wm-toc-text">Backup and Restore</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/elasticseach_vector_model/elasticsearch_vector_model/" class="wm-article-link wm-toc-text">Configuring ElasticSearch for Vector Search</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/implementing_feedback/implementing_feedback/" class="wm-article-link wm-toc-text">Implementing Feedback</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/multimodal/multimodal/" class="wm-article-link wm-toc-text">Multimodal LLM Configuration</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/pinecone_configuration/pinecone/" class="wm-article-link wm-toc-text">Pinecone Integration with NeuralSeek</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/proposals/proposals/" class="wm-article-link wm-toc-text">Using Proposals</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/providing_context/providing_context/" class="wm-article-link wm-toc-text">Passing Conversational Context</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/training_virtual_agents/training_virtual_agents/" class="wm-article-link wm-toc-text">Training Virtual Agents</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/tuning_guide/tuning_guide/" class="wm-article-link wm-toc-text">KnowledgeBase Tuning</a>
</li>
      

<li class="wm-toc-li wm-toc-lev2   "><a href="../../../reference_material/guides/virtualKB/virtualKB/" class="wm-article-link wm-toc-text">Virtual KnowledgeBase</a>
</li>
  </ul>
</li>

      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../reference_material/configuration/configuration/" class="wm-article-link wm-toc-text">Configuration Details</a>
</li>
      

<li class="wm-toc-li wm-toc-lev1   "><a href="../../../reference_material/governance/governance/" class="wm-article-link wm-toc-text">Governance Metrics</a>
</li>
  </ul>
</li>

        

<li class="wm-toc-li wm-toc-lev0   "><a href="../../../plans/" class="wm-article-link wm-toc-text">Available NeuralSeek Plans</a>
</li>
        

<li class="wm-toc-li wm-toc-lev0   "><a href="../../../data_security_and_privacy/" class="wm-article-link wm-toc-text">Data Security and Privacy</a>
</li>
        

<li class="wm-toc-li wm-toc-lev0   "><a href="../../../changelog/" class="wm-article-link wm-toc-text">Changelog</a>
</li>
  </ul>
</nav>

  <div class="wm-content-pane">
    <div class="container-fluid wm-page-content">
        
        <div class="wm-page-real-content">
          <a name="_top"></a>  
          
          <h1 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link"></a></h1>
<p>NeuralSeek supports LLMs from many providers, including:</p>
<ul>
<li>Amazon Bedrock</li>
<li>Azure Cognitive Services</li>
<li>Google Vertex AI</li>
<li>HuggingFace</li>
<li>OpenAI</li>
<li>together.ai</li>
<li>watsonx.ai</li>
</ul>
<p>In addition to any generic OpenAI-compatible endpoints.</p>
<p><br/></p>
<p><strong>Supported LLM details by provider:</strong></p>
<details class="success">
<summary>Amazon Bedrock</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 3 Haiku</td>
<td>Claude 3 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>Claude 3 Opus is Anthropic's most powerful AI model, with state-of-the-art performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Claude 3 Opus shows us the frontier of what’s possible with generative AI. Claude 3 Opus can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude 3 Sonnet</td>
<td>Claude 3 Sonnet by Anthropic strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It offers maximum utility at a lower price than competitors, and is engineered to be the dependable, high-endurance workhorse for scaled AI deployments. Claude 3 Sonnet can process images and return text outputs, and features a 200K context window.</td>
</tr>
<tr>
<td>Claude Instant v1.1</td>
<td>A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering.</td>
</tr>
<tr>
<td>Claude v2</td>
<td>Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</td>
</tr>
<tr>
<td>Claude v2.1</td>
<td>Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.</td>
</tr>
<tr>
<td>Jurassic-2 Mid</td>
<td>Jurassic-2 Mid is AI21’s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others.</td>
</tr>
<tr>
<td>Jurassic-2 Ultra</td>
<td>Jurassic-2 Ultra is AI21’s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more.</td>
</tr>
<tr>
<td>Llama-2-chat 13B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2-chat 70B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mistral-large</td>
<td>The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.</td>
</tr>
<tr>
<td>Mistral-small</td>
<td>Mistraql Small is optimized for high-volume, low-latency language-based tasks. Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Titan Text G1 - Express</td>
<td>Amazon Titan Text Express has a context length of up to 8,000 tokens, making it well-suited for a wide range of advanced, general language tasks such as open-ended text generation and conversational chat, as well as support within Retrieval Augmented Generation (RAG). At launch, the model is optimized for English, with multilingual support for more than 100 additional languages available in preview.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>Azure Cognitive Services</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Azure GPT4 Turbo (Preview)</td>
<td>GPT-4 Turbo provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>GPT-4o  It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages.</td>
</tr>
<tr>
<td>GPT3.5</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT4</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</td>
</tr>
<tr>
<td>GPT4 (32K)</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 32K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>Google Vertex AI</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>gemini-1.5-flash (128K Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-flash (1M Context)</td>
<td>Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.</td>
</tr>
<tr>
<td>gemini-1.5-pro (128K Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
<tr>
<td>gemini-1.5-pro (1M Context)</td>
<td>Gemini 1.5 Pro is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>HuggingFace</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flan-t5-xxl</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Flan-ul2</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Llama-2</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>Llama-2-chat</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-chat</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. It outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>MPT-7B-instruct</td>
<td>The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>OpenAI</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-3.5-turbo-0125</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>GPT-4o  It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages.</td>
</tr>
<tr>
<td>GPT3.5</td>
<td>GPT-3.5 provides a good balance of speed and capability.</td>
</tr>
<tr>
<td>GPT3.5 (16K)</td>
<td>GPT-3.5 provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT4</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout.</td>
</tr>
<tr>
<td>GPT4 (32K)</td>
<td>GPT-4 can often take longer than 30 seconds for a full response.  Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
<tr>
<td>GPT4 Turbo (Preview)</td>
<td>GPT-4 Turbo provides a good balance of speed and capability.  The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>together.ai</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2 Chat 13B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2 Chat 70B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2 Chat 7B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-2-13b</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>llama-2-70b</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>LLaMA-2-7B-32K-Instruct</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf)</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. It outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
</tbody>
</table>
</details>
<details class="success">
<summary>watsonx.ai</summary>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>elyza-japanese-llama-2-7b-instruct</td>
<td>ELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。</td>
</tr>
<tr>
<td>Flan-t5-xxl</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>Flan-ul2</td>
<td>The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better.  Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%.</td>
</tr>
<tr>
<td>granite-13b-chat-v1</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-13b-chat-v2</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-13b-instruct-v1</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-13b-instruct-v2</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-20b-multilingual</td>
<td>The Granite series of models are a step ahead of their counterpart t5 and UL2 models.  They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents.  They do not have much ability to reason, however.  This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation.  Use semantic scoring to block this hallucination.</td>
</tr>
<tr>
<td>granite-7b-lab</td>
<td>The Granite 7 Billion LAB (granite-7b-lab) model is the chat-focused variant initialized from the pre-trained Granite 7 Billion (granite-7b) model, which is Meta Llama 2 7B architecture trained to 2T tokens.</td>
</tr>
<tr>
<td>granite-8b-japanese</td>
<td>The Granite 8 Billion Japanese model is an instruct variant initialized from the pre-trained Granite Base 8 Billion Japanese model. Pre-training went through 1.0T tokens of English, 0.5T tokens of Japanese, and 0.1T tokens of code. This model is designed to work with Japanese text. IBM Generative AI Large Language Foundation Models are Enterprise-level Multilingual models trained with large volumes of data that has been subjected to intensive pre-processing and careful analysis.</td>
</tr>
<tr>
<td>jais-13b-chat</td>
<td>Jais-13b-chat is Jais-13b fine-tuned over a curated set of 4 million Arabic and 6 million English prompt-response pairs.</td>
</tr>
<tr>
<td>Llama-2-chat 13B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Llama-2-chat 70B</td>
<td>Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>llama-3-70b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>llama-3-8b-instruct</td>
<td>Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks..</td>
</tr>
<tr>
<td>merlinite-7b</td>
<td>Merlinite is Mistral fine-tuned by Mixtral using IBM's LAB methodology.  Merlinite tends to hallucinate to the extreme, and show difficulty containing its output without running away. It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v01-q</td>
<td>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks. Mistral is good at joining thoughts across multiple documents.  It is also highly sensitive.  Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning.  This model is the instruct version.</td>
</tr>
<tr>
<td>MPT-7B-instruct2</td>
<td>The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses.  Be sure to set a minimum confidence level to control this. Not reccomended for public usecases.</td>
</tr>
</tbody>
</table>
</details>
<blockquote>
<p>💡 LLM choice is available with NeuralSeek’s BYOLLM (bring your own Large Language Model) plan.</p>
<p>💡 LLMs can vary in their capabilities and performances. Some LLM can take up to 30 seconds and longer to generate a full response. Use caution when using in conjunction with a virtual agent platform that imposes a strict timeout.</p>
</blockquote>
<h1 id="configuring-an-llm">Configuring an LLM<a class="headerlink" href="#configuring-an-llm" title="Permanent link"></a></h1>
<blockquote>
<p>⚠️ In order to configure an LLM, make sure that you have subscribed to the Bring Your Own LLM (BYOLLM) plan. All other plans will default to NeuralSeek's curated LLM, and this option will not be available.</p>
</blockquote>
<ol>
<li>In NeuralSeek UI, navigate to <code>Configure &gt; LLM Details</code> page, using the top menu.</li>
<li>Click <code>Add an LLM</code> button.</li>
<li>Select the Platform and LLM Selection. (e.g. Platform: Self-Hosted, LLM: Flan-u2)</li>
<li>Click <code>Add</code>.</li>
<li>Enter the <code>LLM API key</code> in the LLM API Key input field.</li>
<li>Review the Enabled Languages (presented as multi-select)</li>
<li>Review the LLM functions available (presented as checkbox)</li>
<li>Click <code>Test</code> button to test whether the API key works.</li>
</ol>
<blockquote>
<p>💡 You must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled.</p>
</blockquote>

          <br>
          <footer class="wm-footer">
                <p>Ⓒ 2024 NeuralSeek, all rights reserved.</p>
          </footer>
        </div>
      <br>
    </div>
  </div>
</div>




<script src="../../../scripts/watson.js"></script>
<script> 
  function isInFrame() {
    return (window.top !== window);
  }
  if (!isInFrame()) {
    loadWatson(); 
  }
</script>
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>