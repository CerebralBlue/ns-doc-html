{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview NeuralSeek is an AI-powered Answers-as-a-Service, designed to enhance information sharing and customer support within organizations\u2019 virtual agents. NeuralSeek works by leveraging the capabilities of a sophisticated Large Language Model(LLM) and the users\u2019 corporate KnowledgeBase, allowing virtual agents to provide concise and contextually relevant responses to user queries. NeuralSeek empowers businesses. Unlike most AI, NeuralSeek provides a clickable path to fact check AI generated responses, data analytics to improve AI natural language, and step-by-step instructions to use AI to clean and maintain accurate resource data. It is the business solution to use AI in a professional workplace. By leveraging a comprehensive knowledge base - Supported KnowledgeBases - NeuralSeek excels at answering user questions. What sets NeuralSeek apart from conventional AI solutions is its incorporated set of features. NeuralSeek offers a clickable path to fact-check AI response, utilization of data analytics to enhance AI natural language capabilities, and comprehensive step-by-step instructions for maintaining accuracy and clean resource data. With these additional capabilities, NeuralSeek emerges as the ideal AI solution for empowering professional businesses.","title":"NeuralSeek Overview"},{"location":"#overview","text":"NeuralSeek is an AI-powered Answers-as-a-Service, designed to enhance information sharing and customer support within organizations\u2019 virtual agents. NeuralSeek works by leveraging the capabilities of a sophisticated Large Language Model(LLM) and the users\u2019 corporate KnowledgeBase, allowing virtual agents to provide concise and contextually relevant responses to user queries. NeuralSeek empowers businesses. Unlike most AI, NeuralSeek provides a clickable path to fact check AI generated responses, data analytics to improve AI natural language, and step-by-step instructions to use AI to clean and maintain accurate resource data. It is the business solution to use AI in a professional workplace. By leveraging a comprehensive knowledge base - Supported KnowledgeBases - NeuralSeek excels at answering user questions. What sets NeuralSeek apart from conventional AI solutions is its incorporated set of features. NeuralSeek offers a clickable path to fact-check AI response, utilization of data analytics to enhance AI natural language capabilities, and comprehensive step-by-step instructions for maintaining accuracy and clean resource data. With these additional capabilities, NeuralSeek emerges as the ideal AI solution for empowering professional businesses.","title":"Overview"},{"location":"changelog/","text":"December 2023 New features Multilingual chain-of-thought prompting to enhance smaller LLMs like Llama and Granite for non-English languages. ElasticSearch / Watsonx Discovery Vector Search setup for hybrid or full vector search capabilities. KB ReRanker for custom result prioritization by field/tag and value lists. Profanity Filter implemented for multi-language profanity and hate speech filtering across all LLMs. Role-based access control for managing user permissions within the NeuralSeek UI. Explore enhancements: OpenAPI spec generator for easy integration with Watson Assistant. Inspector tool for debugging the Explore flow and variable states. REST connector for making various HTTP requests and auto-parsing JSON into variables. JSON to Variables stage for automatic variable creation from JSON input. Output Variables formatting to match input parameters for seamless chaining in Explore. Import/Export functionality for sharing templates across instances. New functionality: DB2 database connector Table Prep (convert tables into natural language statements) KB search filters Stump for Seek (to sideload trusted data) Regex Several new example templates New integrations Added Llama-2-chat Portuguese 13B to Watsonx Tech Preview. Release of Granite V2 in the model cards, offering improved performance over V1. Updates Watsonx.ai models transitioned to streaming for improved timeout handling. Enhanced error reporting in the UI for Knowledge Bases (KBs) to show more detailed configuration feedback. Semantic Scoring model improvements with lemmatization consideration for partial match scoring. Watsonx Discovery automatic API key generation for simplified access. November 2023 New features Explore: Expanded NTL-based explore functionality with drag-and-drop simplicity for building Explore routines. Added the ability to create and save templates within the UI. Introduced variables for easy API calling by passing template name and variable values. Dynamic Variable Setting - Introduce the ability to dynamically set variables within a chain or flow, capture outputs into variables for endless reuse, and return all variables via the API (multi-output capability). Recursion / Chained Explore - Enabled the creation of small, repeatable task templates that can be called from other explore templates, with shared variable memory space across templates, facilitating the creation of complex flows with ease. New functionality: Math Equations - Implemented full graphing-calculator level equations, overcoming the LLM's limitations with math by allowing users to set variables with LLMs, perform calculations in the math node, and then provide correct answers back into the LLM. Force Numeric - Added a feature to extract numbers from text, ensuring that when a number is requested from the LLM, a numeric response is provided. Split - Automated the removal of document headers and footers, enabling users to extract the content they need with ease. POST - Provided the ability to call any REST service to submit data or initiate a downstream process. Email - Introduced the functionality to send the output of a flow or variable content directly via email. Updates Semantic Details on Seek - Unveiled the math behind the semantic score through a new modal on the seek tab, previously exclusive to API/developer use. Enhanced context keeping and semantic score for improved abilities in Spanish. Rolled out a new Spanish micro-model to assist with Spanish NLP. Updated base weights and prompting to counter GPT's recent drifting. Semantic Scoring now has the ability to consider document title and URL, capturing unique words that may be missing in the document itself. Added the ability to pass a filter column for regression testing. October 2023 New features \"Generate Data\" options in Explore tab \u2013 Send to LLM, Table Understanding \"Logs\" tab - See history of questions/answers given Hyper-personalization (Corporate document filtering) Corporate Logging - Connect NeuralSeek to an ElasticSearch instance to log everything around Seek, updates, edits, changes Configuration Logs - History of changed settings Enhancements to Explore: \"Seek\" data PII removal Table Understanding New integrations Elastic Search integration Multi-Turn Conversation Generation for Cognigy Mistral 7B Model support Updates Released On-Prem \"Flex\" plan Added version numbering to \"Integrate\" tab sidebar Seek tab - \"Show generated\" option when the minimum confidence is not met September 2023 New features Explore: An Open-Ended Retrieval Augmented Generation Playground Vector Similarity for Intent Matching New integrations Kore.ai Round Trip Monitoring IBM watsonx Granite Models Supported AWS Bedrock Integration / Models Supported Llama 2 Chat Model Support OpenSearch Integration HuggingFace Integration for Supported Models Updates Refinements to Vector Similarity Matching August 2023 New features BYO-LLM plans \u2013 IBM watsonx language translation Option for summarization of document passage results from KB Option for Link Summarization of NeuralSeek Results, 1-5 Result Links 'Bring Your Own' Large Language Model (BYO-LLM) cards \u2013 ability to use multiple LLMs for a specific task New integrations IBM Watson Assistant Dialog Multi-Turn Conversation Templates AWS Kendra Integration AWS Lex Multi-Turn Conversation Generation Templates Updates New \u2018Seek\u2019 Parameter Call to Indicate LLM Preference Ability to set specific language on each LLM \u2013 e.g., \u201cuse THIS model for Spanish Seek / Translation\u201d July 2023 New features Slot Filler - Ability to auto-fill slots when gathering information Offline spreadsheet editing with upload to Curate tab ConsoleAPI under Integrate tab Answer Streaming \u2013 users can now enable streaming responses from NeuralSeek with supported LLMs Translate Endpoint Curate to CSV / Upload Curated QA from CSV On-Prem deployment support New 'Identify Language' Endpoint Entity Extraction feature - Custom Entity Creation New integrations IBM watsonx Model Compatibility AWS Lex Round-Trip Monitoring Updates KnowledgeBase translation updated \u2013 questions now get translated to KnowledgeBase source language for summarization Cross-lingual support when using language code \u201cxx\u201d (Match Input) enhanced Semantic Match Analysis to describe the logic for the Semantic Score enhanced June 2023 New integrations IBM watsonx (LLM) connector Updates AWS Partnership Announcement Improvements to Caching Confidence and Coverage Score Graphs added to Curate tab May 2023 New features Analytics API endpoint Table Extraction model to enable answers from tabular data Updates Data Cleanser for non-HTML enabled April 2023 New features New plan - 'Bring Your Own' Large Language Model (BYO-LLM) Semantic Score Model, Improved Provenance and Semantic Source Re-Rank New integrations Curate answers to Kore.ai, Cognigy, AWS Lex Updates IBM Frankfurt (FRA) data center availability IBM Sydney (SYD) data center availability March 2023 New features Personal Identifiable Information (PII) Detection Sentiment Analysis Source Document Monitoring and Answer Regeneration New integrations Watson Assistant Round-Trip Logging Updates User-specified input length enabled February 2023 New features Personalization of generated answers New integrations Auto-Build Watson Assistant Multi-Step Action Updates Additional languages enabled (Chinese, Czech, Dutch, Indonesian, Japanese) Enhanced API to allow run-time modification of all parameters KB tuning parameters enabled Large Language Model (LLM) tuning","title":"Changelog"},{"location":"changelog/#december-2023","text":"New features Multilingual chain-of-thought prompting to enhance smaller LLMs like Llama and Granite for non-English languages. ElasticSearch / Watsonx Discovery Vector Search setup for hybrid or full vector search capabilities. KB ReRanker for custom result prioritization by field/tag and value lists. Profanity Filter implemented for multi-language profanity and hate speech filtering across all LLMs. Role-based access control for managing user permissions within the NeuralSeek UI. Explore enhancements: OpenAPI spec generator for easy integration with Watson Assistant. Inspector tool for debugging the Explore flow and variable states. REST connector for making various HTTP requests and auto-parsing JSON into variables. JSON to Variables stage for automatic variable creation from JSON input. Output Variables formatting to match input parameters for seamless chaining in Explore. Import/Export functionality for sharing templates across instances. New functionality: DB2 database connector Table Prep (convert tables into natural language statements) KB search filters Stump for Seek (to sideload trusted data) Regex Several new example templates New integrations Added Llama-2-chat Portuguese 13B to Watsonx Tech Preview. Release of Granite V2 in the model cards, offering improved performance over V1. Updates Watsonx.ai models transitioned to streaming for improved timeout handling. Enhanced error reporting in the UI for Knowledge Bases (KBs) to show more detailed configuration feedback. Semantic Scoring model improvements with lemmatization consideration for partial match scoring. Watsonx Discovery automatic API key generation for simplified access.","title":"December 2023"},{"location":"changelog/#november-2023","text":"New features Explore: Expanded NTL-based explore functionality with drag-and-drop simplicity for building Explore routines. Added the ability to create and save templates within the UI. Introduced variables for easy API calling by passing template name and variable values. Dynamic Variable Setting - Introduce the ability to dynamically set variables within a chain or flow, capture outputs into variables for endless reuse, and return all variables via the API (multi-output capability). Recursion / Chained Explore - Enabled the creation of small, repeatable task templates that can be called from other explore templates, with shared variable memory space across templates, facilitating the creation of complex flows with ease. New functionality: Math Equations - Implemented full graphing-calculator level equations, overcoming the LLM's limitations with math by allowing users to set variables with LLMs, perform calculations in the math node, and then provide correct answers back into the LLM. Force Numeric - Added a feature to extract numbers from text, ensuring that when a number is requested from the LLM, a numeric response is provided. Split - Automated the removal of document headers and footers, enabling users to extract the content they need with ease. POST - Provided the ability to call any REST service to submit data or initiate a downstream process. Email - Introduced the functionality to send the output of a flow or variable content directly via email. Updates Semantic Details on Seek - Unveiled the math behind the semantic score through a new modal on the seek tab, previously exclusive to API/developer use. Enhanced context keeping and semantic score for improved abilities in Spanish. Rolled out a new Spanish micro-model to assist with Spanish NLP. Updated base weights and prompting to counter GPT's recent drifting. Semantic Scoring now has the ability to consider document title and URL, capturing unique words that may be missing in the document itself. Added the ability to pass a filter column for regression testing.","title":"November 2023"},{"location":"changelog/#october-2023","text":"New features \"Generate Data\" options in Explore tab \u2013 Send to LLM, Table Understanding \"Logs\" tab - See history of questions/answers given Hyper-personalization (Corporate document filtering) Corporate Logging - Connect NeuralSeek to an ElasticSearch instance to log everything around Seek, updates, edits, changes Configuration Logs - History of changed settings Enhancements to Explore: \"Seek\" data PII removal Table Understanding New integrations Elastic Search integration Multi-Turn Conversation Generation for Cognigy Mistral 7B Model support Updates Released On-Prem \"Flex\" plan Added version numbering to \"Integrate\" tab sidebar Seek tab - \"Show generated\" option when the minimum confidence is not met","title":"October 2023"},{"location":"changelog/#september-2023","text":"New features Explore: An Open-Ended Retrieval Augmented Generation Playground Vector Similarity for Intent Matching New integrations Kore.ai Round Trip Monitoring IBM watsonx Granite Models Supported AWS Bedrock Integration / Models Supported Llama 2 Chat Model Support OpenSearch Integration HuggingFace Integration for Supported Models Updates Refinements to Vector Similarity Matching","title":"September 2023"},{"location":"changelog/#august-2023","text":"New features BYO-LLM plans \u2013 IBM watsonx language translation Option for summarization of document passage results from KB Option for Link Summarization of NeuralSeek Results, 1-5 Result Links 'Bring Your Own' Large Language Model (BYO-LLM) cards \u2013 ability to use multiple LLMs for a specific task New integrations IBM Watson Assistant Dialog Multi-Turn Conversation Templates AWS Kendra Integration AWS Lex Multi-Turn Conversation Generation Templates Updates New \u2018Seek\u2019 Parameter Call to Indicate LLM Preference Ability to set specific language on each LLM \u2013 e.g., \u201cuse THIS model for Spanish Seek / Translation\u201d","title":"August 2023"},{"location":"changelog/#july-2023","text":"New features Slot Filler - Ability to auto-fill slots when gathering information Offline spreadsheet editing with upload to Curate tab ConsoleAPI under Integrate tab Answer Streaming \u2013 users can now enable streaming responses from NeuralSeek with supported LLMs Translate Endpoint Curate to CSV / Upload Curated QA from CSV On-Prem deployment support New 'Identify Language' Endpoint Entity Extraction feature - Custom Entity Creation New integrations IBM watsonx Model Compatibility AWS Lex Round-Trip Monitoring Updates KnowledgeBase translation updated \u2013 questions now get translated to KnowledgeBase source language for summarization Cross-lingual support when using language code \u201cxx\u201d (Match Input) enhanced Semantic Match Analysis to describe the logic for the Semantic Score enhanced","title":"July 2023"},{"location":"changelog/#june-2023","text":"New integrations IBM watsonx (LLM) connector Updates AWS Partnership Announcement Improvements to Caching Confidence and Coverage Score Graphs added to Curate tab","title":"June 2023"},{"location":"changelog/#may-2023","text":"New features Analytics API endpoint Table Extraction model to enable answers from tabular data Updates Data Cleanser for non-HTML enabled","title":"May 2023"},{"location":"changelog/#april-2023","text":"New features New plan - 'Bring Your Own' Large Language Model (BYO-LLM) Semantic Score Model, Improved Provenance and Semantic Source Re-Rank New integrations Curate answers to Kore.ai, Cognigy, AWS Lex Updates IBM Frankfurt (FRA) data center availability IBM Sydney (SYD) data center availability","title":"April 2023"},{"location":"changelog/#march-2023","text":"New features Personal Identifiable Information (PII) Detection Sentiment Analysis Source Document Monitoring and Answer Regeneration New integrations Watson Assistant Round-Trip Logging Updates User-specified input length enabled","title":"March 2023"},{"location":"changelog/#february-2023","text":"New features Personalization of generated answers New integrations Auto-Build Watson Assistant Multi-Step Action Updates Additional languages enabled (Chinese, Czech, Dutch, Indonesian, Japanese) Enhanced API to allow run-time modification of all parameters KB tuning parameters enabled Large Language Model (LLM) tuning","title":"February 2023"},{"location":"data_security_and_privacy/","text":"Overview NeuralSeek is both a UI and a (REST) API. All data that flows to or thru us is encrypted SSL/TLS. All data stored is also encrypted. Neither NeuralSeek nor any of our sub processors use any customer data to learn/train models or systems. All user data and generated answers are owned by and for the sole use of the customer. Data processing locations for our Pay-per-answer plan: Sydney + Dallas \u2013 we use US-based LLM\u2019s. Frankfurt: We use EU-based LLM\u2019s Data is stored within a datacenter. So data storage can be localized to a region. (Eg within the EU) We do not generate any data that is personally identifiable while delivering the service. We utilize optional session tokens, decided on and provided by the customer\u2019s calling service to maintain an optional state. We have an option on our API to generate \u201cPersonalized\u201d answers, where the customer passes us personal data on a defined option on our endpoint. This flags the result as potentially containing PII, and will be treated the same as any content the system automatically flags as containing PII. (Flag, Mask, Hide, Delete) Data is retained on our service for a minimum of 30 days before it is automatically deleted. A customer may delete their generated data from their account at any time, however we may retain the data for the full 30 days for purpose of monitoring abuse of our service. EG \u2013 a customer cannot bypass our terms of service by immediately issuing delete requests on all generated answers. For enterprise customers these terms can be negotiated. In terms of specifics on which LLM\u2019s and sub-processors we use, we can have those conversations as needed under NDA with enterprise customers. The short answer is we use multiple, some internally developed, some provided by third part sub-processors. For enterprise customers there is also a pathway to data isolation, as we are open to offering dedicated instances / storage. For more information, please visit https://neuralseek.com/eula","title":"Data Security and Privacy"},{"location":"data_security_and_privacy/#overview","text":"NeuralSeek is both a UI and a (REST) API. All data that flows to or thru us is encrypted SSL/TLS. All data stored is also encrypted. Neither NeuralSeek nor any of our sub processors use any customer data to learn/train models or systems. All user data and generated answers are owned by and for the sole use of the customer. Data processing locations for our Pay-per-answer plan: Sydney + Dallas \u2013 we use US-based LLM\u2019s. Frankfurt: We use EU-based LLM\u2019s Data is stored within a datacenter. So data storage can be localized to a region. (Eg within the EU) We do not generate any data that is personally identifiable while delivering the service. We utilize optional session tokens, decided on and provided by the customer\u2019s calling service to maintain an optional state. We have an option on our API to generate \u201cPersonalized\u201d answers, where the customer passes us personal data on a defined option on our endpoint. This flags the result as potentially containing PII, and will be treated the same as any content the system automatically flags as containing PII. (Flag, Mask, Hide, Delete) Data is retained on our service for a minimum of 30 days before it is automatically deleted. A customer may delete their generated data from their account at any time, however we may retain the data for the full 30 days for purpose of monitoring abuse of our service. EG \u2013 a customer cannot bypass our terms of service by immediately issuing delete requests on all generated answers. For enterprise customers these terms can be negotiated. In terms of specifics on which LLM\u2019s and sub-processors we use, we can have those conversations as needed under NDA with enterprise customers. The short answer is we use multiple, some internally developed, some provided by third part sub-processors. For enterprise customers there is also a pathway to data isolation, as we are open to offering dedicated instances / storage. For more information, please visit https://neuralseek.com/eula","title":"Overview"},{"location":"how_to_use/","text":"Available Cloud Platforms NeuralSeek is a SaaS solution. The most popular and easiest way to use NeuralSeek is to use it either in IBM cloud or Amazon Web Services (AWS). If you have a cloud account in either IBM cloud or AWS, you can quickly provision an instance of NeuralSeek today. IBM cloud Visit https://cloud.ibm.com/catalog/services/neuralseek for details about NeuralSeek\u2019s available plan, features, and other resources. AWS marketplace Visit https://aws.amazon.com/marketplace/pp/prodview-d7cymwnii2xrq for details about NeuralSeek\u2019s available plan, features, and other resources. Videos https://www.youtube.com/@Cerebral_Blue/featured : There are many helpful videos available to learn about NeuralSeek and its features. Demos Sign up for a demo which can be a fast and easy way to learn how NeuralSeek works. NeuralSeek team can schedule and demonstrate the key features of NeuralSeek according to your convenience, and also answer any questions that you may have regarding the product. Schedule a demo today, at https://neuralseek.com/demo . Training https://academy.neuralseek.com/ is the website that hosts NeuralSeek academy, where you can go through a self-pace training to study and learn about its features via tutorials, hands-on-lab, and additional resources. Please reach out to support@neuralseek.com to access the academy. Use Cases Virtual Agent/Chatbot NeuralSeek can integrate with Virtual Agents/Chatbots such as IBM Watson Assistant or AWS Lex to provide a tool to automate and improve the customer care process. NeuralSeek can allow Virtual Agents to handle a customer care process, only delegating to a Live Agent if necessary. NeuralSeek with a virtual agent can be used as an internal tool as well to provide a corporate KnowledgeBase-backed solution to assist teams in decision making. Internal Organization Tool NeuralSeek can be used as an internal organization tool for companies with large amounts of data/documentation to sort through. Through the \u201cSeek\u201d, \u201cCurate\u201d, and \u201cAnalytics\u201d section, NeuralSeek allows a company to share information from a virtual agent to an employee/user without delegating to live agents in the business. NeuralSeek provides managers a solution to aid their employees when they feel as though they don\u2019t have the bandwidth to support large & vastly unique demographics. It maintains conversational context to provide users with concrete answers to each and every question that they present to the virtual agent. Internal Content Managing The NeuralSeek \"Explore\" feature is a verstile tool designed to make the most of Large Language Models (LLMs) in a user-friendly way. It serves as an internal content manager, offering the choice of LLM, NeuralSeek Template Language for queries, versatile data retrieval, content enhancement, guided prompts, and effortless output. \"Explore\" is your go-to tool for managing and improving content within your organization using the power of LLMs.","title":"How to Use"},{"location":"how_to_use/#available-cloud-platforms","text":"NeuralSeek is a SaaS solution. The most popular and easiest way to use NeuralSeek is to use it either in IBM cloud or Amazon Web Services (AWS). If you have a cloud account in either IBM cloud or AWS, you can quickly provision an instance of NeuralSeek today. IBM cloud Visit https://cloud.ibm.com/catalog/services/neuralseek for details about NeuralSeek\u2019s available plan, features, and other resources. AWS marketplace Visit https://aws.amazon.com/marketplace/pp/prodview-d7cymwnii2xrq for details about NeuralSeek\u2019s available plan, features, and other resources.","title":"Available Cloud Platforms"},{"location":"how_to_use/#videos","text":"https://www.youtube.com/@Cerebral_Blue/featured : There are many helpful videos available to learn about NeuralSeek and its features.","title":"Videos"},{"location":"how_to_use/#demos","text":"Sign up for a demo which can be a fast and easy way to learn how NeuralSeek works. NeuralSeek team can schedule and demonstrate the key features of NeuralSeek according to your convenience, and also answer any questions that you may have regarding the product. Schedule a demo today, at https://neuralseek.com/demo .","title":"Demos"},{"location":"how_to_use/#training","text":"https://academy.neuralseek.com/ is the website that hosts NeuralSeek academy, where you can go through a self-pace training to study and learn about its features via tutorials, hands-on-lab, and additional resources. Please reach out to support@neuralseek.com to access the academy.","title":"Training"},{"location":"how_to_use/#use-cases","text":"","title":"Use Cases"},{"location":"how_to_use/#virtual-agentchatbot","text":"NeuralSeek can integrate with Virtual Agents/Chatbots such as IBM Watson Assistant or AWS Lex to provide a tool to automate and improve the customer care process. NeuralSeek can allow Virtual Agents to handle a customer care process, only delegating to a Live Agent if necessary. NeuralSeek with a virtual agent can be used as an internal tool as well to provide a corporate KnowledgeBase-backed solution to assist teams in decision making.","title":"Virtual Agent/Chatbot"},{"location":"how_to_use/#internal-organization-tool","text":"NeuralSeek can be used as an internal organization tool for companies with large amounts of data/documentation to sort through. Through the \u201cSeek\u201d, \u201cCurate\u201d, and \u201cAnalytics\u201d section, NeuralSeek allows a company to share information from a virtual agent to an employee/user without delegating to live agents in the business. NeuralSeek provides managers a solution to aid their employees when they feel as though they don\u2019t have the bandwidth to support large & vastly unique demographics. It maintains conversational context to provide users with concrete answers to each and every question that they present to the virtual agent.","title":"Internal Organization Tool"},{"location":"how_to_use/#internal-content-managing","text":"The NeuralSeek \"Explore\" feature is a verstile tool designed to make the most of Large Language Models (LLMs) in a user-friendly way. It serves as an internal content manager, offering the choice of LLM, NeuralSeek Template Language for queries, versatile data retrieval, content enhancement, guided prompts, and effortless output. \"Explore\" is your go-to tool for managing and improving content within your organization using the power of LLMs.","title":"Internal Content Managing"},{"location":"how-tos/backup_and_restore/backup_and_restore/","text":"Overview This document outlines available options for backing up and restoring NeuralSeek instance data. Backup / Restore Settings Open NeuralSeek's Configure tab, and select \"Show advanced options\" (if not already shown): From here, you are now offered the option to download/backup and upload/restore your instance settings. Curated Data (Backup) Open NeuralSeek's Curate tab Select some, or all, curated intents to backup As seen above, upon selecting curated intents, you are offered a \"Download to CSV\" button. This is useful not only for backing up your curated data, but also for allowing subject matter experts to edit curated Q&A content without direct access to the UI. After editing, you're able to re-upload the curated content in the next set of steps (Restore). Curated Data (Restore) When no intents are selected, you are offered a \"Load Q&A\" button near the top-right: This takes us to the Q&A Upload page: From here, we are able to upload a Curated Q&A CSV file (downloaded from previous steps). For Restoring purposes, you will not want to use \"Improve my answers\". Data Policy All user data and generated answers are owned by and for the sole use of the customer. It is your responsibility to regularly backup curated content. There is no option to configure product availablilty at this time.","title":"Backup and Restore"},{"location":"how-tos/backup_and_restore/backup_and_restore/#overview","text":"This document outlines available options for backing up and restoring NeuralSeek instance data.","title":"Overview"},{"location":"how-tos/backup_and_restore/backup_and_restore/#backup-restore-settings","text":"Open NeuralSeek's Configure tab, and select \"Show advanced options\" (if not already shown): From here, you are now offered the option to download/backup and upload/restore your instance settings.","title":"Backup / Restore Settings"},{"location":"how-tos/backup_and_restore/backup_and_restore/#curated-data-backup","text":"Open NeuralSeek's Curate tab Select some, or all, curated intents to backup As seen above, upon selecting curated intents, you are offered a \"Download to CSV\" button. This is useful not only for backing up your curated data, but also for allowing subject matter experts to edit curated Q&A content without direct access to the UI. After editing, you're able to re-upload the curated content in the next set of steps (Restore).","title":"Curated Data (Backup)"},{"location":"how-tos/backup_and_restore/backup_and_restore/#curated-data-restore","text":"When no intents are selected, you are offered a \"Load Q&A\" button near the top-right: This takes us to the Q&A Upload page: From here, we are able to upload a Curated Q&A CSV file (downloaded from previous steps). For Restoring purposes, you will not want to use \"Improve my answers\".","title":"Curated Data (Restore)"},{"location":"how-tos/backup_and_restore/backup_and_restore/#data-policy","text":"All user data and generated answers are owned by and for the sole use of the customer. It is your responsibility to regularly backup curated content. There is no option to configure product availablilty at this time.","title":"Data Policy"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/","text":"Overview What is it? NeuralSeek will automatically generate IBM Watson Assistant \u201cActions\u201d or \u201cDialogs,\u201d based on user questions that are asked. Generally, IBM Watson Assistant needs five (5) or more user question examples to train on for a high confidence match to a user query. When user questions are cataloged by the system, NeuralSeek automatically tries to generate similar worded questions to meet the minimum of five (5) user examples. Similar Question generation may take up to one (1) minute to show inside the Curate tab after a new user question is logged. Why is it important? Users who develop and maintain Watson Assistant have to work with its Actions and Dialogs, and can quickly get overwhelmed by its vast numbers. Coming up with multiple number of questions for each intent is also very time consuming, but it also quickly becomes burdensome when you have to continuously monitor and update them by yourself. How does it work? NeuralSeek provides ways to generate the candiate questions and answers based on the contents inside the KnowledgeBase, and let users download the whole thing or portions of it, so that it could be created either as 1) Watson Assistant Actions, or 2) Watson Assistant Dialogs. Generating Questions and Answers After you have configured NeuralSeek, in its home , you will see an option to auto-generate questions. Clicking will let NeuralSeek scan through your KnowledgeBase, and start generating potential questions that would be most commonly used. The resulting list of questions appear at the bottom. If you do not like the list of questions, you can re-generate them again, or edit them on the spot. When you feel like you can generate the Answers for those questions, you can click Submit button and those questions will be available on the curate tab of the top menu. Usually the most recently entered questions and answers appear at the top: Testing Questions During the curation process, usually the user would need to use Seek tab to submit questions to see how well the answer is generated. However, this process can be tedious if you have a certain set of questions that you want to ask in bulk and derive the results. In that case, you can use Upload Test Questions to upload multiple questions and generate their answers easily. Go to Home of NeuralSeek, and click Upload Test Questions . In the instructions, you will see a link of template file that you can download from. It's a template file in CSV format. click it to download. Use the file to enter the list of questions. For example, ID,Question 1,\"What are the main features of NeuralSeek?\" 2,\"What are the knowledgebases supported by NeuralSeek?\" 3,\"I want to integration NeuralSeek with Watson Assistant. What do I need to do?\" 4,\"Where can I see the demo?\" Click the upload button to upload the file. Click Submit button. NeuralSeek will run through the questions and let you know how many are being processed. When it is finsihed it When finished, you can either Download the report, Export All Q&A, or Delete the generated report. Download the report: it will give you a CSV file that has the following columns: ID,question,score,semanticScore,kbCoverage,totalCount,url,document,answer,categoryId,category,intent,pii,sentiment which will give you the answer and score of how well it got generated. Export All Q&A: it will export all the Q&A currently stored in NeuralSeek, in JSON format suitable to be imported as Watson Assistant Actions. Delete Report: it will delete the generated report, and will not be available anymore. Uploading Curated Q/A This feature is very similar to Upload Test Questions , but uses the CSV format that has ID,Question,Answer . User can create question and answer pairs to submit it, which will then be populated as edited answers in NeuralSeek. This feature is useful when you need to edit and upload answers in bulk fashion. An example format of the CSV is as follows: ID,Question,Answer 1,Tell me about NeralSeek,\"NeuralSeek is an AI-powered platform that generates natural-language answers to complex, open-ended, and contextual questions from real customers.\" Importing Q/A into Watson Assistant Depending on how your NeuralSeek is setup, it can either product questions and answers into Action type or Dialog type. That depends on whether your Watson Assistant is enabled with dialog or not. Importing into Watson Assistant as Actions \ud83d\udc49 As for importing Q&A into Watson Assistant, you can do it on both Watson Assistant Classic mode or new Dialog mode. 1. Go to your Watson Assistant, and to go to Actions . Click the gear icon on top right to go into the settings. 2. In the global settings, move to the right most tab which is Upload/Download , and click Download button to download the action's JSON file. 3. A JSON file should be saved. 4. Go to NeuralSeek, click Curate tab. 5. Click Import Base Watson Assistant Actions . 6. Upload the downloaded JSON file. 7. Now, select one or more intents which you want to import into Watson Assistant. You will notice a new button is display which is Export to Watson Assistant Actions . 8. It will download a JSON file called actions.json which will contain the selected intents that you want to convert it into Watson Assistant Actions. 9. Go to Watson Assistant. At the same page where you just downloaded the JSON, click to select a file, and select the actions.json and click Upload button. 10. You will see a warning message. Click Upload and replace 11. Now, close this page, and you will see the exported actions appear on your actions list. 12. Click one of the actions. You should be able to see the list of the quesitons generated by NeuralSeek nicely populated. With these, you can easy save time to jump start Watson Assistant to provide better answers to the questions and answers generated by NeuralSeek. One other nice thing about this is that if you find any particular questions and answers that does not yet exist in Watson Assistant, you can easily move them from NeuralSeek. Importing into Watson Assistant as Dialogs Unlike importing them as Actions, you first need to export your Watson Assistant's dialogs and set them as Base Watson Assistant Dialog into NeuralSeek. That is because Watson Assistant, when uploading a Dialog, would simply override the existing dialog and upload a new one. In order to make sure any existing actions or dialogs are not deleted, NeuralSeek needs to have it first, and then merge the dialogs into it. Go to your Watson Assistant, and to go Dialog > Options > Upload / Download : Click Download tab and click Doanload button: A JSON file should be downloaded. Now go to NeuralSeek, and go to Curate tab. Click Import Base Watson Assistant Dialog button. Select the downloaded JSON file. The button will now be turned to Base Watson Assistant Dialog Uploaded . \u26a0\ufe0f whenever there is a change of your Watson Assistant Dialog, make sure to delete the older one and upload the recent one in order to not risk losing your most up-to-date dialogs. Now, select the list of questions that you want to load it into. As soon as you select them, a new button Export to Watson Assistant Dialog will appear. You can obviously select all the questions by checking the all box at top left. Click the button to export these dialogs. Now, a JSON file should be downloaded. Load the file back into Watson Assistant using its upload tab. Note that uploading this JSON will overwrite any existing dialog contents. Click Upload and replace . If everything goes well, it will say the skills were uploaded successfully. You now have the curated answer from NeuralSeek populated as a Dialog node in Watson Assistant. Next time when the user asks the same question, Watson Assistant should be able to answer it the same way as NeuralSeek did. This is a great way to effectively manage some of the most frequent questions and answers that you uncover from NeuralSeek to be able to be transferred into the Virtual Agent's dialog, such that it will be able to be trained with better set of answers.","title":"Training Virtual Agents"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#overview","text":"What is it? NeuralSeek will automatically generate IBM Watson Assistant \u201cActions\u201d or \u201cDialogs,\u201d based on user questions that are asked. Generally, IBM Watson Assistant needs five (5) or more user question examples to train on for a high confidence match to a user query. When user questions are cataloged by the system, NeuralSeek automatically tries to generate similar worded questions to meet the minimum of five (5) user examples. Similar Question generation may take up to one (1) minute to show inside the Curate tab after a new user question is logged. Why is it important? Users who develop and maintain Watson Assistant have to work with its Actions and Dialogs, and can quickly get overwhelmed by its vast numbers. Coming up with multiple number of questions for each intent is also very time consuming, but it also quickly becomes burdensome when you have to continuously monitor and update them by yourself. How does it work? NeuralSeek provides ways to generate the candiate questions and answers based on the contents inside the KnowledgeBase, and let users download the whole thing or portions of it, so that it could be created either as 1) Watson Assistant Actions, or 2) Watson Assistant Dialogs.","title":"Overview"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#generating-questions-and-answers","text":"After you have configured NeuralSeek, in its home , you will see an option to auto-generate questions. Clicking will let NeuralSeek scan through your KnowledgeBase, and start generating potential questions that would be most commonly used. The resulting list of questions appear at the bottom. If you do not like the list of questions, you can re-generate them again, or edit them on the spot. When you feel like you can generate the Answers for those questions, you can click Submit button and those questions will be available on the curate tab of the top menu. Usually the most recently entered questions and answers appear at the top:","title":"Generating Questions and Answers"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#testing-questions","text":"During the curation process, usually the user would need to use Seek tab to submit questions to see how well the answer is generated. However, this process can be tedious if you have a certain set of questions that you want to ask in bulk and derive the results. In that case, you can use Upload Test Questions to upload multiple questions and generate their answers easily. Go to Home of NeuralSeek, and click Upload Test Questions . In the instructions, you will see a link of template file that you can download from. It's a template file in CSV format. click it to download. Use the file to enter the list of questions. For example, ID,Question 1,\"What are the main features of NeuralSeek?\" 2,\"What are the knowledgebases supported by NeuralSeek?\" 3,\"I want to integration NeuralSeek with Watson Assistant. What do I need to do?\" 4,\"Where can I see the demo?\" Click the upload button to upload the file. Click Submit button. NeuralSeek will run through the questions and let you know how many are being processed. When it is finsihed it When finished, you can either Download the report, Export All Q&A, or Delete the generated report. Download the report: it will give you a CSV file that has the following columns: ID,question,score,semanticScore,kbCoverage,totalCount,url,document,answer,categoryId,category,intent,pii,sentiment which will give you the answer and score of how well it got generated. Export All Q&A: it will export all the Q&A currently stored in NeuralSeek, in JSON format suitable to be imported as Watson Assistant Actions. Delete Report: it will delete the generated report, and will not be available anymore.","title":"Testing Questions"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#uploading-curated-qa","text":"This feature is very similar to Upload Test Questions , but uses the CSV format that has ID,Question,Answer . User can create question and answer pairs to submit it, which will then be populated as edited answers in NeuralSeek. This feature is useful when you need to edit and upload answers in bulk fashion. An example format of the CSV is as follows: ID,Question,Answer 1,Tell me about NeralSeek,\"NeuralSeek is an AI-powered platform that generates natural-language answers to complex, open-ended, and contextual questions from real customers.\"","title":"Uploading Curated Q/A"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#importing-qa-into-watson-assistant","text":"Depending on how your NeuralSeek is setup, it can either product questions and answers into Action type or Dialog type. That depends on whether your Watson Assistant is enabled with dialog or not.","title":"Importing Q/A into Watson Assistant"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#importing-into-watson-assistant-as-actions","text":"\ud83d\udc49 As for importing Q&A into Watson Assistant, you can do it on both Watson Assistant Classic mode or new Dialog mode. 1. Go to your Watson Assistant, and to go to Actions . Click the gear icon on top right to go into the settings. 2. In the global settings, move to the right most tab which is Upload/Download , and click Download button to download the action's JSON file. 3. A JSON file should be saved. 4. Go to NeuralSeek, click Curate tab. 5. Click Import Base Watson Assistant Actions . 6. Upload the downloaded JSON file. 7. Now, select one or more intents which you want to import into Watson Assistant. You will notice a new button is display which is Export to Watson Assistant Actions . 8. It will download a JSON file called actions.json which will contain the selected intents that you want to convert it into Watson Assistant Actions. 9. Go to Watson Assistant. At the same page where you just downloaded the JSON, click to select a file, and select the actions.json and click Upload button. 10. You will see a warning message. Click Upload and replace 11. Now, close this page, and you will see the exported actions appear on your actions list. 12. Click one of the actions. You should be able to see the list of the quesitons generated by NeuralSeek nicely populated. With these, you can easy save time to jump start Watson Assistant to provide better answers to the questions and answers generated by NeuralSeek. One other nice thing about this is that if you find any particular questions and answers that does not yet exist in Watson Assistant, you can easily move them from NeuralSeek.","title":"Importing into Watson Assistant as Actions"},{"location":"how-tos/training_virtual_agents/training_virtual_agents/#importing-into-watson-assistant-as-dialogs","text":"Unlike importing them as Actions, you first need to export your Watson Assistant's dialogs and set them as Base Watson Assistant Dialog into NeuralSeek. That is because Watson Assistant, when uploading a Dialog, would simply override the existing dialog and upload a new one. In order to make sure any existing actions or dialogs are not deleted, NeuralSeek needs to have it first, and then merge the dialogs into it. Go to your Watson Assistant, and to go Dialog > Options > Upload / Download : Click Download tab and click Doanload button: A JSON file should be downloaded. Now go to NeuralSeek, and go to Curate tab. Click Import Base Watson Assistant Dialog button. Select the downloaded JSON file. The button will now be turned to Base Watson Assistant Dialog Uploaded . \u26a0\ufe0f whenever there is a change of your Watson Assistant Dialog, make sure to delete the older one and upload the recent one in order to not risk losing your most up-to-date dialogs. Now, select the list of questions that you want to load it into. As soon as you select them, a new button Export to Watson Assistant Dialog will appear. You can obviously select all the questions by checking the all box at top left. Click the button to export these dialogs. Now, a JSON file should be downloaded. Load the file back into Watson Assistant using its upload tab. Note that uploading this JSON will overwrite any existing dialog contents. Click Upload and replace . If everything goes well, it will say the skills were uploaded successfully. You now have the curated answer from NeuralSeek populated as a Dialog node in Watson Assistant. Next time when the user asks the same question, Watson Assistant should be able to answer it the same way as NeuralSeek did. This is a great way to effectively manage some of the most frequent questions and answers that you uncover from NeuralSeek to be able to be transferred into the Virtual Agent's dialog, such that it will be able to be trained with better set of answers.","title":"Importing into Watson Assistant as Dialogs"},{"location":"integrations/integrations/","text":"Integrations NeuralSeek offers integrations with various platforms and tools to enhance their functionalities. These integrations include Watson Assistant Custom Extension, Corporate KnowledgeBase integrations such as Watson Discovery and Elastic AppSearch, and cloud platform integrations with IBM Cloud and Amazon Web Services (AWS). NeuralSeek also provides REST API and WebHooks for any compatible applications to be able to invoke its services easily. List of NeuralSeek integrations:","title":"Integrations"},{"location":"integrations/integrations/#integrations","text":"NeuralSeek offers integrations with various platforms and tools to enhance their functionalities. These integrations include Watson Assistant Custom Extension, Corporate KnowledgeBase integrations such as Watson Discovery and Elastic AppSearch, and cloud platform integrations with IBM Cloud and Amazon Web Services (AWS). NeuralSeek also provides REST API and WebHooks for any compatible applications to be able to invoke its services easily.","title":"Integrations"},{"location":"integrations/integrations/#list-of-neuralseek-integrations","text":"","title":"List of NeuralSeek integrations:"},{"location":"integrations/lexv2_lambda/lexv2_lambda/","text":"Overview Use the pre-configured AWS Lambda Archive to setup and send user input that are routed through the Lex FallbackIntent to NeuralSeek. Set by step instruction Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file. Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your API key and instance URL. Click \"Deploy\". Select \"Configuration\" tab. Select \"General Configuration\" pane. Click \"Edit\". Update under Timeout, set min to be 1 and sec to be 0. Click \"Save\". Open the Amazon Lex console at https://console.aws.amazon.com/lexv2/home#bots From the list of bots, choose the name of the bot that you want to use. Under \"Deployment\" on the left panel select \"Aliases\". From the list of aliases, choose the name of the alias that you want to use. From the list of supported languages, choose the language that the Lambda function is used for. Choose the name of the Lambda function to use, then choose the version or alias of the function. Choose Save to save your changes. On the left panel under \"All languages\", select \"Intents\" under the language that the fallback intent is used for. Select \"FallbackIntent\". Under \"Fulfillment\" set to Active by clicking the select box next to \"Active\". Under \"Fulfillment\" Click \"Advanced options\". Check the checkbox next to \"Use a Lambda function for fulfillment\". Click \"Updated options\". Click \"Save intent\". Click \"Build\" to build the bot.","title":"LexV2 Lambda"},{"location":"integrations/lexv2_lambda/lexv2_lambda/#overview","text":"Use the pre-configured AWS Lambda Archive to setup and send user input that are routed through the Lex FallbackIntent to NeuralSeek.","title":"Overview"},{"location":"integrations/lexv2_lambda/lexv2_lambda/#set-by-step-instruction","text":"Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file. Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your API key and instance URL. Click \"Deploy\". Select \"Configuration\" tab. Select \"General Configuration\" pane. Click \"Edit\". Update under Timeout, set min to be 1 and sec to be 0. Click \"Save\". Open the Amazon Lex console at https://console.aws.amazon.com/lexv2/home#bots From the list of bots, choose the name of the bot that you want to use. Under \"Deployment\" on the left panel select \"Aliases\". From the list of aliases, choose the name of the alias that you want to use. From the list of supported languages, choose the language that the Lambda function is used for. Choose the name of the Lambda function to use, then choose the version or alias of the function. Choose Save to save your changes. On the left panel under \"All languages\", select \"Intents\" under the language that the fallback intent is used for. Select \"FallbackIntent\". Under \"Fulfillment\" set to Active by clicking the select box next to \"Active\". Under \"Fulfillment\" Click \"Advanced options\". Check the checkbox next to \"Use a Lambda function for fulfillment\". Click \"Updated options\". Click \"Save intent\". Click \"Build\" to build the bot.","title":"Set by step instruction"},{"location":"integrations/lexv2_logs/lexv2_logs/","text":"Overview Use the AWS Lambda archive to send user input that routes to the Lex FallbackIntent to NeuralSeek. Step by step instruction Navigate to Amazon Cloudwatch Log Groups. Make sure you are in the same region as your bot. Click \"Create log group\". Give the log group a name (this will be logs of your bot's chat). Click \"Create\". Open the Amazon LexV2 console. From the list, choose a bot and select the link on the name. From the left menu, choose Aliases. In the list of aliases, choose the alias for which you want to configure conversation logs. In the Conversation logs section, choose Manage conversation logs. For text logs, choose Enable then enter the log group name from the group you created earlier. Choose Save to start logging conversations. If necessary, Amazon LexV2 will update your service role with permissions to access the CloudWatch Logs log group and selected S3 bucket. Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file (upload the zip file from the bottom of the page). Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your API key and instance URL LogGroupName is the name you gave to the log group that you created earlier. Click \"Deploy\" Select \"Configuration\" tab Select \"General Configuration\" pane Click \"Edit\" Update under Timeout, set min to be 5 and sec to be 0 Click \"Save\" Select \"Permissions\" pane Click on the link under \"Role name\" Click \"Add permissions\" Click \"Attach policies\" Search for \"CloudWatchReadOnlyAccess\" Select the box next to \"CloudWatchReadOnlyAccess\" Click \"Add permissions\" Navigate to the Amazon EventBridge Rules page. Click \"Create rule\" Give the rule a name. Select \"Schedule\" under Rule type. Click \"Continue to create rule\". Select \"A schedule that runs at a regular rate, such as every 10 minutes.\" under Schedule pattern. Under Rate expression set the \"Value\" to be 15 and the \"Unit, e.g. mins, hours...\" to be Minutes. Click \"Next\". Under \"Select a target\" select Lambda function. Under \"Function\" select the lambda we created earlier. Click \"Next\". Click \"Next\". Click \"Create rule\". Validate the connection by triggering an intent in the language tab in AWS LexV2. You will see a icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek after the next Amazon EventBridge run (the rule will run every 15 minutes).","title":"LexV2 Logs"},{"location":"integrations/lexv2_logs/lexv2_logs/#overview","text":"Use the AWS Lambda archive to send user input that routes to the Lex FallbackIntent to NeuralSeek.","title":"Overview"},{"location":"integrations/lexv2_logs/lexv2_logs/#step-by-step-instruction","text":"Navigate to Amazon Cloudwatch Log Groups. Make sure you are in the same region as your bot. Click \"Create log group\". Give the log group a name (this will be logs of your bot's chat). Click \"Create\". Open the Amazon LexV2 console. From the list, choose a bot and select the link on the name. From the left menu, choose Aliases. In the list of aliases, choose the alias for which you want to configure conversation logs. In the Conversation logs section, choose Manage conversation logs. For text logs, choose Enable then enter the log group name from the group you created earlier. Choose Save to start logging conversations. If necessary, Amazon LexV2 will update your service role with permissions to access the CloudWatch Logs log group and selected S3 bucket. Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file (upload the zip file from the bottom of the page). Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your API key and instance URL LogGroupName is the name you gave to the log group that you created earlier. Click \"Deploy\" Select \"Configuration\" tab Select \"General Configuration\" pane Click \"Edit\" Update under Timeout, set min to be 5 and sec to be 0 Click \"Save\" Select \"Permissions\" pane Click on the link under \"Role name\" Click \"Add permissions\" Click \"Attach policies\" Search for \"CloudWatchReadOnlyAccess\" Select the box next to \"CloudWatchReadOnlyAccess\" Click \"Add permissions\" Navigate to the Amazon EventBridge Rules page. Click \"Create rule\" Give the rule a name. Select \"Schedule\" under Rule type. Click \"Continue to create rule\". Select \"A schedule that runs at a regular rate, such as every 10 minutes.\" under Schedule pattern. Under Rate expression set the \"Value\" to be 15 and the \"Unit, e.g. mins, hours...\" to be Minutes. Click \"Next\". Under \"Select a target\" select Lambda function. Under \"Function\" select the lambda we created earlier. Click \"Next\". Click \"Next\". Click \"Create rule\". Validate the connection by triggering an intent in the language tab in AWS LexV2. You will see a icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek after the next Amazon EventBridge run (the rule will run every 15 minutes).","title":"Step by step instruction"},{"location":"integrations/rest_api/rest_api/","text":"Overview Virtual Agents, chatbots, and applications can send user questions and receive answers via NeuralSeek\u2019s REST API. In the Integrate > API , you can access its openAPI documentation that covers its service endpoints, and also can test its executions, as well as access the message schema. For more information, please refer to https://api.neuralseek.com/ . Example of curl command to invoke REST API curl -X 'POST' \\ 'http://localhost:2755/v1/test/seek' \\ -H 'accept: application/json' \\ -H 'apikey: xxxbbfxf-xxx84x90-xxxa7ex3-xxxba06x' \\ -H 'Content-Type: application/json' \\ -d '{ \"question\": \"I wan to know more about NeuralSeek\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, \"options\": { \"personalize\": { \"preferredName\": \"string\", \"noWelcome\": \"string\", \"forceFirstPerson\": \"string\", \"products\": [ \"string\" ], \"additionalDetails\": \"string\" }, \"language\": \"string\", \"filter\": \"string\", \"company\": \"string\", \"semanticScore\": { \"enabled\": \"string\", \"primary\": \"string\" }, \"lastTurn\": { \"input\": \"string\", \"response\": \"string\" }, \"pullToCompany\": \"string\", \"promptEngineering\": \"string\", \"promptEngineeringPhrase\": \"string\", \"answerEngineering\": \"[{'\\''re'\\'':'\\''/hi/gi'\\'', '\\''rp'\\'':'\\''Bye!'\\''}]\", \"warnConfidence\": \"5\", \"warnConfidenceText\": \"string\", \"minConfidence\": \"0\", \"minConfidenceText\": \"string\", \"misTolerance\": \"0\", \"sensitiveText\": \"string\", \"minText\": \"string\", \"maxWords\": \"string\", \"url\": \"string\", \"stump\": \"string\", \"includeSourceResults\": \"string\" } }' Example of JSON Response { \"answer\": \"Hello string,\\n\\nNeuralSeek is an AI-powered Answers-as-a-Service, designed to enhance information sharing and customer support within organizations\u2019 virtual agents. It works by leveraging the capabilities of a sophisticated Large Language Model and the users\u2019 corporate KnowledgeBase, allowing virtual agents to provide concise and contextually relevant responses to user queries. \\n\\nNeuralSeek offers a clickable path to fact-check AI response, utilization of data analytics to enhance AI natural language capabilities and comprehensive step-by-step instructions for maintaining accuracy and clean resource data. It also features an advanced Personal Identifiable Information (PII) detection routine that automatically identifies any PII within user inputs. \\n\\nIt was created in October of 2022, by CerebralBlue. It can be used either in IBM cloud or Amazon Web Services (AWS).\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 5, \"totalCount\": 25, \"KBscore\": 100, \"score\": 100, \"url\": \"https://uneven-base-562.com/Overview-4faf22fbaa3343fba8a4d0594dc7789\", \"document\": \"https://uneven-base-562.com/Overview-4faf22fbaa3343fba8a4d0594dc7789\", \"kbTime\": 6173, \"kbCoverage\": 88, \"time\": 20928 }","title":"REST API"},{"location":"integrations/rest_api/rest_api/#overview","text":"Virtual Agents, chatbots, and applications can send user questions and receive answers via NeuralSeek\u2019s REST API. In the Integrate > API , you can access its openAPI documentation that covers its service endpoints, and also can test its executions, as well as access the message schema. For more information, please refer to https://api.neuralseek.com/ .","title":"Overview"},{"location":"integrations/rest_api/rest_api/#example-of-curl-command-to-invoke-rest-api","text":"curl -X 'POST' \\ 'http://localhost:2755/v1/test/seek' \\ -H 'accept: application/json' \\ -H 'apikey: xxxbbfxf-xxx84x90-xxxa7ex3-xxxba06x' \\ -H 'Content-Type: application/json' \\ -d '{ \"question\": \"I wan to know more about NeuralSeek\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, \"options\": { \"personalize\": { \"preferredName\": \"string\", \"noWelcome\": \"string\", \"forceFirstPerson\": \"string\", \"products\": [ \"string\" ], \"additionalDetails\": \"string\" }, \"language\": \"string\", \"filter\": \"string\", \"company\": \"string\", \"semanticScore\": { \"enabled\": \"string\", \"primary\": \"string\" }, \"lastTurn\": { \"input\": \"string\", \"response\": \"string\" }, \"pullToCompany\": \"string\", \"promptEngineering\": \"string\", \"promptEngineeringPhrase\": \"string\", \"answerEngineering\": \"[{'\\''re'\\'':'\\''/hi/gi'\\'', '\\''rp'\\'':'\\''Bye!'\\''}]\", \"warnConfidence\": \"5\", \"warnConfidenceText\": \"string\", \"minConfidence\": \"0\", \"minConfidenceText\": \"string\", \"misTolerance\": \"0\", \"sensitiveText\": \"string\", \"minText\": \"string\", \"maxWords\": \"string\", \"url\": \"string\", \"stump\": \"string\", \"includeSourceResults\": \"string\" } }'","title":"Example of curl command to invoke REST API"},{"location":"integrations/rest_api/rest_api/#example-of-json-response","text":"{ \"answer\": \"Hello string,\\n\\nNeuralSeek is an AI-powered Answers-as-a-Service, designed to enhance information sharing and customer support within organizations\u2019 virtual agents. It works by leveraging the capabilities of a sophisticated Large Language Model and the users\u2019 corporate KnowledgeBase, allowing virtual agents to provide concise and contextually relevant responses to user queries. \\n\\nNeuralSeek offers a clickable path to fact-check AI response, utilization of data analytics to enhance AI natural language capabilities and comprehensive step-by-step instructions for maintaining accuracy and clean resource data. It also features an advanced Personal Identifiable Information (PII) detection routine that automatically identifies any PII within user inputs. \\n\\nIt was created in October of 2022, by CerebralBlue. It can be used either in IBM cloud or Amazon Web Services (AWS).\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 5, \"totalCount\": 25, \"KBscore\": 100, \"score\": 100, \"url\": \"https://uneven-base-562.com/Overview-4faf22fbaa3343fba8a4d0594dc7789\", \"document\": \"https://uneven-base-562.com/Overview-4faf22fbaa3343fba8a4d0594dc7789\", \"kbTime\": 6173, \"kbCoverage\": 88, \"time\": 20928 }","title":"Example of JSON Response"},{"location":"integrations/supported_knowledgebases/supported_knowledgebases/","text":"Overview NeuralSeek supports the following KnowledgeBases: Watson Discovery ( https://cloud.ibm.com/docs/discovery-data?topic=discovery-data-about ) Elastic AppSearch ( https://www.elastic.co/guide/en/app-search/current/index.html ) Amazon Kendra Support ( https://aws.amazon.com/kendra/ ) OpenSearch ( https://opensearch.org/ ) Setting up the integration to KnowledgeBase is done in Configure > Corporate Knowledge Base Details page. Step by step instruction In NeuralSeek UI, navigate to Configure > Corporate Knowledge Base Details page, using the top menu. Select the KnowledgeBase type: Watson Discovery Watson Discovery CP4D (CloudPak for Data) Elastic AppSearch AWS Kendra OpenSearch Enter necessary information such as service URL, API Key, and additional information like project ID, etc. Click Save.","title":"Supported KnowledgeBases"},{"location":"integrations/supported_knowledgebases/supported_knowledgebases/#overview","text":"NeuralSeek supports the following KnowledgeBases: Watson Discovery ( https://cloud.ibm.com/docs/discovery-data?topic=discovery-data-about ) Elastic AppSearch ( https://www.elastic.co/guide/en/app-search/current/index.html ) Amazon Kendra Support ( https://aws.amazon.com/kendra/ ) OpenSearch ( https://opensearch.org/ ) Setting up the integration to KnowledgeBase is done in Configure > Corporate Knowledge Base Details page.","title":"Overview"},{"location":"integrations/supported_knowledgebases/supported_knowledgebases/#step-by-step-instruction","text":"In NeuralSeek UI, navigate to Configure > Corporate Knowledge Base Details page, using the top menu. Select the KnowledgeBase type: Watson Discovery Watson Discovery CP4D (CloudPak for Data) Elastic AppSearch AWS Kendra OpenSearch Enter necessary information such as service URL, API Key, and additional information like project ID, etc. Click Save.","title":"Step by step instruction"},{"location":"integrations/supported_llms/supported_llms/","text":"Overview NeuralSeek supports the following Large Language Models (LLMs): Amazon Bedrock - Claude Instant v1.1 Amazon Bedrock - Claude v1.3 Amazon Bedrock - Claude v2 Amazon Bedrock - Jurassic-2 Mid Amazon Bedrock - Jurassic-2 Ultra Amazon Bedrock - Titan Azure Cognitive Services - GPT3.5 Azure Cognitive Services - GPT4 Azure Cognitive Services - GPT4 (32K) HuggingFace - Flan-t5-xxl HuggingFace - Flan-ul2 HuggingFace - Llama-2 HuggingFace - Llama-2-chat HuggingFace - Mistral-7B-Instruct HuggingFace - MPT-7B-instruct OpenAI - GPT3.5 OpenAI - GPT3.5 (16K) OpenAI - GPT4 OpenAI - GPT4 (32K) Self-Hosted - Flan-t5-xxl Self-Hosted - Flan-ul2 Self-Hosted - Llama-2 Self-Hosted - Llama-2-chat Self-Hosted - Mistral-7B-Instruct Self-Hosted - MPT-7B-instruct watsonx - Flan-t5-xxl watsonx - Flan-ul2 watsonx - granite-13b-chat-v1 watsonx - granite-13b-instruct-v1 watsonx - Llama-2-chat watsonx - MPT-7B-instruct2 watsonx (Tech Preview - Deprecated) - Flan-t5-xxl watsonx (Tech Preview - Deprecated) - Flan-ul2 watsonx (Tech Preview - Deprecated) - MPT-7B-instruct Not all LLMs are equal. The table below summarizes some of the capabilities for each model. LLM Capabilities / Notes Platform LLM Notes Amazon Bedrock Claude Instant v1.1 A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering. Amazon Bedrock Claude v1.3 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Amazon Bedrock Claude v2 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Amazon Bedrock Jurassic-2 Mid Jurassic-2 Mid is AI21\u2019s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others. Amazon Bedrock Jurassic-2 Ultra Jurassic-2 Ultra is AI21\u2019s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more. Amazon Bedrock Titan Amazon Titan Foundation Models are pretrained on large datasets, making them powerful, general-purpose models. Use them as is or customize them by fine tuning the models with your own data for a particular task without annotating large volumes of data Azure Cognitive Services GPT3.5 GPT-3.5 provides a good balance of speed and capability. Azure Cognitive Services GPT4 GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. Azure Cognitive Services GPT4 (32K) GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 32K context window version of the model allows for more information to be passed to it, generally yeilding better responses. HuggingFace Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. HuggingFace Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. HuggingFace Llama-2 Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) HuggingFace Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. HuggingFace Mistral-7B-Instruct Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the instruct version. HuggingFace MPT-7B-instruct The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. OpenAI GPT3.5 GPT-3.5 provides a good balance of speed and capability. OpenAI GPT3.5 (16K) GPT-3.5 provides a good balance of speed and capability. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses. OpenAI GPT4 GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. OpenAI GPT4 (32K) GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses. Self-Hosted Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. Self-Hosted Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. Self-Hosted Llama-2 Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) Self-Hosted Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. Self-Hosted Mistral-7B-Instruct Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents. Mistral operates well on single-GPU instances, and is generally stronger than other models in its class. This model is the instruct version. Self-Hosted MPT-7B-instruct The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. watsonx Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx granite-13b-chat-v1 The Granite series of models are a step ahead of their counterpart t5 and UL2 models. They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents. They do not have much ability to reason, however. This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation. Use semantic scoring to block this hallucination. watsonx granite-13b-instruct-v1 The Granite series of models are a step ahead of their counterpart t5 and UL2 models. They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents. They do not have much ability to reason, however. This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation. Use semantic scoring to block this hallucination. watsonx Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. watsonx MPT-7B-instruct2 The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. watsonx (Tech Preview - Deprecated) Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx (Tech Preview - Deprecated) Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx (Tech Preview - Deprecated) MPT-7B-instruct The mpt-7b-instruct model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. \ud83d\udca1 LLM choice is available with NeuralSeek\u2019s BYOLLM (bring your own Large Language Model) plan. \ud83d\udca1 LLMs can vary in their capabilities and performances. Some LLM can take up to 30 seconds and longer to generate a full response. Use caution when using in conjunction with a virtual agent platform that imposes a strict timeout. Step by step instruction \u26a0\ufe0f In order to configure a LLM, make sure that you have subscribed to the Bring Your Own LLM (BYOLLM) plan. All other plans will default to NeuralSeek's curated LLM, and this option will not be available. In NeuralSeek UI, navigate to Configure > LLM Details page, using the top menu. Click Add an LLM button. Select the Platform and LLM Selection. (e.g. Platform: Self-Hosted, LLM: Flan-u2) Click Add . Enter the LLM API key in the LLM API Key input field. Review the Enabled Languages (presented as multi-select) Review the LLM functions available (presented as checkbox) Click Test button to test whether the API key works. \ud83d\udca1 You must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled.","title":"Supported LLMs"},{"location":"integrations/supported_llms/supported_llms/#overview","text":"NeuralSeek supports the following Large Language Models (LLMs): Amazon Bedrock - Claude Instant v1.1 Amazon Bedrock - Claude v1.3 Amazon Bedrock - Claude v2 Amazon Bedrock - Jurassic-2 Mid Amazon Bedrock - Jurassic-2 Ultra Amazon Bedrock - Titan Azure Cognitive Services - GPT3.5 Azure Cognitive Services - GPT4 Azure Cognitive Services - GPT4 (32K) HuggingFace - Flan-t5-xxl HuggingFace - Flan-ul2 HuggingFace - Llama-2 HuggingFace - Llama-2-chat HuggingFace - Mistral-7B-Instruct HuggingFace - MPT-7B-instruct OpenAI - GPT3.5 OpenAI - GPT3.5 (16K) OpenAI - GPT4 OpenAI - GPT4 (32K) Self-Hosted - Flan-t5-xxl Self-Hosted - Flan-ul2 Self-Hosted - Llama-2 Self-Hosted - Llama-2-chat Self-Hosted - Mistral-7B-Instruct Self-Hosted - MPT-7B-instruct watsonx - Flan-t5-xxl watsonx - Flan-ul2 watsonx - granite-13b-chat-v1 watsonx - granite-13b-instruct-v1 watsonx - Llama-2-chat watsonx - MPT-7B-instruct2 watsonx (Tech Preview - Deprecated) - Flan-t5-xxl watsonx (Tech Preview - Deprecated) - Flan-ul2 watsonx (Tech Preview - Deprecated) - MPT-7B-instruct Not all LLMs are equal. The table below summarizes some of the capabilities for each model.","title":"Overview"},{"location":"integrations/supported_llms/supported_llms/#llm-capabilities-notes","text":"Platform LLM Notes Amazon Bedrock Claude Instant v1.1 A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering. Amazon Bedrock Claude v1.3 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Amazon Bedrock Claude v2 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Amazon Bedrock Jurassic-2 Mid Jurassic-2 Mid is AI21\u2019s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others. Amazon Bedrock Jurassic-2 Ultra Jurassic-2 Ultra is AI21\u2019s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more. Amazon Bedrock Titan Amazon Titan Foundation Models are pretrained on large datasets, making them powerful, general-purpose models. Use them as is or customize them by fine tuning the models with your own data for a particular task without annotating large volumes of data Azure Cognitive Services GPT3.5 GPT-3.5 provides a good balance of speed and capability. Azure Cognitive Services GPT4 GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. Azure Cognitive Services GPT4 (32K) GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 32K context window version of the model allows for more information to be passed to it, generally yeilding better responses. HuggingFace Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. HuggingFace Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. HuggingFace Llama-2 Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) HuggingFace Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. HuggingFace Mistral-7B-Instruct Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the instruct version. HuggingFace MPT-7B-instruct The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. OpenAI GPT3.5 GPT-3.5 provides a good balance of speed and capability. OpenAI GPT3.5 (16K) GPT-3.5 provides a good balance of speed and capability. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses. OpenAI GPT4 GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. OpenAI GPT4 (32K) GPT-4 can often take longer than 30 seconds for a full response. Use caution when using in conjunction with a Virtual Agent platform that imposes a strict timeout. The 16K context window version of the model allows for more information to be passed to it, generally yeilding better responses. Self-Hosted Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. Self-Hosted Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. Self-Hosted Llama-2 Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. This model is the non-chat version (Llama-2-7b-hf, Llama-2-13b-hf, Llama-2-70b-hf) Self-Hosted Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. Self-Hosted Mistral-7B-Instruct Mistral brings capabilities similar to many popular commercial models. Mistral is good at joining thoughts across multiple documents. Mistral operates well on single-GPU instances, and is generally stronger than other models in its class. This model is the instruct version. Self-Hosted MPT-7B-instruct The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. watsonx Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx granite-13b-chat-v1 The Granite series of models are a step ahead of their counterpart t5 and UL2 models. They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents. They do not have much ability to reason, however. This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation. Use semantic scoring to block this hallucination. watsonx granite-13b-instruct-v1 The Granite series of models are a step ahead of their counterpart t5 and UL2 models. They excel at retrieving correct information from good documentation, and can join phrases from a limited number of documents. They do not have much ability to reason, however. This can be good or bad, depending on your usecase. Use granite to answer a well defined set of questions from good documentation. Granite likes to generate short results, and will create runaway responses if pressed to generate longer than it wants to. Granite will hallucinate if asked questions without a good reference in your knowledgeBase, or that stray too closely to its training data, and may refuse to follow your documentation. Use semantic scoring to block this hallucination. watsonx Llama-2-chat Llama-2 brings capabilities similar to many popular commercial models. Llama-2 is good at joining thoughts across multiple documents. It is also highly sensitive. Slight variations in prompt and weighting can have a profound impact on usability of the system. Use extreme caution if applying prompt engineering or weight tuning. watsonx MPT-7B-instruct2 The mpt-7b-instruct2 model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. watsonx (Tech Preview - Deprecated) Flan-t5-xxl The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx (Tech Preview - Deprecated) Flan-ul2 The Flan models are primarily english-only, and may struggle with joining thoughts across multiple documents. You will find answers tend to be selected from a single source, even when a stitched answer may be better. Flan does suffer from strong hallucinations, so it is recommended to only use Flan for internal usecases and ensure the Semantic Scoring model is on and primary with a minimum confidence level set of at least 10-15%. watsonx (Tech Preview - Deprecated) MPT-7B-instruct The mpt-7b-instruct model can generate longer text than the Flan models. Use caution, however, as the model is prone to both extreme hallucination and runaway responses. Be sure to set a minimum confidence level to control this. Not reccomended for public usecases. \ud83d\udca1 LLM choice is available with NeuralSeek\u2019s BYOLLM (bring your own Large Language Model) plan. \ud83d\udca1 LLMs can vary in their capabilities and performances. Some LLM can take up to 30 seconds and longer to generate a full response. Use caution when using in conjunction with a virtual agent platform that imposes a strict timeout.","title":"LLM Capabilities / Notes"},{"location":"integrations/supported_llms/supported_llms/#step-by-step-instruction","text":"\u26a0\ufe0f In order to configure a LLM, make sure that you have subscribed to the Bring Your Own LLM (BYOLLM) plan. All other plans will default to NeuralSeek's curated LLM, and this option will not be available. In NeuralSeek UI, navigate to Configure > LLM Details page, using the top menu. Click Add an LLM button. Select the Platform and LLM Selection. (e.g. Platform: Self-Hosted, LLM: Flan-u2) Click Add . Enter the LLM API key in the LLM API Key input field. Review the Enabled Languages (presented as multi-select) Review the LLM functions available (presented as checkbox) Click Test button to test whether the API key works. \ud83d\udca1 You must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled.","title":"Step by step instruction"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/","text":"Overview NeuralSeek monitors curated responses between a user and a Virtual Agent by receiving logs from the supported Virtual Agents listed below: watsonx Assistant AWS Lex V2 Kore.ai Step by Step Instruction The Source Virtual Agent is connected to NeuralSeek via the specific instructions per platform on the \"Integrate\" tab in the NeuralSeek user interface. Watson Logs Enable Round-Trip monitoring on deployed NeuralSeek Intents. NeuralSeek will monitor the usage of NeuralSeek-curated intents within both the Actions and Dialogs frameworks of Watson Assistant, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. On the \"Environments\" tab of your Watson Assistant instance, click on the gear icon (near the upper left, with the Environment name). Under \"Webhooks\", select \"Log webhook\". Copy the URL and Secret key provided in the NeuralSeek user interface for webhook setup. Click the box for \"Subscribe to conversation logs\". Validate the connection by triggering an intent via the \"Preview\" tab in Watson Assistant. You will see a star icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek. AWS Lex V2 Logs Use the AWS Lambda archive to send user input that routes to the Lex FallbackIntent to NeuralSeek. Navigate to Amazon Cloudwatch Log Groups. Make sure you are in the same region as your bot. Click \"Create log group\". Give the log group a name (this will be logs of your bot's chat). Click \"Create\". Open the Amazon LexV2 console. From the list, choose a bot and select the link on the name. From the left menu, choose Aliases. In the list of aliases, choose the alias for which you want to configure conversation logs. In the Conversation logs section, choose Manage conversation logs. For text logs, choose Enable then enter the log group name from the group you created earlier. Choose Save to start logging conversations. If necessary, Amazon LexV2 will update your service role with permissions to access the CloudWatch Logs log group and selected S3 bucket. Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file (upload the zip file from the bottom of the page). Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your copied api key and instance URL from the NeuralSeek user interface. \"LogGroupName\" is the name you gave to the log group that you created earlier. Click \"Deploy\". Select \"Configuration\" tab. Select \"General Configuration\" pane. Click \"Edit\". Update under Timeout, set min to be 5 and sec to be 0. Click \"Save\". Select \"Permissions\" pane. Click on the link under \"Role name\". Click \"Add permissions\". Click \"Attach policies\". Search for \"CloudWatchReadOnlyAccess\". Select the box next to \"CloudWatchReadOnlyAccess\". Click \"Add permissions\". Navigate to the Amazon EventBridge Rules page. Click \"Create rule\". Give the rule a name. Select \"Schedule\" under Rule type. Click \"Continue to create rule\". Select \"A schedule that runs at a regular rate, such as every 10 minutes.\" under Schedule pattern. Under Rate expression set the \"Value\" to be 15 and the \"Unit, e.g. mins, hours...\" to be Minutes. Click \"Next\". Under \"Select a target\" select Lambda function. Under \"Function\" select the lambda we created earlier. Click \"Next\". Click \"Next\". Click \"Create rule\". Validate the connection by triggering an intent in the language tab in AWS LexV2. You will see a star icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek after the next Amazon EventBridge run (the rule will run every 15 minutes). Kore.ai Logs Enable Round-Trip monitoring on deployed NeuralSeek Intents. NeuralSeek will monitor the usage of NeuralSeek-curated intents leveraging KoreAI event Tasks, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. First we need to create a new Dialog Task to enable NeuralSeek RTM. Navigate to 'Dialog Tasks' on the KoreAI side panel under 'Converdational Skills'. Click on 'Create Dialog'. Give the Dialog a name (e.g. NeuralSeek RTM), a description (e.g. NeuralSeek Logs), and click 'Proceed'. Click and drag the 'Bot Action' option from the side panel to the '+' icon below your intent. Click and drag the 'Service' option from the side panel to the '+'' icon within the Bot Action. Click on the Service node to open the right side panel. Adjust the Service Type to Custom Service, the Sub Type to REST. Click on the 'Define Request' option below Request Definition. Change the Requst Type from GET to POST. Copy the instance URL provided in the NeuralSeek user interface into the URL section. Click on Headers and create a new key named 'apiKey' and copy the key provided in the NeuralSeek user interface into the Value section. Click on Body and using the drop down select 'application/json'. Click on Headers and create a new key named 'apiKey' and copy the key provided in the NeuralSeek user interface into the Value section. Validate the connnection using the 'Test' option in the top right. Click Save in the top right corner of the page. Back out of this Dialog Task and click on 'Intelligence' then 'Events' on the side panel. Click on 'End of Task', select 'Initiate Task' and then use the drop down to select the Dialog Task we just created. You are set up for KoreAI Logs!","title":"Supported Round Trip Monitoring"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/#overview","text":"NeuralSeek monitors curated responses between a user and a Virtual Agent by receiving logs from the supported Virtual Agents listed below: watsonx Assistant AWS Lex V2 Kore.ai","title":"Overview"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/#step-by-step-instruction","text":"The Source Virtual Agent is connected to NeuralSeek via the specific instructions per platform on the \"Integrate\" tab in the NeuralSeek user interface.","title":"Step by Step Instruction"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/#watson-logs","text":"Enable Round-Trip monitoring on deployed NeuralSeek Intents. NeuralSeek will monitor the usage of NeuralSeek-curated intents within both the Actions and Dialogs frameworks of Watson Assistant, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. On the \"Environments\" tab of your Watson Assistant instance, click on the gear icon (near the upper left, with the Environment name). Under \"Webhooks\", select \"Log webhook\". Copy the URL and Secret key provided in the NeuralSeek user interface for webhook setup. Click the box for \"Subscribe to conversation logs\". Validate the connection by triggering an intent via the \"Preview\" tab in Watson Assistant. You will see a star icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek.","title":"Watson Logs"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/#aws-lex-v2-logs","text":"Use the AWS Lambda archive to send user input that routes to the Lex FallbackIntent to NeuralSeek. Navigate to Amazon Cloudwatch Log Groups. Make sure you are in the same region as your bot. Click \"Create log group\". Give the log group a name (this will be logs of your bot's chat). Click \"Create\". Open the Amazon LexV2 console. From the list, choose a bot and select the link on the name. From the left menu, choose Aliases. In the list of aliases, choose the alias for which you want to configure conversation logs. In the Conversation logs section, choose Manage conversation logs. For text logs, choose Enable then enter the log group name from the group you created earlier. Choose Save to start logging conversations. If necessary, Amazon LexV2 will update your service role with permissions to access the CloudWatch Logs log group and selected S3 bucket. Open the Functions page on the Lambda console. Select \"Create function\". Create a function from scratch. In the Code Source pane, choose Upload from and then .zip file (upload the zip file from the bottom of the page). Choose Upload to select your Lambda Archive .zip file. Choose Save. Click on index.mjs and enter your copied api key and instance URL from the NeuralSeek user interface. \"LogGroupName\" is the name you gave to the log group that you created earlier. Click \"Deploy\". Select \"Configuration\" tab. Select \"General Configuration\" pane. Click \"Edit\". Update under Timeout, set min to be 5 and sec to be 0. Click \"Save\". Select \"Permissions\" pane. Click on the link under \"Role name\". Click \"Add permissions\". Click \"Attach policies\". Search for \"CloudWatchReadOnlyAccess\". Select the box next to \"CloudWatchReadOnlyAccess\". Click \"Add permissions\". Navigate to the Amazon EventBridge Rules page. Click \"Create rule\". Give the rule a name. Select \"Schedule\" under Rule type. Click \"Continue to create rule\". Select \"A schedule that runs at a regular rate, such as every 10 minutes.\" under Schedule pattern. Under Rate expression set the \"Value\" to be 15 and the \"Unit, e.g. mins, hours...\" to be Minutes. Click \"Next\". Under \"Select a target\" select Lambda function. Under \"Function\" select the lambda we created earlier. Click \"Next\". Click \"Next\". Click \"Create rule\". Validate the connection by triggering an intent in the language tab in AWS LexV2. You will see a star icon appear next to the corresponding Intent on the \"Curate\" tab in NeuralSeek after the next Amazon EventBridge run (the rule will run every 15 minutes).","title":"AWS Lex V2 Logs"},{"location":"integrations/supported_roundtrip_monitoring/supported_roundtrip_monitoring/#koreai-logs","text":"Enable Round-Trip monitoring on deployed NeuralSeek Intents. NeuralSeek will monitor the usage of NeuralSeek-curated intents leveraging KoreAI event Tasks, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. First we need to create a new Dialog Task to enable NeuralSeek RTM. Navigate to 'Dialog Tasks' on the KoreAI side panel under 'Converdational Skills'. Click on 'Create Dialog'. Give the Dialog a name (e.g. NeuralSeek RTM), a description (e.g. NeuralSeek Logs), and click 'Proceed'. Click and drag the 'Bot Action' option from the side panel to the '+' icon below your intent. Click and drag the 'Service' option from the side panel to the '+'' icon within the Bot Action. Click on the Service node to open the right side panel. Adjust the Service Type to Custom Service, the Sub Type to REST. Click on the 'Define Request' option below Request Definition. Change the Requst Type from GET to POST. Copy the instance URL provided in the NeuralSeek user interface into the URL section. Click on Headers and create a new key named 'apiKey' and copy the key provided in the NeuralSeek user interface into the Value section. Click on Body and using the drop down select 'application/json'. Click on Headers and create a new key named 'apiKey' and copy the key provided in the NeuralSeek user interface into the Value section. Validate the connnection using the 'Test' option in the top right. Click Save in the top right corner of the page. Back out of this Dialog Task and click on 'Intelligence' then 'Events' on the side panel. Click on 'End of Task', select 'Initiate Task' and then use the drop down to select the Dialog Task we just created. You are set up for KoreAI Logs!","title":"Kore.ai Logs"},{"location":"integrations/supported_virtual_agents/supported_virtual_agents/","text":"Overview The following Virtual Agents are directly supported by NeuralSeek, as of July of 2023: AWS Lex V2 Azure Knowledge Base Cognigy Kore.ai Watson Assistant","title":"Supported Virtual Agents"},{"location":"integrations/supported_virtual_agents/supported_virtual_agents/#overview","text":"The following Virtual Agents are directly supported by NeuralSeek, as of July of 2023: AWS Lex V2 Azure Knowledge Base Cognigy Kore.ai Watson Assistant","title":"Overview"},{"location":"integrations/watson_assistant_custom_extension/watson_assistant_custom_extension/","text":"Overview Use the custom extension to call NeuralSeek within Watson Assistant's \"Actions\" framework. Step by step instruction On the \"Integrations\" tab of Watson Assistant, click \"Build Custom Extension\" then \"Next\". Name the extension \"NeuralSeek\" and give a brief description. Click \"Next\". Upload your NeuralSeek OpenApi file. Click \"Next\" then \"Finish\". On the new \"NeuralSeek\" extension tile that appears, click \"Add\", \"Add\", then \"Next\". On the authentication screen, select \"API key auth\", and enter your api key. Click \"Next\", \"Finish\", then \"Close\". On the \"Actions\" tab of Watson Assistant, click \"Create Action\". Choose Quick Start, then select the NeuralSeek Starter Kit. Open your action and connect it to the extension you created in the previous steps. More information about Watson Assistant NeuralSeek Extension Please refer to the following documentation for more information regarding NeuralSeek starter-kit available on Watson Assistant Extension. https://github.com/watson-developer-cloud/assistant-toolkit/tree/master/integrations/extensions/starter-kits/neuralseek","title":"Watson Assistant Custom Extension"},{"location":"integrations/watson_assistant_custom_extension/watson_assistant_custom_extension/#overview","text":"Use the custom extension to call NeuralSeek within Watson Assistant's \"Actions\" framework.","title":"Overview"},{"location":"integrations/watson_assistant_custom_extension/watson_assistant_custom_extension/#step-by-step-instruction","text":"On the \"Integrations\" tab of Watson Assistant, click \"Build Custom Extension\" then \"Next\". Name the extension \"NeuralSeek\" and give a brief description. Click \"Next\". Upload your NeuralSeek OpenApi file. Click \"Next\" then \"Finish\". On the new \"NeuralSeek\" extension tile that appears, click \"Add\", \"Add\", then \"Next\". On the authentication screen, select \"API key auth\", and enter your api key. Click \"Next\", \"Finish\", then \"Close\". On the \"Actions\" tab of Watson Assistant, click \"Create Action\". Choose Quick Start, then select the NeuralSeek Starter Kit. Open your action and connect it to the extension you created in the previous steps.","title":"Step by step instruction"},{"location":"integrations/watson_assistant_custom_extension/watson_assistant_custom_extension/#more-information-about-watson-assistant-neuralseek-extension","text":"Please refer to the following documentation for more information regarding NeuralSeek starter-kit available on Watson Assistant Extension. https://github.com/watson-developer-cloud/assistant-toolkit/tree/master/integrations/extensions/starter-kits/neuralseek","title":"More information about Watson Assistant NeuralSeek Extension"},{"location":"integrations/watson_logs/watson_logs/","text":"Overview NeuralSeek will monitor the usage of NeuralSeek-curated intents within both the Actions and Dialogs frameworks of Watson Assistant, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. In the environments tab of your Watson Assistant, click the gear icon Under the Webhooks, select Log webhook. Provide the URL and secret key for the WebHook. Click the box for Subscribe to conversation logs .","title":"Watson Logs"},{"location":"integrations/watson_logs/watson_logs/#overview","text":"NeuralSeek will monitor the usage of NeuralSeek-curated intents within both the Actions and Dialogs frameworks of Watson Assistant, and will inform you of any curated intents that may need to be updated, based on changes to relevant documents in the connected KnowledgeBase. In the environments tab of your Watson Assistant, click the gear icon Under the Webhooks, select Log webhook. Provide the URL and secret key for the WebHook. Click the box for Subscribe to conversation logs .","title":"Overview"},{"location":"main_features/main_features/","text":"Main Features NeuralSeek's main features include a clickable path to fact-check AI responses, data analytics to enhance AI natural language capabilities, and step-by-step instructions for maintaining accurate resource data. With Semantic Match Scoring, NeuralSeek generates responses by directly utilizing content from corporate sources, ensuring transparency between the sources and answers. NeuralSeek enables virtual agents to handle open-ended, complex, contextual questions from real customers, based on your data, reducing the need for live agents by up to 80%. Additionally, NeuralSeek offers content analytics, multi-language support, PII detection, and can be easily integrated with many popular virtual agents in the market.","title":"Overview"},{"location":"main_features/main_features/#main-features","text":"NeuralSeek's main features include a clickable path to fact-check AI responses, data analytics to enhance AI natural language capabilities, and step-by-step instructions for maintaining accurate resource data. With Semantic Match Scoring, NeuralSeek generates responses by directly utilizing content from corporate sources, ensuring transparency between the sources and answers. NeuralSeek enables virtual agents to handle open-ended, complex, contextual questions from real customers, based on your data, reducing the need for live agents by up to 80%. Additionally, NeuralSeek offers content analytics, multi-language support, PII detection, and can be easily integrated with many popular virtual agents in the market.","title":"Main Features"},{"location":"main_features/automatic_data_cleansing_and_preparation/automatic_data_cleansing_and_preparation/","text":"Overview What is it? When using webpages as documentation for the KnowledgeBase, nuisance information such as banners and cookies will deteriorate information relevant to the users organization. The Automatic Data Cleansing feature of NeuralSeek will automatically cleanse those scraped webpages, exposing information pertinent to the organization, at the users own pace. Why is it important? Condensing and focusing the information, while removing useless wording returned by the KnowledgeBase is critical to high quality answer generation. Most web content is not great at directly answering questions because of the amount of nusiance webpage language that gets extracted with the core content. How does it work? NeuralSeek will identify documents in the KnowledgeBase that come from webscrapes. NeuralSeek will then run its own algorithm against the full webpage HTML to extract just the core content and remove as much of the extraneous information as possible.","title":"Automatic Data Cleansing And Preparation"},{"location":"main_features/automatic_data_cleansing_and_preparation/automatic_data_cleansing_and_preparation/#overview","text":"","title":"Overview"},{"location":"main_features/automatic_data_cleansing_and_preparation/automatic_data_cleansing_and_preparation/#what-is-it","text":"When using webpages as documentation for the KnowledgeBase, nuisance information such as banners and cookies will deteriorate information relevant to the users organization. The Automatic Data Cleansing feature of NeuralSeek will automatically cleanse those scraped webpages, exposing information pertinent to the organization, at the users own pace.","title":"What is it?"},{"location":"main_features/automatic_data_cleansing_and_preparation/automatic_data_cleansing_and_preparation/#why-is-it-important","text":"Condensing and focusing the information, while removing useless wording returned by the KnowledgeBase is critical to high quality answer generation. Most web content is not great at directly answering questions because of the amount of nusiance webpage language that gets extracted with the core content.","title":"Why is it important?"},{"location":"main_features/automatic_data_cleansing_and_preparation/automatic_data_cleansing_and_preparation/#how-does-it-work","text":"NeuralSeek will identify documents in the KnowledgeBase that come from webscrapes. NeuralSeek will then run its own algorithm against the full webpage HTML to extract just the core content and remove as much of the extraneous information as possible.","title":"How does it work?"},{"location":"main_features/caching/caching/","text":"Overview What is it? NeuralSeek uses caching strategy in two areas (Corporate KnowledgeBase and Answer) to enhance performance and reduce computational cost during its operation. The first part is when NeuralSeek searches through corporate knowledge base to obtain the original information. You can set the cache duration of such those responses to be cached, so that the original information\u2019s retrieval time can be reduced. NeuralSeek then utilizes two types of caches for both your edited answers and generated answers that can serve cached answers to user questions in order to speed up response times and product more consistent results. Why is it important? Caching frequently returned answers save both time and computation cost to run virtual agents, as it reduces NeuralSeek to having to generate responses repeatedly, especially on the more frequently asked questions or seldom updated answers. How does it work? Corporate KnowledgeBase Cache When NeuralSeek accesses the Corporate KnowledgeBase, it processes the original data from it, cleanses its contents (e.g. removing unnecessary contents, filtering, deduplicating, etc.), compresses it, and prioritize the returned contents which is then processed with LLM (Large Language Model) to form the completed response, usually at the range of 8,000 ~ 9,000 characters. It then derives a hash value of that response window which acts as a check to later see if the original data is updated. This response is what actually gets cached within the NeuralSeek, so that all those search, processing, and LLM-generated time is effectively saved when the same answer needs to be derived. Under the Configure > Corporate KnowledgeBase Details section, user can set the duration of the cache measured in minutes to control how long these responses need to be cached. Answer Cache When the user asks question to NeuralSeek, it tries to use the question to find the matching \u2018intent\u2019 of the question. And when the matching intent is discovered (usually via a fuzzy matching), the provided answer, either normal or user edited, can then be cached. Under the Configure > Intent Matching & Cache Configuration section, you can enable or disable the edited answer cache or normal answer cache, and set the following parameters to control how it works: Each cache type (edited answer, normal) would have the answer threshold bar and edited answer match tolerance. You can adjust the threshold to control when the caching will start caching for the answer, depending on how many answers exist for a given user question. For example, if you set the threshold to 5, the caching will not start until there exist 5 or more different answers to the given question. Setting the threshold to 1 would let NeuralSeek start caching as soon as it sees at least a single answer exists. Setting the value to 0 will disable caching completely. The matching method (Exact Match, Fuzzy Match, etc.) is the method you can specify to tell NeuralSeek on how to perform the intent matching on the question. There is also a more advanced matching method of \u2018Exact Match, exact conversational context\u2019 on the normal answers that would try to find the match if the consecutive conversation (e.g. one and the one after) both have the matching result, so that the match could be more correct in terms of how the conversation flow is occurring. In terms of the edited answers, this \u2018conversational context\u2019 matching is not provided given that the edited answers should be more concise and based on a more substantial ground and thus should not rely on the conversational context. Detecting changes in the original source In order to make sure the cached answers retain the authenticity, every cached answers are fed into an hashing algorithm to generate a unique hash key, which is then compared with the original source to detect whether the original source has been altered or not. If the hash keys do not match, NeuralSeek will notify users that the answers are not up-to-date with what\u2019s found in the KnowledgeBase. This would happen when a particular answer is being used during the Seek time, so that the answer would be kept in check with the original. User can then take a look at the outdated answer, and can either delete and re-load it, or edit it and then mark it as current, so that NeuralSeek will be able to check it off from its outdated list. One other way the answer would be checked is when NeuralSeek is handling round trip logging. During that time, NeuralSeek would check which answers are getting frequently returned and also perform an asynchronous checks with the KnowledgeBase to make sure they are up-to-date. How do we know the answers are coming from cache? You can check whether your query matched and returned the cached answer in the Seek tab. For example, this is an example of the answer returned from the cache. Next to the Total Response Time , you will see a label Cached which indicate that the answer came straight from the cache.","title":"Caching"},{"location":"main_features/caching/caching/#overview","text":"","title":"Overview"},{"location":"main_features/caching/caching/#what-is-it","text":"NeuralSeek uses caching strategy in two areas (Corporate KnowledgeBase and Answer) to enhance performance and reduce computational cost during its operation. The first part is when NeuralSeek searches through corporate knowledge base to obtain the original information. You can set the cache duration of such those responses to be cached, so that the original information\u2019s retrieval time can be reduced. NeuralSeek then utilizes two types of caches for both your edited answers and generated answers that can serve cached answers to user questions in order to speed up response times and product more consistent results.","title":"What is it?"},{"location":"main_features/caching/caching/#why-is-it-important","text":"Caching frequently returned answers save both time and computation cost to run virtual agents, as it reduces NeuralSeek to having to generate responses repeatedly, especially on the more frequently asked questions or seldom updated answers.","title":"Why is it important?"},{"location":"main_features/caching/caching/#how-does-it-work","text":"","title":"How does it work?"},{"location":"main_features/caching/caching/#corporate-knowledgebase-cache","text":"When NeuralSeek accesses the Corporate KnowledgeBase, it processes the original data from it, cleanses its contents (e.g. removing unnecessary contents, filtering, deduplicating, etc.), compresses it, and prioritize the returned contents which is then processed with LLM (Large Language Model) to form the completed response, usually at the range of 8,000 ~ 9,000 characters. It then derives a hash value of that response window which acts as a check to later see if the original data is updated. This response is what actually gets cached within the NeuralSeek, so that all those search, processing, and LLM-generated time is effectively saved when the same answer needs to be derived. Under the Configure > Corporate KnowledgeBase Details section, user can set the duration of the cache measured in minutes to control how long these responses need to be cached.","title":"Corporate KnowledgeBase Cache"},{"location":"main_features/caching/caching/#answer-cache","text":"When the user asks question to NeuralSeek, it tries to use the question to find the matching \u2018intent\u2019 of the question. And when the matching intent is discovered (usually via a fuzzy matching), the provided answer, either normal or user edited, can then be cached. Under the Configure > Intent Matching & Cache Configuration section, you can enable or disable the edited answer cache or normal answer cache, and set the following parameters to control how it works: Each cache type (edited answer, normal) would have the answer threshold bar and edited answer match tolerance. You can adjust the threshold to control when the caching will start caching for the answer, depending on how many answers exist for a given user question. For example, if you set the threshold to 5, the caching will not start until there exist 5 or more different answers to the given question. Setting the threshold to 1 would let NeuralSeek start caching as soon as it sees at least a single answer exists. Setting the value to 0 will disable caching completely. The matching method (Exact Match, Fuzzy Match, etc.) is the method you can specify to tell NeuralSeek on how to perform the intent matching on the question. There is also a more advanced matching method of \u2018Exact Match, exact conversational context\u2019 on the normal answers that would try to find the match if the consecutive conversation (e.g. one and the one after) both have the matching result, so that the match could be more correct in terms of how the conversation flow is occurring. In terms of the edited answers, this \u2018conversational context\u2019 matching is not provided given that the edited answers should be more concise and based on a more substantial ground and thus should not rely on the conversational context.","title":"Answer Cache"},{"location":"main_features/caching/caching/#detecting-changes-in-the-original-source","text":"In order to make sure the cached answers retain the authenticity, every cached answers are fed into an hashing algorithm to generate a unique hash key, which is then compared with the original source to detect whether the original source has been altered or not. If the hash keys do not match, NeuralSeek will notify users that the answers are not up-to-date with what\u2019s found in the KnowledgeBase. This would happen when a particular answer is being used during the Seek time, so that the answer would be kept in check with the original. User can then take a look at the outdated answer, and can either delete and re-load it, or edit it and then mark it as current, so that NeuralSeek will be able to check it off from its outdated list. One other way the answer would be checked is when NeuralSeek is handling round trip logging. During that time, NeuralSeek would check which answers are getting frequently returned and also perform an asynchronous checks with the KnowledgeBase to make sure they are up-to-date.","title":"Detecting changes in the original source"},{"location":"main_features/caching/caching/#how-do-we-know-the-answers-are-coming-from-cache","text":"You can check whether your query matched and returned the cached answer in the Seek tab. For example, this is an example of the answer returned from the cache. Next to the Total Response Time , you will see a label Cached which indicate that the answer came straight from the cache.","title":"How do we know the answers are coming from cache?"},{"location":"main_features/content_analytics/content_analytics/","text":"Overview What is it? NeuralSeek incorporates content analytics as a built-in feature, eliminating the need for additional code. With Content Analytics in NeuralSeek, users can effortlessly gather information about what users are searching for, assess the extent of documentation available on those topics, and evaluate documentation efficiency in addressing user queries. Why is it important? Content Analytics are a powerful feature that enable insight on the performance of your corporate documentation. You can gain insights on where content is exellent, underperforming, nonexistant or seldom used - and inform the groups responsible for creating or updating that information on how better to allocate their time. How does it work? Two main scores are returned when a user asks a question to NeuralSeek: Coverage Score: This score represents the number of documents or sections of documents that discuss the subject area(s) of a question from NeuralSeek. Confidence Score: The confidence score represents the likelihood that the information found in the KnowledgeBase of NeuralSeek and presented as a response is correct. This probability is given as a percentage. Low scoring questions with low converage scores tend to mean there is little or no documentation on the subject. Low scoring questions with high converage tends to mean there are conflicting source documents.","title":"Content Analytics"},{"location":"main_features/content_analytics/content_analytics/#overview","text":"","title":"Overview"},{"location":"main_features/content_analytics/content_analytics/#what-is-it","text":"NeuralSeek incorporates content analytics as a built-in feature, eliminating the need for additional code. With Content Analytics in NeuralSeek, users can effortlessly gather information about what users are searching for, assess the extent of documentation available on those topics, and evaluate documentation efficiency in addressing user queries.","title":"What is it?"},{"location":"main_features/content_analytics/content_analytics/#why-is-it-important","text":"Content Analytics are a powerful feature that enable insight on the performance of your corporate documentation. You can gain insights on where content is exellent, underperforming, nonexistant or seldom used - and inform the groups responsible for creating or updating that information on how better to allocate their time.","title":"Why is it important?"},{"location":"main_features/content_analytics/content_analytics/#how-does-it-work","text":"Two main scores are returned when a user asks a question to NeuralSeek: Coverage Score: This score represents the number of documents or sections of documents that discuss the subject area(s) of a question from NeuralSeek. Confidence Score: The confidence score represents the likelihood that the information found in the KnowledgeBase of NeuralSeek and presented as a response is correct. This probability is given as a percentage. Low scoring questions with low converage scores tend to mean there is little or no documentation on the subject. Low scoring questions with high converage tends to mean there are conflicting source documents.","title":"How does it work?"},{"location":"main_features/conversational_context/conversational_context/","text":"Overview What is it? NeuralSeek maintains context during each user interaction (conversation). When initiating a conversation, a session token is generated. Using this token and several Natural Language Processing (NLP) models, NeuralSeek tracks the topic of conversation to keep interactions focused and structured, allowing it to follow-up on questions that do not directly refer to the topic. In addition, these NLP models enable NeuralSeek to filter corporate knowledge topically by date to ensure that the information being returned is focused on the time period of the question. Why is it important? Conversational Context allows for NeuralSeek to answer questions without users being specific about their language for ever turn of the conversation. This enables higher containment rates in customer-facing conversations. How does it work? NeuralSeek employs several NLP models to identify and extract meaning, intent, and main subject from user questions and generated responses. These then inform later turns of the conversation so that the proper context can be brought forward from the KnowledgeBase and used for answer formation. It also weights heavily on caching and how the data can be cached. For example - The answer to a user question like \"how does it work\" depends heavily on the previous statements from a user. NeuralSeek requires that you pass an ID that can uniqley identify a user's session to enable this conversational context. The can be either or both the user_id and the session_id properties on the seek request. You do not need to maintain consistent id's over time for a specific acutal person - the id must just be constant for the session that you wish to maintain context for.","title":"Conversational Context"},{"location":"main_features/conversational_context/conversational_context/#overview","text":"","title":"Overview"},{"location":"main_features/conversational_context/conversational_context/#what-is-it","text":"NeuralSeek maintains context during each user interaction (conversation). When initiating a conversation, a session token is generated. Using this token and several Natural Language Processing (NLP) models, NeuralSeek tracks the topic of conversation to keep interactions focused and structured, allowing it to follow-up on questions that do not directly refer to the topic. In addition, these NLP models enable NeuralSeek to filter corporate knowledge topically by date to ensure that the information being returned is focused on the time period of the question.","title":"What is it?"},{"location":"main_features/conversational_context/conversational_context/#why-is-it-important","text":"Conversational Context allows for NeuralSeek to answer questions without users being specific about their language for ever turn of the conversation. This enables higher containment rates in customer-facing conversations.","title":"Why is it important?"},{"location":"main_features/conversational_context/conversational_context/#how-does-it-work","text":"NeuralSeek employs several NLP models to identify and extract meaning, intent, and main subject from user questions and generated responses. These then inform later turns of the conversation so that the proper context can be brought forward from the KnowledgeBase and used for answer formation. It also weights heavily on caching and how the data can be cached. For example - The answer to a user question like \"how does it work\" depends heavily on the previous statements from a user. NeuralSeek requires that you pass an ID that can uniqley identify a user's session to enable this conversational context. The can be either or both the user_id and the session_id properties on the seek request. You do not need to maintain consistent id's over time for a specific acutal person - the id must just be constant for the session that you wish to maintain context for.","title":"How does it work?"},{"location":"main_features/curation_of_answers/curation_of_answers/","text":"Overview What is it? NeuralSeek is directly trained off of the documentation loaded into the KnowledgeBase. If there is undesired answers from NeuralSeek, the first step is to review the documentation within the KnowledgeBase, and effectively curate the answer which can then be used by NeuralSeek to training itself better the next time it answers. Why is it important? One of the key factors in reducing costs is the utilization of curated answers sourced from a pool of responses, which proves to be more economical. Also, when the collection of answers becomes stagnant, potentially leading to outdated information, this feature will be able to detect it and refresh those with less manual process. How does it work? To tackle this challenge, NeuralSeek provides a solution by automatically monitoring the sources of information. It continuously tracks and compares the generated responses with the source documents to determine if any changes have occurred. By doing so, NeuralSeek ensures that the answers remain up-to-date and relevant. This eliminates the need for manual intervention and the potential for outdated information, allowing users to trust the accuracy and currency of the answers provided. Curating Intents and Answers Let's first visit the UI page for curating intents and answers. Click Curate tab on the top menu. The UI is composed of two the following columns: - Intent - Intents are collection of questions that may be related to the similar intent of the question. It is prefixed by certain type of intents such as FAQ , followed by the question's subject areas. By default, all the intents do fall under a category Others , but You can also define your own category in NeuralSeek's configuration. - Intents also have number of indicators that help users to understand the status of the intent. For example, it can show whether the intent has any new answers, whether the intent contains any PII (personally identifiable information), or whether the intent's underlying data has been outdated, etc. - Q&A - Shows the number of questions (white dialog icon) and answers (blue dialog icon) that this particular intent contains. - Coverage % - Indicates how much KnowledgeBase had contributed to the answer's coverage. If NeuralSeek was able to find all the necessary information from the KnowledgeBase, this percentage is going to be very high. - Confidence % - Indicates how much NeuralSeek its answer is most likely to satisfy the user. If this score is high, it means the answer has a high score of being legitimate and true to the facts. Reading the trend Each of the graph (coverage and confidence) has color codes that lets user visibly understand the state and trend. Coverage uses blue color with intensity that changes as its coverage is low or high. Confidence shows green to display high confidence to red meaning low confidence. You may also notice the slope has different heights, which gets smaller as there are more changes on its value. Hovering over the graph would reveal the trend of changes: In this case, there were instance of when the confidence had dropped from 83% to 22%, over the period between 14:07:31 to 14:12:15 on July 20th. Displaying Intents and Answers If you click the \u2304 Arrow next to the intent name, you will see the list of example questions and its generated answers: The example questions have either black color or gray color, depending on how they were created. The black colored examples are the ones that were actually submitted by the user's question. NeuralSeek automatically generates similar meaning questions per each question that it receives. As necessary, you can also enter your own Example question in addition to the ones that NeuralSeek generates. It is also possible to add Notes that may save additional information regarding this particular intent. Searching the intent The size of intent can vary but could grow over multiple pages, so you may want to search for a particular intent time to time. You could do that by using the search form at the top of the page. Enter the keyword and it will narrow down your search. Filtering the intent There is more fine-grained way of filtering intents based on critieria such as whether they were edited, or new answer was added, flagged, or out-of-date data was found. click the filter button, set the criterias that you want, and the page will only show the ones that met the filtering condition. Editing the Answer On all the answers generated, you can curate any answer by editing it. One of the reasons why you may need to edit it is because the generated answer could be better by editing, or you could add more details to it that was missing. Edited answer will be saved and will be considered higher priority over non-edited answers, to ensure that what you edited will be picked out first. Editing can be done by clicking the answer, modifying its content, and saving it. After saving, you will see that the answer that you edited will be marked as Edited . Deleting Questions and Answers If you wish to delete either the question or answer under the intent, you can do so by clicking the circle with i icon and selecting Remove . \u26a0\ufe0f Once they are removed, there is no way to roll back the removal, so be careful. Deleting all data You can delete all data by selecting the gear icon at the top and selecting: - Delete all data - Delete all analytics - Delete all unEdited Answers These are a useful feature if you wish to simply reset all of these data and start from the scratch. Intent operations When you select an intent, a popup will be displayed which shows you the operations that you can do with the selected intent. - Edit category - will let you edit the current category - Download to CSV - will export this into CSV file. It will have the following format: ID,question,score,kbCoverage,answer,category,intent,pii - Generate Convsersation - This will convert the intento into conversation, instead of a simple question and answer. This will give a better context for the NeuralSeek to generate answer from. - Flag - Will flag the intent so that you can quickly find it later. - Rename - Will let you rename its name - Delete - Deletes the selected intent(s). - Backup - Backs up the intent for later recovery. Note that the backed up file is not a text file, but in binary format. - Merge - appears only when two or more intents are selected. It merges all of their questions and answers into a single intent. There is no option to rollback the merge, so do it with caution.","title":"Curation Of Answers"},{"location":"main_features/curation_of_answers/curation_of_answers/#overview","text":"","title":"Overview"},{"location":"main_features/curation_of_answers/curation_of_answers/#what-is-it","text":"NeuralSeek is directly trained off of the documentation loaded into the KnowledgeBase. If there is undesired answers from NeuralSeek, the first step is to review the documentation within the KnowledgeBase, and effectively curate the answer which can then be used by NeuralSeek to training itself better the next time it answers.","title":"What is it?"},{"location":"main_features/curation_of_answers/curation_of_answers/#why-is-it-important","text":"One of the key factors in reducing costs is the utilization of curated answers sourced from a pool of responses, which proves to be more economical. Also, when the collection of answers becomes stagnant, potentially leading to outdated information, this feature will be able to detect it and refresh those with less manual process.","title":"Why is it important?"},{"location":"main_features/curation_of_answers/curation_of_answers/#how-does-it-work","text":"To tackle this challenge, NeuralSeek provides a solution by automatically monitoring the sources of information. It continuously tracks and compares the generated responses with the source documents to determine if any changes have occurred. By doing so, NeuralSeek ensures that the answers remain up-to-date and relevant. This eliminates the need for manual intervention and the potential for outdated information, allowing users to trust the accuracy and currency of the answers provided.","title":"How does it work?"},{"location":"main_features/curation_of_answers/curation_of_answers/#curating-intents-and-answers","text":"Let's first visit the UI page for curating intents and answers. Click Curate tab on the top menu. The UI is composed of two the following columns: - Intent - Intents are collection of questions that may be related to the similar intent of the question. It is prefixed by certain type of intents such as FAQ , followed by the question's subject areas. By default, all the intents do fall under a category Others , but You can also define your own category in NeuralSeek's configuration. - Intents also have number of indicators that help users to understand the status of the intent. For example, it can show whether the intent has any new answers, whether the intent contains any PII (personally identifiable information), or whether the intent's underlying data has been outdated, etc. - Q&A - Shows the number of questions (white dialog icon) and answers (blue dialog icon) that this particular intent contains. - Coverage % - Indicates how much KnowledgeBase had contributed to the answer's coverage. If NeuralSeek was able to find all the necessary information from the KnowledgeBase, this percentage is going to be very high. - Confidence % - Indicates how much NeuralSeek its answer is most likely to satisfy the user. If this score is high, it means the answer has a high score of being legitimate and true to the facts.","title":"Curating Intents and Answers"},{"location":"main_features/curation_of_answers/curation_of_answers/#reading-the-trend","text":"Each of the graph (coverage and confidence) has color codes that lets user visibly understand the state and trend. Coverage uses blue color with intensity that changes as its coverage is low or high. Confidence shows green to display high confidence to red meaning low confidence. You may also notice the slope has different heights, which gets smaller as there are more changes on its value. Hovering over the graph would reveal the trend of changes: In this case, there were instance of when the confidence had dropped from 83% to 22%, over the period between 14:07:31 to 14:12:15 on July 20th.","title":"Reading the trend"},{"location":"main_features/curation_of_answers/curation_of_answers/#displaying-intents-and-answers","text":"If you click the \u2304 Arrow next to the intent name, you will see the list of example questions and its generated answers: The example questions have either black color or gray color, depending on how they were created. The black colored examples are the ones that were actually submitted by the user's question. NeuralSeek automatically generates similar meaning questions per each question that it receives. As necessary, you can also enter your own Example question in addition to the ones that NeuralSeek generates. It is also possible to add Notes that may save additional information regarding this particular intent.","title":"Displaying Intents and Answers"},{"location":"main_features/curation_of_answers/curation_of_answers/#searching-the-intent","text":"The size of intent can vary but could grow over multiple pages, so you may want to search for a particular intent time to time. You could do that by using the search form at the top of the page. Enter the keyword and it will narrow down your search.","title":"Searching the intent"},{"location":"main_features/curation_of_answers/curation_of_answers/#filtering-the-intent","text":"There is more fine-grained way of filtering intents based on critieria such as whether they were edited, or new answer was added, flagged, or out-of-date data was found. click the filter button, set the criterias that you want, and the page will only show the ones that met the filtering condition.","title":"Filtering the intent"},{"location":"main_features/curation_of_answers/curation_of_answers/#editing-the-answer","text":"On all the answers generated, you can curate any answer by editing it. One of the reasons why you may need to edit it is because the generated answer could be better by editing, or you could add more details to it that was missing. Edited answer will be saved and will be considered higher priority over non-edited answers, to ensure that what you edited will be picked out first. Editing can be done by clicking the answer, modifying its content, and saving it. After saving, you will see that the answer that you edited will be marked as Edited .","title":"Editing the Answer"},{"location":"main_features/curation_of_answers/curation_of_answers/#deleting-questions-and-answers","text":"If you wish to delete either the question or answer under the intent, you can do so by clicking the circle with i icon and selecting Remove . \u26a0\ufe0f Once they are removed, there is no way to roll back the removal, so be careful.","title":"Deleting Questions and Answers"},{"location":"main_features/curation_of_answers/curation_of_answers/#deleting-all-data","text":"You can delete all data by selecting the gear icon at the top and selecting: - Delete all data - Delete all analytics - Delete all unEdited Answers These are a useful feature if you wish to simply reset all of these data and start from the scratch.","title":"Deleting all data"},{"location":"main_features/curation_of_answers/curation_of_answers/#intent-operations","text":"When you select an intent, a popup will be displayed which shows you the operations that you can do with the selected intent. - Edit category - will let you edit the current category - Download to CSV - will export this into CSV file. It will have the following format: ID,question,score,kbCoverage,answer,category,intent,pii - Generate Convsersation - This will convert the intento into conversation, instead of a simple question and answer. This will give a better context for the NeuralSeek to generate answer from. - Flag - Will flag the intent so that you can quickly find it later. - Rename - Will let you rename its name - Delete - Deletes the selected intent(s). - Backup - Backs up the intent for later recovery. Note that the backed up file is not a text file, but in binary format. - Merge - appears only when two or more intents are selected. It merges all of their questions and answers into a single intent. There is no option to rollback the merge, so do it with caution.","title":"Intent operations"},{"location":"main_features/dynamic_personalization/dynamic_personalization/","text":"Overview What is it? One way NeuralSeek quickly ties into the users business is by automatically personalizing results based on information from their Customer Relationship Management (CRM) system. By analyzing user data such as past interactions, preferences, purchase history, and demographic information, NeuralSeek can dynamically adjust its outputs to match the specific needs and preferences of each individual user. Why is it important? Personalized answers tend to engage users more, and can result in higher satisfaction and containment. How does it work? This can be previewed in the \u201cSeek\u201d tab of the NeuralSeek UI, and in production environments users will pass the personalization details via our API as the REST call to when /seek is made.","title":"Dynamic Personalization"},{"location":"main_features/dynamic_personalization/dynamic_personalization/#overview","text":"","title":"Overview"},{"location":"main_features/dynamic_personalization/dynamic_personalization/#what-is-it","text":"One way NeuralSeek quickly ties into the users business is by automatically personalizing results based on information from their Customer Relationship Management (CRM) system. By analyzing user data such as past interactions, preferences, purchase history, and demographic information, NeuralSeek can dynamically adjust its outputs to match the specific needs and preferences of each individual user.","title":"What is it?"},{"location":"main_features/dynamic_personalization/dynamic_personalization/#why-is-it-important","text":"Personalized answers tend to engage users more, and can result in higher satisfaction and containment.","title":"Why is it important?"},{"location":"main_features/dynamic_personalization/dynamic_personalization/#how-does-it-work","text":"This can be previewed in the \u201cSeek\u201d tab of the NeuralSeek UI, and in production environments users will pass the personalization details via our API as the REST call to when /seek is made.","title":"How does it work?"},{"location":"main_features/entity_extraction/entity_extraction/","text":"Overview What is it? NeuralSeek has a feature called \u2018Extract\u2019 which is a service to let user extract entities within a given user text. Users can also define their custom entities and provide description for NeuralSeek to detect and extract entities that are defined by users. The service is provided with a REST endpoint which can be used by external applications such as virtual agent or chat bots to invoke it within their conversational flow to enhance their capabilities to detect entities within it. Why is it important? Virtual Agents can define various \u2018entities\u2019, which may have values that needs to be categorized into concepts or types that can play various roles during their request handling. For example, when a user types a question like: \u201cI would like to buy a movie ticket.\u201d The term \u201cmovie ticket\u201d could be categorized as \u201cproduct\u201d that the virtual agent might need to understand so that the agent could start a dialog that would continue like: \u201cSure, what kind of movie ticket do you want to purchase?\u201d knowing that the user is interested in buying (intent) a movie ticket (product), so the agent should perform an action of providing list of the movies, as well as letting user choose the date and time, and ultimately proceeding with billing and payment. The inherent challenges in configuring virtual agent is to make sure these entities are accurately identified by providing various patterns or values or an entity type, so that when those words appear in the conversation, such entity could be identified. An example of that is how IBM Watson Assistant in dialog mode can define entity and its related values as such: In the above example, the entity \u2018product\u2019 would be identified in the dialog if the user mentioned these words such as \u2018movie reservation,\u2019 \u2018movie ticket,\u2019 or simply \u2018ticket.\u2019 Watson Assistant also provides fuzzy matching to match any incorrect spellings or slight deviation from these words to help it better cope with the request. However, there are obviously clear limitations and caveats in doing this approach. You have to provide every possible values necessary for the bot to understand it as certain type of entity. Anything out of the given value might not be categorized at all, or even categorized incorrectly. Maintaining a large set of entity and its subsequent values can be costly and time consuming. If you have to support multiple languages, you may need to provide all the possible values as legal vocabularies which can then be a pretty challenging feat. How does it work? NeuralSeek\u2019s Entity Extraction uses natural language processing to extract key entities that your virtual agent needs to understand, without requiring you to specify possible values or patterns and having the burden of constantly maintaining it. Entity Extraction from conversation Let\u2019s take a look at the above example of defining a movie ticket as a product. In the tab \u2018Extract,\u2019 enter the same text of \u2018I would like to buy a movie ticket\u2019 and click \u2018Extract\u2019 button. You will see NeuralSeek, without specifying anything, was able to identify the movie ticket as an entity of product and properly extracted it from the given string. Moreover, you can ask the phrase in different languages, and NeuralSeek\u2019s entity extraction will still work, without you doing anything! Custom Entities In case there is a specific way that you need to make to categorize an entity, NeuralSeek provides a simpler and better way to define what your entity is, by using Custom Entity definition. Using this, Neural Seek can perform entity extraction in much more robust way: And obviously, this single customer entity definition would work in other languages too! Entity Extraction REST API NeuralSeek\u2019s entity extraction supports integration via REST API, so it make calling the service easy with any external applications such as virtual agents or chatbots. It is easy to test its functionality by using API documentation under Integrate tab. This will return the following JSON type response:","title":"Entity Extraction"},{"location":"main_features/entity_extraction/entity_extraction/#overview","text":"","title":"Overview"},{"location":"main_features/entity_extraction/entity_extraction/#what-is-it","text":"NeuralSeek has a feature called \u2018Extract\u2019 which is a service to let user extract entities within a given user text. Users can also define their custom entities and provide description for NeuralSeek to detect and extract entities that are defined by users. The service is provided with a REST endpoint which can be used by external applications such as virtual agent or chat bots to invoke it within their conversational flow to enhance their capabilities to detect entities within it.","title":"What is it?"},{"location":"main_features/entity_extraction/entity_extraction/#why-is-it-important","text":"Virtual Agents can define various \u2018entities\u2019, which may have values that needs to be categorized into concepts or types that can play various roles during their request handling. For example, when a user types a question like: \u201cI would like to buy a movie ticket.\u201d The term \u201cmovie ticket\u201d could be categorized as \u201cproduct\u201d that the virtual agent might need to understand so that the agent could start a dialog that would continue like: \u201cSure, what kind of movie ticket do you want to purchase?\u201d knowing that the user is interested in buying (intent) a movie ticket (product), so the agent should perform an action of providing list of the movies, as well as letting user choose the date and time, and ultimately proceeding with billing and payment. The inherent challenges in configuring virtual agent is to make sure these entities are accurately identified by providing various patterns or values or an entity type, so that when those words appear in the conversation, such entity could be identified. An example of that is how IBM Watson Assistant in dialog mode can define entity and its related values as such: In the above example, the entity \u2018product\u2019 would be identified in the dialog if the user mentioned these words such as \u2018movie reservation,\u2019 \u2018movie ticket,\u2019 or simply \u2018ticket.\u2019 Watson Assistant also provides fuzzy matching to match any incorrect spellings or slight deviation from these words to help it better cope with the request. However, there are obviously clear limitations and caveats in doing this approach. You have to provide every possible values necessary for the bot to understand it as certain type of entity. Anything out of the given value might not be categorized at all, or even categorized incorrectly. Maintaining a large set of entity and its subsequent values can be costly and time consuming. If you have to support multiple languages, you may need to provide all the possible values as legal vocabularies which can then be a pretty challenging feat.","title":"Why is it important?"},{"location":"main_features/entity_extraction/entity_extraction/#how-does-it-work","text":"NeuralSeek\u2019s Entity Extraction uses natural language processing to extract key entities that your virtual agent needs to understand, without requiring you to specify possible values or patterns and having the burden of constantly maintaining it.","title":"How does it work?"},{"location":"main_features/entity_extraction/entity_extraction/#entity-extraction-from-conversation","text":"Let\u2019s take a look at the above example of defining a movie ticket as a product. In the tab \u2018Extract,\u2019 enter the same text of \u2018I would like to buy a movie ticket\u2019 and click \u2018Extract\u2019 button. You will see NeuralSeek, without specifying anything, was able to identify the movie ticket as an entity of product and properly extracted it from the given string. Moreover, you can ask the phrase in different languages, and NeuralSeek\u2019s entity extraction will still work, without you doing anything!","title":"Entity Extraction from conversation"},{"location":"main_features/entity_extraction/entity_extraction/#custom-entities","text":"In case there is a specific way that you need to make to categorize an entity, NeuralSeek provides a simpler and better way to define what your entity is, by using Custom Entity definition. Using this, Neural Seek can perform entity extraction in much more robust way: And obviously, this single customer entity definition would work in other languages too!","title":"Custom Entities"},{"location":"main_features/entity_extraction/entity_extraction/#entity-extraction-rest-api","text":"NeuralSeek\u2019s entity extraction supports integration via REST API, so it make calling the service easy with any external applications such as virtual agents or chatbots. It is easy to test its functionality by using API documentation under Integrate tab. This will return the following JSON type response:","title":"Entity Extraction REST API"},{"location":"main_features/explore_platform/explore_platform/","text":"Overview What is it? NeuralSeek offers a feature called \"Explore\" which is a versatile and innovative platform, offering an open-ended playground for retrieval augmented generation. It empowers users to seamlessly integrate their preferred Language Model (LLM), select from a range of data sources including Knowledge Bases, websites, local files, or typed text, and employ the NeuralSeek Template Language (NTL) markup for dynamic content retrieval. Notably, \"Explore\" enhances data by incorporating features like summarization, stopwords removal, and keyword extraction, all while providing expert guidance with LLM prompt syntax and base weighting. With the ability to output results to an editor or directly to a Word document, \"Explore\" delivers a powerful and user-friendly experience, making it a standout feature in content generation and retrieval. Why is it important? The \"Explore\" feature within NeuralSeek is important for several reasons: Efficient Content Retrival \"Explore\" simplifies the process of accessing and retrieving content from various sources. This efficiency is crucial for anyone who relies on accurate and relevant information. Enhanced Data Quality The feature enhances data quality by providing tools for summarization, stopwords removal, and keyword extraction. This ensures that the retrieved content is refined, concise, and tailored to the user's needs, saving time and effort in manual data preprocessing. User-Friendly Interface \"Explore\" offers a user-friendly interface that makes interacting with Language Models and crafting dynamic prompts accessible to a broader audience. This accessibility is vital for individuals who may not have advanced technical skills but still require the benefits of advanced language models. Expert Guidance This feature provides users with expert guidance by offering correct LLM prompt parameters and model-specific base weights. This guidance helps users achieve optimal results without the need for in-depth knowledge of language model intricacies. Output Flexibility The ability to output results to an editor or directly to a Word document enhances flexibility and convenience for users, allowing them to seamlessly integrate the generated content into their workflows. Semantic Scoring The incorporation of a Semantic Scoring model allows users to assess the relevance and alignment of generated content with their specific requirements. This feature adds a layer of precision and control to the content generation process. How does it work? \"Explore\" streamlines the interaction with Language Models, making it accessible and user-friendly while providing powerful tools for content retrieval and enhancement. Users can seamlessly integrate retrieved content into their workflows with precision and control, making it a valuable asset for various professional fields. Here's a step-by-step explanation of how it works: Language Model Selection Users on a Bring You Own Large Language Model plan (BYOLLM) begin by selecting their desired LLM(s) to use with \"Explore\" in the menu setting. Dynamic Prompt Creation Users craft dynamic prompts using a combination of regular words and NeuralSeek Template Language (NTL) markup. These prompts serve as instructions to the LLM for content retrieval and generation. Add Content Users can easily retrieve content from various sources with just a point-and-click: Knowledge Bases (KB): Users can query KBs and retrieve information based on specific search terms or queries. Websites: Users can extract data from websites, fetching content as needed. Local Files: Content from local files such as PDFs, Docs, CSVs, XLS, and TXT can be accessed and used. Typed Text: Users can input their own text, integrating it seamlessly into the content generation process. Data Enhancement This features enhances retrieved data by: Summarization: It can summarize lengthy content, condensing it into a more concise format. Stopwords Removal: Unnecessary words are removed, further refining the content. Keyword Extraction: Important keywords or phrases are extracted, aiding in content analysis and understanding. Optimization \"Explore\" provides users with the correct LLM parameters and base weighting across models, ensuring that prompts are formulated effectively for optimal results. Content Management NeuralSeek's \"Explore\" feature offers tools that can greatly benefit internal content management processes. Users are able to seamlessly make multiple interactions with the LLM in succession and generate insightful responses about source data tables utilizing these tools: Send to LLM Within the \"Generate Data\" sidebar option of the new Explore feature in NeuralSeek, users can simply instruct the Language Model (LLM) by providing prompts in natural language to generate desired outputs. What sets this feature apart is its capacity for looped or chained LLM calls. This simply means that users can make multiple requests or interactions with the LLM one after another. This eliminates the need to start a new separate interaction each time. This capability to chain or loop LLM calls can be valuable for tasks that require iterative or sequential interactions with the language model. Overall, the \"Send to LLM\" feature improves efficiency, streamlines processes, and can be utilized in various applications. Table Understanding Within the \"Generate Data\" sidebar option of NeuralSeek's Explore feature is the \"Table Understanding\" feature. Users will start by uploading a spreadsheet, either an Excel or CSV file. Then, the user is able to generate insightful responses about the source data by providing queries in natural language through the \"Table Understanding\" prompt. For example, the user uploads an Excel file containing diverse data with the goal of writing a memo highlighting noteable data points. Utilizing NeuralSeek's point-to-click feature, the user will proceed to click to add the data file, click to intitate the Table Understanding prompt, add a query, then click to evaluate results. NeuralSeek undertakes a comprehensive examination of the provided data to output an accurate response. At the bottom of the screen, NeuralSeek provides a confidence percentage, accompanied by the statement, \"Table Understanding reports a confidence level of X%.\". This percentage is based on how much the system trusts the answer it gave you based on what it found in the data. Output Options Users choose where the generated content is directed: Editor: Content can be viewed and edited directly within the built-in editor. Word Document: Users can export the generated content directly to a Word document for further use. Semantic Scoring The generated content is evaluated against a Semantic Scoring model. This assessment provides users with insights into the relevance and alignment of the generated content with their specific needs or preferences. A Practical Use Case Consider a scenario where you need to extract valuable information from a webpage. With \"Explore,\" you can create an NTL prompt that retrieves the essential content, eliminates irrelevant material, summarizes it to approximately 1500 characters, and sends it to the LLM: {% raw %} write a marketing email for watsonx.ai Here is some documentation: {{ web|url:\"https://www.ibm.com/products/watsonx-ai\" }}=>{{ summarize|length:1500 }} {% endraw %} Explore has the ability to perform Knowledge Base (KB) queries while dynamically configuring scoring parameters and snippet overrides. It then seamlessly transfers the processed data to the Language Model (LLM). {% raw %} We need to update documentation to talk about our new release. The new release adds a feature called generative dashboarding. It can be accessed from the file menu. Here is the old documentation to update: {{ kb|query:\"cognos analytics\"|snippet:2000|scoreRange:0.1 }} {% endraw %}","title":"Explore"},{"location":"main_features/explore_platform/explore_platform/#overview","text":"","title":"Overview"},{"location":"main_features/explore_platform/explore_platform/#what-is-it","text":"NeuralSeek offers a feature called \"Explore\" which is a versatile and innovative platform, offering an open-ended playground for retrieval augmented generation. It empowers users to seamlessly integrate their preferred Language Model (LLM), select from a range of data sources including Knowledge Bases, websites, local files, or typed text, and employ the NeuralSeek Template Language (NTL) markup for dynamic content retrieval. Notably, \"Explore\" enhances data by incorporating features like summarization, stopwords removal, and keyword extraction, all while providing expert guidance with LLM prompt syntax and base weighting. With the ability to output results to an editor or directly to a Word document, \"Explore\" delivers a powerful and user-friendly experience, making it a standout feature in content generation and retrieval.","title":"What is it?"},{"location":"main_features/explore_platform/explore_platform/#why-is-it-important","text":"The \"Explore\" feature within NeuralSeek is important for several reasons:","title":"Why is it important?"},{"location":"main_features/explore_platform/explore_platform/#efficient-content-retrival","text":"\"Explore\" simplifies the process of accessing and retrieving content from various sources. This efficiency is crucial for anyone who relies on accurate and relevant information.","title":"Efficient Content Retrival"},{"location":"main_features/explore_platform/explore_platform/#enhanced-data-quality","text":"The feature enhances data quality by providing tools for summarization, stopwords removal, and keyword extraction. This ensures that the retrieved content is refined, concise, and tailored to the user's needs, saving time and effort in manual data preprocessing.","title":"Enhanced Data Quality"},{"location":"main_features/explore_platform/explore_platform/#user-friendly-interface","text":"\"Explore\" offers a user-friendly interface that makes interacting with Language Models and crafting dynamic prompts accessible to a broader audience. This accessibility is vital for individuals who may not have advanced technical skills but still require the benefits of advanced language models.","title":"User-Friendly Interface"},{"location":"main_features/explore_platform/explore_platform/#expert-guidance","text":"This feature provides users with expert guidance by offering correct LLM prompt parameters and model-specific base weights. This guidance helps users achieve optimal results without the need for in-depth knowledge of language model intricacies.","title":"Expert Guidance"},{"location":"main_features/explore_platform/explore_platform/#output-flexibility","text":"The ability to output results to an editor or directly to a Word document enhances flexibility and convenience for users, allowing them to seamlessly integrate the generated content into their workflows.","title":"Output Flexibility"},{"location":"main_features/explore_platform/explore_platform/#semantic-scoring","text":"The incorporation of a Semantic Scoring model allows users to assess the relevance and alignment of generated content with their specific requirements. This feature adds a layer of precision and control to the content generation process.","title":"Semantic Scoring"},{"location":"main_features/explore_platform/explore_platform/#how-does-it-work","text":"\"Explore\" streamlines the interaction with Language Models, making it accessible and user-friendly while providing powerful tools for content retrieval and enhancement. Users can seamlessly integrate retrieved content into their workflows with precision and control, making it a valuable asset for various professional fields. Here's a step-by-step explanation of how it works:","title":"How does it work?"},{"location":"main_features/explore_platform/explore_platform/#language-model-selection","text":"Users on a Bring You Own Large Language Model plan (BYOLLM) begin by selecting their desired LLM(s) to use with \"Explore\" in the menu setting.","title":"Language Model Selection"},{"location":"main_features/explore_platform/explore_platform/#dynamic-prompt-creation","text":"Users craft dynamic prompts using a combination of regular words and NeuralSeek Template Language (NTL) markup. These prompts serve as instructions to the LLM for content retrieval and generation.","title":"Dynamic Prompt Creation"},{"location":"main_features/explore_platform/explore_platform/#add-content","text":"Users can easily retrieve content from various sources with just a point-and-click: Knowledge Bases (KB): Users can query KBs and retrieve information based on specific search terms or queries. Websites: Users can extract data from websites, fetching content as needed. Local Files: Content from local files such as PDFs, Docs, CSVs, XLS, and TXT can be accessed and used. Typed Text: Users can input their own text, integrating it seamlessly into the content generation process.","title":"Add Content"},{"location":"main_features/explore_platform/explore_platform/#data-enhancement","text":"This features enhances retrieved data by: Summarization: It can summarize lengthy content, condensing it into a more concise format. Stopwords Removal: Unnecessary words are removed, further refining the content. Keyword Extraction: Important keywords or phrases are extracted, aiding in content analysis and understanding.","title":"Data Enhancement"},{"location":"main_features/explore_platform/explore_platform/#optimization","text":"\"Explore\" provides users with the correct LLM parameters and base weighting across models, ensuring that prompts are formulated effectively for optimal results.","title":"Optimization"},{"location":"main_features/explore_platform/explore_platform/#content-management","text":"NeuralSeek's \"Explore\" feature offers tools that can greatly benefit internal content management processes. Users are able to seamlessly make multiple interactions with the LLM in succession and generate insightful responses about source data tables utilizing these tools:","title":"Content Management"},{"location":"main_features/explore_platform/explore_platform/#send-to-llm","text":"Within the \"Generate Data\" sidebar option of the new Explore feature in NeuralSeek, users can simply instruct the Language Model (LLM) by providing prompts in natural language to generate desired outputs. What sets this feature apart is its capacity for looped or chained LLM calls. This simply means that users can make multiple requests or interactions with the LLM one after another. This eliminates the need to start a new separate interaction each time. This capability to chain or loop LLM calls can be valuable for tasks that require iterative or sequential interactions with the language model. Overall, the \"Send to LLM\" feature improves efficiency, streamlines processes, and can be utilized in various applications.","title":"Send to LLM"},{"location":"main_features/explore_platform/explore_platform/#table-understanding","text":"Within the \"Generate Data\" sidebar option of NeuralSeek's Explore feature is the \"Table Understanding\" feature. Users will start by uploading a spreadsheet, either an Excel or CSV file. Then, the user is able to generate insightful responses about the source data by providing queries in natural language through the \"Table Understanding\" prompt. For example, the user uploads an Excel file containing diverse data with the goal of writing a memo highlighting noteable data points. Utilizing NeuralSeek's point-to-click feature, the user will proceed to click to add the data file, click to intitate the Table Understanding prompt, add a query, then click to evaluate results. NeuralSeek undertakes a comprehensive examination of the provided data to output an accurate response. At the bottom of the screen, NeuralSeek provides a confidence percentage, accompanied by the statement, \"Table Understanding reports a confidence level of X%.\". This percentage is based on how much the system trusts the answer it gave you based on what it found in the data.","title":"Table Understanding"},{"location":"main_features/explore_platform/explore_platform/#output-options","text":"Users choose where the generated content is directed: Editor: Content can be viewed and edited directly within the built-in editor. Word Document: Users can export the generated content directly to a Word document for further use.","title":"Output Options"},{"location":"main_features/explore_platform/explore_platform/#semantic-scoring_1","text":"The generated content is evaluated against a Semantic Scoring model. This assessment provides users with insights into the relevance and alignment of the generated content with their specific needs or preferences.","title":"Semantic Scoring"},{"location":"main_features/explore_platform/explore_platform/#a-practical-use-case","text":"Consider a scenario where you need to extract valuable information from a webpage. With \"Explore,\" you can create an NTL prompt that retrieves the essential content, eliminates irrelevant material, summarizes it to approximately 1500 characters, and sends it to the LLM: {% raw %} write a marketing email for watsonx.ai Here is some documentation: {{ web|url:\"https://www.ibm.com/products/watsonx-ai\" }}=>{{ summarize|length:1500 }} {% endraw %} Explore has the ability to perform Knowledge Base (KB) queries while dynamically configuring scoring parameters and snippet overrides. It then seamlessly transfers the processed data to the Language Model (LLM). {% raw %} We need to update documentation to talk about our new release. The new release adds a feature called generative dashboarding. It can be accessed from the file menu. Here is the old documentation to update: {{ kb|query:\"cognos analytics\"|snippet:2000|scoreRange:0.1 }} {% endraw %}","title":"A Practical Use Case"},{"location":"main_features/explore_platform/ntl/ntl/","text":"Overview What is it? NeuralSeek's explore feature is powered by NeuralSeek Template Language which lets user extract data from different sources, and then extract and format data that then can be used to drive subsequent LLM processing without the need to code in programming language. How to use it? Getting Data KB Documentation KB stands for KnowledgeBase, and the query is used to retrieve snippets of document that the underlying knowledge base would return. Basic syntax looks like the following: {% raw %} {{ kb|query:\"your KB query\" | snippet: \"\" | scoreRange: \"\" | filter: \"\" }} {% endraw %} query: The actual query snippet: snippet size (tokens to be returned). For example, 100 will return snippets of around 100 characters, while 1000 will return ~1000-character snippets. score range: minimum score to be returned the value is between 0.0 to 1.0, where higher score will only return snippets with higher score. filter: If the filter is defined in the knowledge base (e.g. DQL pushdown), you can define the filter value to be used. Any null string value such as \u2018\u2019 or \u201c\u201d will be considered as null. DB2 Database {% raw %} {{ db2|query:\"your db2 query\" | DATABASE: \"\" | HOSTNAME: \"\" | UID: \"\" | PWD: \"\" | PORT: \"\" | SECURE: \"true\" | sentences: \"true\"}} {% endraw %} Uses data returned from SQL statement (db query) on IBM DB2 database. Seek {% raw %} {{ seek|query:\"your Seek query\" | stump: \"\" | filter: \"\" | language: \"\" | seekLLM: \"\" }} {% endraw %} Seek lets users perform seek to NeuralSeek, and provide it as a data. The following parameters can be provided: query: The query itself stump: optional stump speech that overrides existing stump speech. filter: filter to be provided when running seek. Identical to how filter is used in KB documentation. language: seek target language to be used. seekLLM: type of LLM for the seek to explicitly use, if your NeuralSeek uses BYOLLM (bring your own LLM) plan. Explore {% raw %} {{ explore|template: \"\" }} {% endraw %} Lets you invoke explore template to derive results. Provide the template name. For example, if you have a template called neuralseek_updates , that looks like this: {% raw %} Based on the changelogs found here: {{ web|url:\"https://documentation.neuralseek.com/changelog/\" }} list the items for the latest month. {{ LLM }} {% endraw %} {% raw %} Simply using the {{ explore|template: \"neuralseek_updates\" }} will produce the sample result. {% endraw %} Providing the parameters In case you have defined a parameter in your explore: {% raw %} Based on the changelogs found here: {{ web|url:\"<< name:'url' >>\" }} list the items for the latest month. {{ LLM }} {% endraw %} You can simply enter the name and value of the parameter followed by the template to specify values: {% raw %} {{ explore|template: \"neuralseek_updates\"|url: \"https://documentation.neuralseek.com/changelog/\" }} {% endraw %} Notice that in this way your invocation to explore will be much more flexible, as you can dynamically provide the parameters when calling other explore. Website Text {% raw %} {{ web|url:\"[https://yourpage.com](https://yourpage.com/)\" }} {% endraw %} Uses URL to read the HTML documents. Currently do not support any authentication mechanism. For example, {% raw %} {{ web|url:\"https://en.wikipedia.org/wiki/Roman\" }}=>{{ keywords|nouns:false }} {% endraw %} Will extract proper nouns from the wikipedia page related to Roman. The result will be: Wikipedia, encyclopedia, Roman, Romans, rom\u00e2n, Wiktionary, Rome, Italy Ancient Rome, BC, Rome Epistle, Testament, Christian Bible Roman, Music Romans, Sound Horizon, EP, Teen, Boy, Morning Musume Film, Film Roman, Indian Malayalam, Doctor, People Roman, Romans \u1fec\u03c9\u03bc\u03b1\u1fd6\u03bf\u03b9, Rhomaioi, Greeks, Middle Ages, Ottoman, R\u00fbm, Muslim, Bulgaria Roman Municipality Roman, Eure, France Roman, Romania Roman County, Sakha Republic, Russia Roman River, Essex, England Roman Valley, Nova Scotia, Canada Romans Romans, Ain, France Romans, Deux, S\u00e8vres, France Romans dIsonzo, Italy Romans, sur-Is\u00e8re, France Religion Roman Catholic, Roman Catholic, Nancy Grace Roman Space Telescope, Roman Space Telescope, NASA, ROMAN, Search, Wikipedia., Romans History, Greco, Romany, Gypsies, Roma, Disambiguation, Wikidata Upload Document Uploading the document works in two steps. When you click the Upload Document button, you are presented with a file selector to select a local documentation to upload. The supported files are .docx, .doc, .pdf, .txt, .csv, .json, and xlsx. After the document is successfully loaded, it is available in the NTL templates: Clicking the upload document can then be used using the following syntax: {% raw %} {{ doc|name:title_application_receipt.pdf }} {% endraw %} Currently the document does not support OCR, and thus only textual data will be able to be extracted and used. Modifying Data Set Variable {% raw %} {{ variable|name:\"myVariable\" }} {% endraw %} Creates a variable that can be used multiple times inside the NTL expression. For example, {% raw %} 12=>{{ variable|name:\"age\" }} What is your age? My age is << name: age >> {% endraw %} This will first set the variable \u2018age\u2019 with the value 12, and then use it to output the person\u2019s age: What is your age? My age is 12 {% raw %} \u26a0\ufe0f When the variable is NOT found but used in << >> notation, the variable is considered as user input, and explore will ask its value prior to the evaluation. For example, if you have << name: new >> and there is not such thing as {{ variable|name: \"new\" }} in the expression, explore will ask for it: {% endraw %} Remove PII {% raw %} {{ PII }} {% endraw %} When used, will hide all the detected PII information. For example, {% raw %} howardyoo@mail.com Howard Yoo Dog Cat Person {{ PII }} {% endraw %} Will output: ****** ****** Dog Cat Person Summarize Summarization summarizes given text into shorter size, while maintaining the main content. {% raw %} I have 20 cats and 40 dogs - it's a lot of furry friends to take care of! My name is Jane and I run an animal rescue shelter out of my home. It all started a few years ago when I took in a litter of abandoned kittens. I fell in love with them and decided to make it my mission to give unwanted animals a forever home. {{ summarize|length:100 }} {% endraw %} will yield: I have 20 cats and 40 dogs - it's a lot of furry friends to take care of! Remove Stopwords Removes the stop words {% raw %} I have 20 cats and 40 dogs {{ stopwords }} {% endraw %} Will yield 20 cats 40 dogs Notice I have is deemed as insignificant and thus have been removed. Another example: {% raw %} I have 20 cats and 40 dogs, isn't this amazing? {{ stopwords }} {% endraw %} Will yield 20 cats 40 dogs, amazing? Extract Keywords Extracts important keywords from the text. {% raw %} I have 20 cats and 40 dogs {{ keywords|nouns:true }} {% endraw %} Will yield: 20 cats, 40 dogs Notice how nouns:true is used to detect cats and dogs. If nouns:false , only the proper nouns will be extracted. What is this proper nouns? Proper noun is a name used for an individual person, place, or organization, spelled with initial capital letters, e.g., Larry, Mexico, and Boston Red Sox. So, for example, {% raw %} Howard has 20 cats and 40 dogs {{ keywords|nouns:false }} {% endraw %} Will yield: Howard As Howard is a person\u2019s name (and thus falls into the proper noun). If the nouns: true is used, Howard, 20 cats, 40 dogs Is returned. Force Numeric I have 20 cats and 40 dogs contains numeric values. Force Numeric simply removes all non-numerics and concatenates all numerics into a single value. So, running this: {% raw %} I have 20 cats and 40 dogs {{ forceNumeric }} {% endraw %} Will yield: 2040 since 20 concatenated by 40 is 2040. Follow-up question Is there a way to only extract 20 or 40? One way is to use split if you know the start and end portion. For example, {% raw %} I have 20 cats and 40 dogs {{ split | start: \"have\" | end: \"and\" | removeHeaders: false }}=>{{ forceNumeric }} {% endraw %} Notice how split is splitting the sentence to this: have 20 cats Which is then processed with {% raw %} {{ forceNumeric }} {% endraw %} and thus yields: 20 Then, how about the latter part? (40 dogs) {% raw %} I have 20 cats and 40 dogs {{ split | start: \"and\" | end: \"\" | removeHeaders: false }}=>{{ forceNumeric }} {% endraw %} Notice how you can use empty string for \u201cend\u201d and it will split the sentence into: and 40 dogs Which can then be processed with {% raw %} {{ forceNumeric }} {% endraw %} and thus yields: 40 Table Prep {% raw %} {{ tablePrep | query:\"\" }} {% endraw %} tablePrep prepares tabular data to be better understood and processed by LLM. For example, if we have a CSV typed data, table prep will convert it into {% raw %} col1,col2,col3 data1,data2,data3 data11,data22,data33 {{ tablePrep }} {% endraw %} The result will be: { \"col1\": [ \"data1\", \"data11\" ], \"col2\": [ \"data2\", \"data22\" ], \"col3\": [ \"data3\", \"data33\" ] } There is an optional query parameter that you can use to narrow down the data for better performance. For example: {% raw %} col1,col2,col3 data1,data2,data3 data11,data22,data33 {{ tablePrep|query: \"values for col1\" }} {% endraw %} is going to retrieve all the values for col1 which will then be: { \"col1\": [ \"data1\", \"data11\" ] } Split Simple split operation to split (or cut) part of text using start and end string. {% raw %} I have 20 cats and 40 dogs. {{ split | start: \"20\" | end: \"40\" | removeHeaders: false }} {% endraw %} will yield: 20 cats and the start and end keyword is case sensitive, so {% raw %} I have 20 cats and 40 dogs. {{ split | start: \"CAT\" | end: \"dogs\" | removeHeaders: false }} {% endraw %} Will yield I have 20 cats and 40 Notice that the start word is included in the result, while end word is excluded . Using removeHeaders will strip frequently repeating lines out of given input text. For example: {% raw %} My animals: I have 20 cats and 40 dogs. My animals: I have 47 hamsters and 22 pet snakes. My animals: I pet all my animals every day. {{ split | removeHeaders: true }} {% endraw %} Would output: I have 20 cats and 40 dogs. I have 47 hamsters and 22 pet snakes. I pet all my animals every day. This is particularly useful when dealing with web content as input. Regular Expression {% raw %} {{ regex | match: \"\" | replace: \"\" | group: \"\" }} {% endraw %} Performs regular expression on given data. Regular expression can be a powerful feature to extract or replace certain data. For example, if you have a text that you need to replace with something else, you can use the following expression: {% raw %} my name is howardyoo {{ regex | match: \"yoo\" | replace: \"yu\" }} {% endraw %} Which yields: my name is howardyu Regex also supports extraction. For example, if you want to extract the email address in a text message, you can do so: {% raw %} my name is howardyoo@email.com {{ regex | match: \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\" | group: \"0\" }} {% endraw %} This will extract the email address (group 0). The response is: howardyoo@email.com Regex also supports groups, so in case you want to get an last digits of the phone number, you can do so like the following example: {% raw %} my phone number is 213-292-3322 {{ regex | match: \"([0-9]+)-([0-9]+)-([0-9]+)\" | group: \"3\" }} {% endraw %} which will result in: 3322 Generating Data Send to LLM {% raw %} {{ LLM }} {% endraw %} send to LLM may perhaps be the most frequently used syntax in NTL. There\u2019s an additional option, which is called prompt , that you can prepend prompts to it. For example, {% raw %} Write a short poem about NeuralSeek Here is the definition of NeuralSeek: {{ seek|query:\"What is NeuralSeek\" }}=>{{ summarize|length:100 }} {{ LLM }} {% endraw %} This will write a short poem about NeuralSeek, but based on the summary that it gathers from the seek operation that provides information on NeuralSeek. If this was given to LLM without any context of what NeuralSeek was, the LLM would have creatively guessed what NeuralSeek could have been (e.g. Cyberpunk Musician). In the LLM syntax, you can add additional prompts such as: {% raw %} Write a short poem about NeuralSeek Here is the definition of NeuralSeek: {{ seek|query:\"What is NeuralSeek\" }}=>{{ summarize|length:100 }} {{ LLM|prompt: \"write in Spanish\" }} {% endraw %} To generate the final output in Spanish: En el vasto mar de la informaci\u00f3n, NeuralSeek es navegaci\u00f3n, Con su LLM de gran sofisticaci\u00f3n, Da respuestas con precisi\u00f3n. De la base de conocimiento bebe, Y al usuario lo que busca le ofrece, En el contexto siempre se mueve, Con inteligencia que el saber teje. No hay duda intrincada o perdida, Que NeuralSeek no pueda esclarecer, Con su ayuda, la b\u00fasqueda es vencida, Y el conocimiento se puede entender. Como faro en la noche del dato, Ilumina el camino a seguir, NeuralSeek, el asistente innato, Para en el saber siempre fluir. Table Understanding {% raw %} {{ TableUnderstanding|query:\"What year had the highest revenue?\" }} {% endraw %} This works with uploaded Excel or CSV files. Use natural language to query and extract a value from the tabular structure it contains. Mathematical Equation {% raw %} {{ math|equation:\"1 + 1\" }} {% endraw %} Performs mathematical equation on input strings. It features a flexible expression parser with support for symbolic computation, comes with a large set of built-in functions and constants, and works with different data types like numbers, big numbers, complex numbers, fractions, units, and matrices. Powerful and easy to use. Sending Data POST {% raw %} {{ post | url: \"\" | headers: \"\" | body: \"\"}} {% endraw %} Performs HTTP POST operation, with the given data. Email {% raw %} {{ email|host:\"\" | port: \"\" | user: \"\" | pass: \"\" | from: \"\" | to: \"\" | subject: \"\" | message: \"\" }} {% endraw %} Sends the content as email message, with the given data. Useful in templates. host and port is the host and port of SMTP server.","title":"NeuralSeek Template Language"},{"location":"main_features/explore_platform/ntl/ntl/#overview","text":"","title":"Overview"},{"location":"main_features/explore_platform/ntl/ntl/#what-is-it","text":"NeuralSeek's explore feature is powered by NeuralSeek Template Language which lets user extract data from different sources, and then extract and format data that then can be used to drive subsequent LLM processing without the need to code in programming language.","title":"What is it?"},{"location":"main_features/explore_platform/ntl/ntl/#how-to-use-it","text":"","title":"How to use it?"},{"location":"main_features/explore_platform/ntl/ntl/#getting-data","text":"","title":"Getting Data"},{"location":"main_features/explore_platform/ntl/ntl/#kb-documentation","text":"KB stands for KnowledgeBase, and the query is used to retrieve snippets of document that the underlying knowledge base would return. Basic syntax looks like the following: {% raw %} {{ kb|query:\"your KB query\" | snippet: \"\" | scoreRange: \"\" | filter: \"\" }} {% endraw %} query: The actual query snippet: snippet size (tokens to be returned). For example, 100 will return snippets of around 100 characters, while 1000 will return ~1000-character snippets. score range: minimum score to be returned the value is between 0.0 to 1.0, where higher score will only return snippets with higher score. filter: If the filter is defined in the knowledge base (e.g. DQL pushdown), you can define the filter value to be used. Any null string value such as \u2018\u2019 or \u201c\u201d will be considered as null.","title":"KB Documentation"},{"location":"main_features/explore_platform/ntl/ntl/#db2-database","text":"{% raw %} {{ db2|query:\"your db2 query\" | DATABASE: \"\" | HOSTNAME: \"\" | UID: \"\" | PWD: \"\" | PORT: \"\" | SECURE: \"true\" | sentences: \"true\"}} {% endraw %} Uses data returned from SQL statement (db query) on IBM DB2 database.","title":"DB2 Database"},{"location":"main_features/explore_platform/ntl/ntl/#seek","text":"{% raw %} {{ seek|query:\"your Seek query\" | stump: \"\" | filter: \"\" | language: \"\" | seekLLM: \"\" }} {% endraw %} Seek lets users perform seek to NeuralSeek, and provide it as a data. The following parameters can be provided: query: The query itself stump: optional stump speech that overrides existing stump speech. filter: filter to be provided when running seek. Identical to how filter is used in KB documentation. language: seek target language to be used. seekLLM: type of LLM for the seek to explicitly use, if your NeuralSeek uses BYOLLM (bring your own LLM) plan.","title":"Seek"},{"location":"main_features/explore_platform/ntl/ntl/#explore","text":"{% raw %} {{ explore|template: \"\" }} {% endraw %} Lets you invoke explore template to derive results. Provide the template name. For example, if you have a template called neuralseek_updates , that looks like this: {% raw %} Based on the changelogs found here: {{ web|url:\"https://documentation.neuralseek.com/changelog/\" }} list the items for the latest month. {{ LLM }} {% endraw %} {% raw %} Simply using the {{ explore|template: \"neuralseek_updates\" }} will produce the sample result. {% endraw %} Providing the parameters In case you have defined a parameter in your explore: {% raw %} Based on the changelogs found here: {{ web|url:\"<< name:'url' >>\" }} list the items for the latest month. {{ LLM }} {% endraw %} You can simply enter the name and value of the parameter followed by the template to specify values: {% raw %} {{ explore|template: \"neuralseek_updates\"|url: \"https://documentation.neuralseek.com/changelog/\" }} {% endraw %} Notice that in this way your invocation to explore will be much more flexible, as you can dynamically provide the parameters when calling other explore.","title":"Explore"},{"location":"main_features/explore_platform/ntl/ntl/#website-text","text":"{% raw %} {{ web|url:\"[https://yourpage.com](https://yourpage.com/)\" }} {% endraw %} Uses URL to read the HTML documents. Currently do not support any authentication mechanism. For example, {% raw %} {{ web|url:\"https://en.wikipedia.org/wiki/Roman\" }}=>{{ keywords|nouns:false }} {% endraw %} Will extract proper nouns from the wikipedia page related to Roman. The result will be: Wikipedia, encyclopedia, Roman, Romans, rom\u00e2n, Wiktionary, Rome, Italy Ancient Rome, BC, Rome Epistle, Testament, Christian Bible Roman, Music Romans, Sound Horizon, EP, Teen, Boy, Morning Musume Film, Film Roman, Indian Malayalam, Doctor, People Roman, Romans \u1fec\u03c9\u03bc\u03b1\u1fd6\u03bf\u03b9, Rhomaioi, Greeks, Middle Ages, Ottoman, R\u00fbm, Muslim, Bulgaria Roman Municipality Roman, Eure, France Roman, Romania Roman County, Sakha Republic, Russia Roman River, Essex, England Roman Valley, Nova Scotia, Canada Romans Romans, Ain, France Romans, Deux, S\u00e8vres, France Romans dIsonzo, Italy Romans, sur-Is\u00e8re, France Religion Roman Catholic, Roman Catholic, Nancy Grace Roman Space Telescope, Roman Space Telescope, NASA, ROMAN, Search, Wikipedia., Romans History, Greco, Romany, Gypsies, Roma, Disambiguation, Wikidata","title":"Website Text"},{"location":"main_features/explore_platform/ntl/ntl/#upload-document","text":"Uploading the document works in two steps. When you click the Upload Document button, you are presented with a file selector to select a local documentation to upload. The supported files are .docx, .doc, .pdf, .txt, .csv, .json, and xlsx. After the document is successfully loaded, it is available in the NTL templates: Clicking the upload document can then be used using the following syntax: {% raw %} {{ doc|name:title_application_receipt.pdf }} {% endraw %} Currently the document does not support OCR, and thus only textual data will be able to be extracted and used.","title":"Upload Document"},{"location":"main_features/explore_platform/ntl/ntl/#modifying-data","text":"","title":"Modifying Data"},{"location":"main_features/explore_platform/ntl/ntl/#set-variable","text":"{% raw %} {{ variable|name:\"myVariable\" }} {% endraw %} Creates a variable that can be used multiple times inside the NTL expression. For example, {% raw %} 12=>{{ variable|name:\"age\" }} What is your age? My age is << name: age >> {% endraw %} This will first set the variable \u2018age\u2019 with the value 12, and then use it to output the person\u2019s age: What is your age? My age is 12 {% raw %} \u26a0\ufe0f When the variable is NOT found but used in << >> notation, the variable is considered as user input, and explore will ask its value prior to the evaluation. For example, if you have << name: new >> and there is not such thing as {{ variable|name: \"new\" }} in the expression, explore will ask for it: {% endraw %}","title":"Set Variable"},{"location":"main_features/explore_platform/ntl/ntl/#remove-pii","text":"{% raw %} {{ PII }} {% endraw %} When used, will hide all the detected PII information. For example, {% raw %} howardyoo@mail.com Howard Yoo Dog Cat Person {{ PII }} {% endraw %} Will output: ****** ****** Dog Cat Person","title":"Remove PII"},{"location":"main_features/explore_platform/ntl/ntl/#summarize","text":"Summarization summarizes given text into shorter size, while maintaining the main content. {% raw %} I have 20 cats and 40 dogs - it's a lot of furry friends to take care of! My name is Jane and I run an animal rescue shelter out of my home. It all started a few years ago when I took in a litter of abandoned kittens. I fell in love with them and decided to make it my mission to give unwanted animals a forever home. {{ summarize|length:100 }} {% endraw %} will yield: I have 20 cats and 40 dogs - it's a lot of furry friends to take care of!","title":"Summarize"},{"location":"main_features/explore_platform/ntl/ntl/#remove-stopwords","text":"Removes the stop words {% raw %} I have 20 cats and 40 dogs {{ stopwords }} {% endraw %} Will yield 20 cats 40 dogs Notice I have is deemed as insignificant and thus have been removed. Another example: {% raw %} I have 20 cats and 40 dogs, isn't this amazing? {{ stopwords }} {% endraw %} Will yield 20 cats 40 dogs, amazing?","title":"Remove Stopwords"},{"location":"main_features/explore_platform/ntl/ntl/#extract-keywords","text":"Extracts important keywords from the text. {% raw %} I have 20 cats and 40 dogs {{ keywords|nouns:true }} {% endraw %} Will yield: 20 cats, 40 dogs Notice how nouns:true is used to detect cats and dogs. If nouns:false , only the proper nouns will be extracted. What is this proper nouns? Proper noun is a name used for an individual person, place, or organization, spelled with initial capital letters, e.g., Larry, Mexico, and Boston Red Sox. So, for example, {% raw %} Howard has 20 cats and 40 dogs {{ keywords|nouns:false }} {% endraw %} Will yield: Howard As Howard is a person\u2019s name (and thus falls into the proper noun). If the nouns: true is used, Howard, 20 cats, 40 dogs Is returned.","title":"Extract Keywords"},{"location":"main_features/explore_platform/ntl/ntl/#force-numeric","text":"I have 20 cats and 40 dogs contains numeric values. Force Numeric simply removes all non-numerics and concatenates all numerics into a single value. So, running this: {% raw %} I have 20 cats and 40 dogs {{ forceNumeric }} {% endraw %} Will yield: 2040 since 20 concatenated by 40 is 2040.","title":"Force Numeric"},{"location":"main_features/explore_platform/ntl/ntl/#follow-up-question","text":"Is there a way to only extract 20 or 40? One way is to use split if you know the start and end portion. For example, {% raw %} I have 20 cats and 40 dogs {{ split | start: \"have\" | end: \"and\" | removeHeaders: false }}=>{{ forceNumeric }} {% endraw %} Notice how split is splitting the sentence to this: have 20 cats Which is then processed with {% raw %} {{ forceNumeric }} {% endraw %} and thus yields: 20 Then, how about the latter part? (40 dogs) {% raw %} I have 20 cats and 40 dogs {{ split | start: \"and\" | end: \"\" | removeHeaders: false }}=>{{ forceNumeric }} {% endraw %} Notice how you can use empty string for \u201cend\u201d and it will split the sentence into: and 40 dogs Which can then be processed with {% raw %} {{ forceNumeric }} {% endraw %} and thus yields: 40","title":"Follow-up question"},{"location":"main_features/explore_platform/ntl/ntl/#table-prep","text":"{% raw %} {{ tablePrep | query:\"\" }} {% endraw %} tablePrep prepares tabular data to be better understood and processed by LLM. For example, if we have a CSV typed data, table prep will convert it into {% raw %} col1,col2,col3 data1,data2,data3 data11,data22,data33 {{ tablePrep }} {% endraw %} The result will be: { \"col1\": [ \"data1\", \"data11\" ], \"col2\": [ \"data2\", \"data22\" ], \"col3\": [ \"data3\", \"data33\" ] } There is an optional query parameter that you can use to narrow down the data for better performance. For example: {% raw %} col1,col2,col3 data1,data2,data3 data11,data22,data33 {{ tablePrep|query: \"values for col1\" }} {% endraw %} is going to retrieve all the values for col1 which will then be: { \"col1\": [ \"data1\", \"data11\" ] }","title":"Table Prep"},{"location":"main_features/explore_platform/ntl/ntl/#split","text":"Simple split operation to split (or cut) part of text using start and end string. {% raw %} I have 20 cats and 40 dogs. {{ split | start: \"20\" | end: \"40\" | removeHeaders: false }} {% endraw %} will yield: 20 cats and the start and end keyword is case sensitive, so {% raw %} I have 20 cats and 40 dogs. {{ split | start: \"CAT\" | end: \"dogs\" | removeHeaders: false }} {% endraw %} Will yield I have 20 cats and 40 Notice that the start word is included in the result, while end word is excluded . Using removeHeaders will strip frequently repeating lines out of given input text. For example: {% raw %} My animals: I have 20 cats and 40 dogs. My animals: I have 47 hamsters and 22 pet snakes. My animals: I pet all my animals every day. {{ split | removeHeaders: true }} {% endraw %} Would output: I have 20 cats and 40 dogs. I have 47 hamsters and 22 pet snakes. I pet all my animals every day. This is particularly useful when dealing with web content as input.","title":"Split"},{"location":"main_features/explore_platform/ntl/ntl/#regular-expression","text":"{% raw %} {{ regex | match: \"\" | replace: \"\" | group: \"\" }} {% endraw %} Performs regular expression on given data. Regular expression can be a powerful feature to extract or replace certain data. For example, if you have a text that you need to replace with something else, you can use the following expression: {% raw %} my name is howardyoo {{ regex | match: \"yoo\" | replace: \"yu\" }} {% endraw %} Which yields: my name is howardyu Regex also supports extraction. For example, if you want to extract the email address in a text message, you can do so: {% raw %} my name is howardyoo@email.com {{ regex | match: \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\" | group: \"0\" }} {% endraw %} This will extract the email address (group 0). The response is: howardyoo@email.com Regex also supports groups, so in case you want to get an last digits of the phone number, you can do so like the following example: {% raw %} my phone number is 213-292-3322 {{ regex | match: \"([0-9]+)-([0-9]+)-([0-9]+)\" | group: \"3\" }} {% endraw %} which will result in: 3322","title":"Regular Expression"},{"location":"main_features/explore_platform/ntl/ntl/#generating-data","text":"","title":"Generating Data"},{"location":"main_features/explore_platform/ntl/ntl/#send-to-llm","text":"{% raw %} {{ LLM }} {% endraw %} send to LLM may perhaps be the most frequently used syntax in NTL. There\u2019s an additional option, which is called prompt , that you can prepend prompts to it. For example, {% raw %} Write a short poem about NeuralSeek Here is the definition of NeuralSeek: {{ seek|query:\"What is NeuralSeek\" }}=>{{ summarize|length:100 }} {{ LLM }} {% endraw %} This will write a short poem about NeuralSeek, but based on the summary that it gathers from the seek operation that provides information on NeuralSeek. If this was given to LLM without any context of what NeuralSeek was, the LLM would have creatively guessed what NeuralSeek could have been (e.g. Cyberpunk Musician). In the LLM syntax, you can add additional prompts such as: {% raw %} Write a short poem about NeuralSeek Here is the definition of NeuralSeek: {{ seek|query:\"What is NeuralSeek\" }}=>{{ summarize|length:100 }} {{ LLM|prompt: \"write in Spanish\" }} {% endraw %} To generate the final output in Spanish: En el vasto mar de la informaci\u00f3n, NeuralSeek es navegaci\u00f3n, Con su LLM de gran sofisticaci\u00f3n, Da respuestas con precisi\u00f3n. De la base de conocimiento bebe, Y al usuario lo que busca le ofrece, En el contexto siempre se mueve, Con inteligencia que el saber teje. No hay duda intrincada o perdida, Que NeuralSeek no pueda esclarecer, Con su ayuda, la b\u00fasqueda es vencida, Y el conocimiento se puede entender. Como faro en la noche del dato, Ilumina el camino a seguir, NeuralSeek, el asistente innato, Para en el saber siempre fluir.","title":"Send to LLM"},{"location":"main_features/explore_platform/ntl/ntl/#table-understanding","text":"{% raw %} {{ TableUnderstanding|query:\"What year had the highest revenue?\" }} {% endraw %} This works with uploaded Excel or CSV files. Use natural language to query and extract a value from the tabular structure it contains.","title":"Table Understanding"},{"location":"main_features/explore_platform/ntl/ntl/#mathematical-equation","text":"{% raw %} {{ math|equation:\"1 + 1\" }} {% endraw %} Performs mathematical equation on input strings. It features a flexible expression parser with support for symbolic computation, comes with a large set of built-in functions and constants, and works with different data types like numbers, big numbers, complex numbers, fractions, units, and matrices. Powerful and easy to use.","title":"Mathematical Equation"},{"location":"main_features/explore_platform/ntl/ntl/#sending-data","text":"","title":"Sending Data"},{"location":"main_features/explore_platform/ntl/ntl/#post","text":"{% raw %} {{ post | url: \"\" | headers: \"\" | body: \"\"}} {% endraw %} Performs HTTP POST operation, with the given data.","title":"POST"},{"location":"main_features/explore_platform/ntl/ntl/#email","text":"{% raw %} {{ email|host:\"\" | port: \"\" | user: \"\" | pass: \"\" | from: \"\" | to: \"\" | subject: \"\" | message: \"\" }} {% endraw %} Sends the content as email message, with the given data. Useful in templates. host and port is the host and port of SMTP server.","title":"Email"},{"location":"main_features/explore_platform/visual_editor/visual_editor/","text":"Overview What is it? Visual Editor is the editor of choice when you are using NeuralSeek's explore feature. Even though typing the full NTL expression is not overly complex, it can get quickly tedious and time consuming. Visual editor provides a great way for any user to easily and quickly create the expression using simple drag-n-drop with minimum typing. How do you use it? Switching to visual editor In Explore, simply click the Visual Editor tab on the top of the page to enable the visual editor. Click to insert All the elements on the left panel can be created to the editor by clicking them. Element 'Text' is now visible in the editor. Click to edit Click the Text box, and the selected box will be highlighted in blue color, and secondary properties dialog will appear on the right side. Depending on the type of the element, there will be several inputs that you can then specify to it. For example, the text element can have string data. Deleting the node You also have option to delete the node, by clicking the red Delete Node button at the bottom. Stacking elements Clicking elements by default will connect the elements vertically. We call this Stacking . Stacked elements flow from top to bottom, meaning the output produced by the top element will become the input to the bottom element. Evaluating Clicking the evaluate button will run the expression, and generate answers. Chaining elements Aside from stacking, you can also connect elements horizontally. We call this Chaining . In this example, click the element Extract Keywords to get stacked under Send To LLM . And then, click the element, and drag on to the right side of the element that you want to chain. You will see a blue dot indicating that this would be the element that it will be chaining to. Release the click, and the elements are now chained together. Chaining is useful when you want an element's flow to be directed only from its output. In this example, the output of the LLM will only going to be provided as input to the extract keywords element, chained together. Saving as user template You may frequently be using the same expression over and over again, and visually editing them everytime is just not productive. In that case, you can save the template to be reused again. Click the Save Template button. Enter its name and description. Click Save to save it into the user template. Loading the template Your saved template can be loaded or used as necessary. Click Load Template , select User Templates , and click the checkbox to the template that you want to load. Click Load Template to load the saved template into the editor. Deleting the template You can also check and click Delete Template button to delete the checked template. Switching between Visual Editor and NTL editor You can switch between visual editor and NTL editor at any time you want. Any changes that you make on either side will be applied to other automatically.","title":"Visual Editor"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#overview","text":"","title":"Overview"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#what-is-it","text":"Visual Editor is the editor of choice when you are using NeuralSeek's explore feature. Even though typing the full NTL expression is not overly complex, it can get quickly tedious and time consuming. Visual editor provides a great way for any user to easily and quickly create the expression using simple drag-n-drop with minimum typing.","title":"What is it?"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#how-do-you-use-it","text":"","title":"How do you use it?"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#switching-to-visual-editor","text":"In Explore, simply click the Visual Editor tab on the top of the page to enable the visual editor.","title":"Switching to visual editor"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#click-to-insert","text":"All the elements on the left panel can be created to the editor by clicking them. Element 'Text' is now visible in the editor.","title":"Click to insert"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#click-to-edit","text":"Click the Text box, and the selected box will be highlighted in blue color, and secondary properties dialog will appear on the right side. Depending on the type of the element, there will be several inputs that you can then specify to it. For example, the text element can have string data.","title":"Click to edit"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#deleting-the-node","text":"You also have option to delete the node, by clicking the red Delete Node button at the bottom.","title":"Deleting the node"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#stacking-elements","text":"Clicking elements by default will connect the elements vertically. We call this Stacking . Stacked elements flow from top to bottom, meaning the output produced by the top element will become the input to the bottom element.","title":"Stacking elements"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#evaluating","text":"Clicking the evaluate button will run the expression, and generate answers.","title":"Evaluating"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#chaining-elements","text":"Aside from stacking, you can also connect elements horizontally. We call this Chaining . In this example, click the element Extract Keywords to get stacked under Send To LLM . And then, click the element, and drag on to the right side of the element that you want to chain. You will see a blue dot indicating that this would be the element that it will be chaining to. Release the click, and the elements are now chained together. Chaining is useful when you want an element's flow to be directed only from its output. In this example, the output of the LLM will only going to be provided as input to the extract keywords element, chained together.","title":"Chaining elements"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#saving-as-user-template","text":"You may frequently be using the same expression over and over again, and visually editing them everytime is just not productive. In that case, you can save the template to be reused again. Click the Save Template button. Enter its name and description. Click Save to save it into the user template.","title":"Saving as user template"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#loading-the-template","text":"Your saved template can be loaded or used as necessary. Click Load Template , select User Templates , and click the checkbox to the template that you want to load. Click Load Template to load the saved template into the editor.","title":"Loading the template"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#deleting-the-template","text":"You can also check and click Delete Template button to delete the checked template.","title":"Deleting the template"},{"location":"main_features/explore_platform/visual_editor/visual_editor/#switching-between-visual-editor-and-ntl-editor","text":"You can switch between visual editor and NTL editor at any time you want. Any changes that you make on either side will be applied to other automatically.","title":"Switching between Visual Editor and NTL editor"},{"location":"main_features/identify_language/identify_language/","text":"Overview What is it? NeuralSeek provides a service that would analyze and identify the language of a given text. Why is it important? Any application that would need to understand which language a given text is can now use NeuralSeek to do it, rather than relying on other external services. How does it work? Language identification is provided as REST API, and can be tested on NeuralSeek API documentation . Message payload is in text/plain format, and contains text in certain language. An example message would look something like this: \uc774 \uc5b8\uc5b4\ub294 \uc5b4\ub5a4 \uc5b8\uc5b4\uc785\ub2c8\uae4c? NeuralSeek would then identify what language this is in, and returns the language code and the confidence score: ```JSON [ { \"language\": \"ko\", \"confidence\": 0.95 } ]","title":"Identify Language"},{"location":"main_features/identify_language/identify_language/#overview","text":"","title":"Overview"},{"location":"main_features/identify_language/identify_language/#what-is-it","text":"NeuralSeek provides a service that would analyze and identify the language of a given text.","title":"What is it?"},{"location":"main_features/identify_language/identify_language/#why-is-it-important","text":"Any application that would need to understand which language a given text is can now use NeuralSeek to do it, rather than relying on other external services.","title":"Why is it important?"},{"location":"main_features/identify_language/identify_language/#how-does-it-work","text":"Language identification is provided as REST API, and can be tested on NeuralSeek API documentation . Message payload is in text/plain format, and contains text in certain language. An example message would look something like this: \uc774 \uc5b8\uc5b4\ub294 \uc5b4\ub5a4 \uc5b8\uc5b4\uc785\ub2c8\uae4c? NeuralSeek would then identify what language this is in, and returns the language code and the confidence score: ```JSON [ { \"language\": \"ko\", \"confidence\": 0.95 } ]","title":"How does it work?"},{"location":"main_features/intent_categorization/intent_categorization/","text":"Overview What is it? NeuralSeek can automatically categorize user input and questions into categories. These categories can be anything - products, organizations, departments... Users my set up categories on the Configure Tab, by entering category names and descriptions. These will then be used to match user input into categories. User input that does not match any category, or that too closely matches multiple categories will be placed in a default category called \"Other\". This default category cannot be modified. Why is it important? Categorization is very useful at scaling NeuralSeek within an organization. By grouping intents into categories it can make things much easier for subject matter experts to quickly take action on their specific area of content. Categorization can be useful even outside the context of answering ueser questions - for example in routing customer questions to the correct department or live agent. Categorization can be called directly via the API. How does it work? User input is scored and bucketed based on the category title and description, and based on intents that have been manually moved into categories (self-learning). Once categorization is enabled, the Curate and Anaytlics screens will change to show groupings around cateegory. Categorization is not retroactive - meaning if you define a new category - we will not automatically re-run all old user input against the new categories. Users may move intents into categories manually thru the Curate tab or the CSV download/edit features. The edits made will be used to train the system for future categorization events.","title":"Intent Categorization"},{"location":"main_features/intent_categorization/intent_categorization/#overview","text":"","title":"Overview"},{"location":"main_features/intent_categorization/intent_categorization/#what-is-it","text":"NeuralSeek can automatically categorize user input and questions into categories. These categories can be anything - products, organizations, departments... Users my set up categories on the Configure Tab, by entering category names and descriptions. These will then be used to match user input into categories. User input that does not match any category, or that too closely matches multiple categories will be placed in a default category called \"Other\". This default category cannot be modified.","title":"What is it?"},{"location":"main_features/intent_categorization/intent_categorization/#why-is-it-important","text":"Categorization is very useful at scaling NeuralSeek within an organization. By grouping intents into categories it can make things much easier for subject matter experts to quickly take action on their specific area of content. Categorization can be useful even outside the context of answering ueser questions - for example in routing customer questions to the correct department or live agent. Categorization can be called directly via the API.","title":"Why is it important?"},{"location":"main_features/intent_categorization/intent_categorization/#how-does-it-work","text":"User input is scored and bucketed based on the category title and description, and based on intents that have been manually moved into categories (self-learning). Once categorization is enabled, the Curate and Anaytlics screens will change to show groupings around cateegory. Categorization is not retroactive - meaning if you define a new category - we will not automatically re-run all old user input against the new categories. Users may move intents into categories manually thru the Curate tab or the CSV download/edit features. The edits made will be used to train the system for future categorization events.","title":"How does it work?"},{"location":"main_features/language_translation/language_translation/","text":"Overview What is it? NeuralSeek provides language translation that will let users call it to translate languages into different language. Why is it important? Any application that would need to translate a given text to another language can now use NeuralSeek to do it, rather than relying on other external translation services. How does it work? Translation is provided as REST API, and can be tested on NeuralSeek API documentation . Message payload is in JSON format, and contains array of text in certain language(s). Another attribute is target which specifies the target language the translation need to be performed. An example message would look something like this: { \"text\": [ \"NeuralSeek introduced several new features in July 2023, including streaming responses for web use cases, enhanced cross-lingual support, curate to CSV/upload curated QA from CSV, improved semantic match analysis, updated IBM WatsonX model compatibility, and AWS Lex round-trip monitoring.\" ], \"target\": \"ko\" } For more details on what language codes are supported, please refer to Multi Language Support . NeuralSeek would then translate the tiven text into the target language ko which is Korean: { \"word_count\": 39, \"character_count\": 289, \"translations\": [ \"NeuralSeek\uc740 2023\ub144 7\uc6d4\uc5d0 \uc6f9 \uc0ac\uc6a9 \uc0ac\ub840\ub97c \uc704\ud55c \uc2a4\ud2b8\ub9ac\ubc0d \uc751\ub2f5, \ud5a5\uc0c1\ub41c \uad50\ucc28 \uc5b8\uc5b4 \uc9c0\uc6d0, CSV\uc5d0 \ub300\ud55c \uc120\ubcc4/\uc120\ubcc4 QA \uc5c5\ub85c\ub4dc, \uac1c\uc120\ub41c \uc758\ubbf8 \uc77c\uce58 \ubd84\uc11d, \uc5c5\ub370\uc774\ud2b8\ub41c IBM WatsonX \ubaa8\ub378 \ud638\ud658\uc131 \ubc0f AWS Lex \uc655\ubcf5 \ubaa8\ub2c8\ud130\ub9c1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac00\uc9c0 \uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc744 \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4.\" ], \"detected_language\": \"en\", \"detected_language_confidence\": 0.9999967787054185 } You can also provide texts in different languages that can all be translated into the target language: { \"text\": [ \"soy un chico.\", \"\ub098\ub294 \uc18c\ub144\uc785\ub2c8\ub2e4.\", \"\u79c1\u306f\u7537\u306e\u5b50\u3067\u3059.\" ], \"target\": \"en\" } Which will be translated into en which is English: { \"word_count\": 6, \"character_count\": 30, \"translations\": [ \"I am a boy.\", \"I am a boy.\", \"I am a boy.\" ], \"detected_language\": \"es\", \"detected_language_confidence\": 0.95 }","title":"Language Translation"},{"location":"main_features/language_translation/language_translation/#overview","text":"","title":"Overview"},{"location":"main_features/language_translation/language_translation/#what-is-it","text":"NeuralSeek provides language translation that will let users call it to translate languages into different language.","title":"What is it?"},{"location":"main_features/language_translation/language_translation/#why-is-it-important","text":"Any application that would need to translate a given text to another language can now use NeuralSeek to do it, rather than relying on other external translation services.","title":"Why is it important?"},{"location":"main_features/language_translation/language_translation/#how-does-it-work","text":"Translation is provided as REST API, and can be tested on NeuralSeek API documentation . Message payload is in JSON format, and contains array of text in certain language(s). Another attribute is target which specifies the target language the translation need to be performed. An example message would look something like this: { \"text\": [ \"NeuralSeek introduced several new features in July 2023, including streaming responses for web use cases, enhanced cross-lingual support, curate to CSV/upload curated QA from CSV, improved semantic match analysis, updated IBM WatsonX model compatibility, and AWS Lex round-trip monitoring.\" ], \"target\": \"ko\" } For more details on what language codes are supported, please refer to Multi Language Support . NeuralSeek would then translate the tiven text into the target language ko which is Korean: { \"word_count\": 39, \"character_count\": 289, \"translations\": [ \"NeuralSeek\uc740 2023\ub144 7\uc6d4\uc5d0 \uc6f9 \uc0ac\uc6a9 \uc0ac\ub840\ub97c \uc704\ud55c \uc2a4\ud2b8\ub9ac\ubc0d \uc751\ub2f5, \ud5a5\uc0c1\ub41c \uad50\ucc28 \uc5b8\uc5b4 \uc9c0\uc6d0, CSV\uc5d0 \ub300\ud55c \uc120\ubcc4/\uc120\ubcc4 QA \uc5c5\ub85c\ub4dc, \uac1c\uc120\ub41c \uc758\ubbf8 \uc77c\uce58 \ubd84\uc11d, \uc5c5\ub370\uc774\ud2b8\ub41c IBM WatsonX \ubaa8\ub378 \ud638\ud658\uc131 \ubc0f AWS Lex \uc655\ubcf5 \ubaa8\ub2c8\ud130\ub9c1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac00\uc9c0 \uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc744 \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4.\" ], \"detected_language\": \"en\", \"detected_language_confidence\": 0.9999967787054185 } You can also provide texts in different languages that can all be translated into the target language: { \"text\": [ \"soy un chico.\", \"\ub098\ub294 \uc18c\ub144\uc785\ub2c8\ub2e4.\", \"\u79c1\u306f\u7537\u306e\u5b50\u3067\u3059.\" ], \"target\": \"en\" } Which will be translated into en which is English: { \"word_count\": 6, \"character_count\": 30, \"translations\": [ \"I am a boy.\", \"I am a boy.\", \"I am a boy.\" ], \"detected_language\": \"es\", \"detected_language_confidence\": 0.95 }","title":"How does it work?"},{"location":"main_features/multi_language_support/multi_language_support/","text":"Overview What is it? NeuralSeek has several different language options available for understanding questions and delivering answers. These include English, Spanish, Portuguese, French, German, Italian, Arabic, Korean, Chinese, Czech, Dutch, Indonesian, Japanese, and more. These can be adjusted on the \u201cConfigure\u201d section of the NeuralSeek console, or on the \u201cSeek\u201d endpoint. Please see the below table for the full list of supported languages. Supported Languages Language Lang code English en Match Input xx Arabic ar Basque eu Bengali bn Bosnian bs Bulgarian bg Catalan ca Chinese (Simplified) zh-cn Chinese (Traditional) zh-tw Croatian hr Czech cs Danish da Dutch nl Estonian et Finnish fi French fr German de Greek el Gujarati gu Hebrew he Hindi hi Hungarian hu Irish ga Indonesian id Italian it Japanese ja Kannada kn Korean ko Latvian lv Lithuanian lt Malay ms Malayalam ml Maltese mt Marathi mr Montenegrin cnr Nepali ne Norwegian Bokm\u00e5l nb Polish pl Portuguese pt-br Punjabi pa Romanian ro Russian ru Serbian sr Sinhala si Slovak sk Slovenian sl Spanish es Swedish sv Tamil ta Telugu te Thai th Turkish tr Ukrainian uk Urdu ur Vietnamese vi Welsh cy Match Input Feature: NeuralSeek can understand and support conversations that are initiated in languages other than the ones listed through the Match Input Feature. On the \u201cSeek\u201d endpoint, click the dropdown for language navigation and click \"Match Input\". Why is it important? Being able to automatically translate answers to an appropriate language without relying on virtual agent is important, as this greatly simplifies running it. Instead of having to train your virtual agents to understand various different languages, your question can be automatically converted into the response in the language of your choice. How does it work? NeuralSeek will try to determine if the user is asking question in certain language (e.g. Spanish), and will try to convert the responses into the language that the user asked without any additional set ups. Specifying a langauage If you would like to specify certain target language that you want NeuralSeek to generate answers into, you can do so by specifying a language code (e.g. es) in the request when you are invoking Seek . The same can be achieved when you are invoking Seek using REST API. You can specify the language under the options > language .","title":"Multi Language Support"},{"location":"main_features/multi_language_support/multi_language_support/#overview","text":"","title":"Overview"},{"location":"main_features/multi_language_support/multi_language_support/#what-is-it","text":"NeuralSeek has several different language options available for understanding questions and delivering answers. These include English, Spanish, Portuguese, French, German, Italian, Arabic, Korean, Chinese, Czech, Dutch, Indonesian, Japanese, and more. These can be adjusted on the \u201cConfigure\u201d section of the NeuralSeek console, or on the \u201cSeek\u201d endpoint. Please see the below table for the full list of supported languages.","title":"What is it?"},{"location":"main_features/multi_language_support/multi_language_support/#supported-languages","text":"Language Lang code English en Match Input xx Arabic ar Basque eu Bengali bn Bosnian bs Bulgarian bg Catalan ca Chinese (Simplified) zh-cn Chinese (Traditional) zh-tw Croatian hr Czech cs Danish da Dutch nl Estonian et Finnish fi French fr German de Greek el Gujarati gu Hebrew he Hindi hi Hungarian hu Irish ga Indonesian id Italian it Japanese ja Kannada kn Korean ko Latvian lv Lithuanian lt Malay ms Malayalam ml Maltese mt Marathi mr Montenegrin cnr Nepali ne Norwegian Bokm\u00e5l nb Polish pl Portuguese pt-br Punjabi pa Romanian ro Russian ru Serbian sr Sinhala si Slovak sk Slovenian sl Spanish es Swedish sv Tamil ta Telugu te Thai th Turkish tr Ukrainian uk Urdu ur Vietnamese vi Welsh cy Match Input Feature: NeuralSeek can understand and support conversations that are initiated in languages other than the ones listed through the Match Input Feature. On the \u201cSeek\u201d endpoint, click the dropdown for language navigation and click \"Match Input\".","title":"Supported Languages"},{"location":"main_features/multi_language_support/multi_language_support/#why-is-it-important","text":"Being able to automatically translate answers to an appropriate language without relying on virtual agent is important, as this greatly simplifies running it. Instead of having to train your virtual agents to understand various different languages, your question can be automatically converted into the response in the language of your choice.","title":"Why is it important?"},{"location":"main_features/multi_language_support/multi_language_support/#how-does-it-work","text":"NeuralSeek will try to determine if the user is asking question in certain language (e.g. Spanish), and will try to convert the responses into the language that the user asked without any additional set ups.","title":"How does it work?"},{"location":"main_features/multi_language_support/multi_language_support/#specifying-a-langauage","text":"If you would like to specify certain target language that you want NeuralSeek to generate answers into, you can do so by specifying a language code (e.g. es) in the request when you are invoking Seek . The same can be achieved when you are invoking Seek using REST API. You can specify the language under the options > language .","title":"Specifying a langauage"},{"location":"main_features/pii_detection/pii_detection/","text":"Overview What is it? NeuralSeek features an advanced Personal Identifiable Information (PII) detection routine that automatically identifies any PII within user inputs. It allows users to flag, mask, hide, or delete the detected PII. Why is it important? Users can maintain a secure environment while providing accurate responses to user queries, ensuring compliance with data privacy regulations and protecting sensitive information. How does it work? Common and well known PII detection is enabled by default in NeuralSeek. When you enter a PII information, for example, when you enter a credit card number in Seek: In NeuralSeek, the question above will be logged and flagged as containing PII information and warns user about a potential risk. The credit card number is also masked and removed, so that the data is protected from from being viewed. The answers to these questions also indicate that they were generated from a question with PII in it, so that you can easily identify them. Defining a specific PII However, this is what NeuralSeek does against common PII patterns, and there may be a specific PII that you would like to hide for your specific business needs. If you want to help NeuralSeek better detect and process PII for that, you can configure it under Configure > Personal Identifiable Information (PII) Handling in the top menu: How it works is based on example sentence, and does not have to be an exact pattern or rules. For example, setting the example sentence as: My name is Howard Yoo and my blood type is O, and I live in Chicago. For each PII element in that sentence, you can define the PII elements in that sentence delimited by comma as such: Howard Yoo, 0 So, next time, when somebody enters a PII matching the example as such: This is my blood type: A NeuralSeek now detects that and masks the blood type that the user provided from being exposed: Ignoring certain PII You can also make NeuralSeek ignore certain PII by entering \u201cNo PII\u201d to the element. For example, by setting the element as \u201cNo PII\u201d with a given example sentence, NeuralSeek will not filter out the question even though it would contain an PII element: Therefore, when asked about a similar question, notice how dog\u2019s name is now visible as not a PII information: The base reason to use this is that sometimes, NeuralSeek would mistake certain question to be containing PII, even though the sentence may clearly not contain any such data. In that case, setting what not to consider as PII would be very helpful.","title":"PII Detection"},{"location":"main_features/pii_detection/pii_detection/#overview","text":"","title":"Overview"},{"location":"main_features/pii_detection/pii_detection/#what-is-it","text":"NeuralSeek features an advanced Personal Identifiable Information (PII) detection routine that automatically identifies any PII within user inputs. It allows users to flag, mask, hide, or delete the detected PII.","title":"What is it?"},{"location":"main_features/pii_detection/pii_detection/#why-is-it-important","text":"Users can maintain a secure environment while providing accurate responses to user queries, ensuring compliance with data privacy regulations and protecting sensitive information.","title":"Why is it important?"},{"location":"main_features/pii_detection/pii_detection/#how-does-it-work","text":"Common and well known PII detection is enabled by default in NeuralSeek. When you enter a PII information, for example, when you enter a credit card number in Seek: In NeuralSeek, the question above will be logged and flagged as containing PII information and warns user about a potential risk. The credit card number is also masked and removed, so that the data is protected from from being viewed. The answers to these questions also indicate that they were generated from a question with PII in it, so that you can easily identify them.","title":"How does it work?"},{"location":"main_features/pii_detection/pii_detection/#defining-a-specific-pii","text":"However, this is what NeuralSeek does against common PII patterns, and there may be a specific PII that you would like to hide for your specific business needs. If you want to help NeuralSeek better detect and process PII for that, you can configure it under Configure > Personal Identifiable Information (PII) Handling in the top menu: How it works is based on example sentence, and does not have to be an exact pattern or rules. For example, setting the example sentence as: My name is Howard Yoo and my blood type is O, and I live in Chicago. For each PII element in that sentence, you can define the PII elements in that sentence delimited by comma as such: Howard Yoo, 0 So, next time, when somebody enters a PII matching the example as such: This is my blood type: A NeuralSeek now detects that and masks the blood type that the user provided from being exposed:","title":"Defining a specific PII"},{"location":"main_features/pii_detection/pii_detection/#ignoring-certain-pii","text":"You can also make NeuralSeek ignore certain PII by entering \u201cNo PII\u201d to the element. For example, by setting the element as \u201cNo PII\u201d with a given example sentence, NeuralSeek will not filter out the question even though it would contain an PII element: Therefore, when asked about a similar question, notice how dog\u2019s name is now visible as not a PII information: The base reason to use this is that sometimes, NeuralSeek would mistake certain question to be containing PII, even though the sentence may clearly not contain any such data. In that case, setting what not to consider as PII would be very helpful.","title":"Ignoring certain PII"},{"location":"main_features/round_trip_logging/round_trip_logging/","text":"Overview What is it? Round-trip logging is a process that involves recording and storing all interactions between a user and a Virtual Agent. NeuralSeek supports receiving logs from Virtual Agents in order to monitor curated responses. This includes the user\u2019s question, the Virtual Agent\u2019s response, and any follow-up questions or clarifications. Why is it important? The purpose of round-trip logging is to improve the Virtual Agent\u2019s performance by alerting to content in the Virtual Agent that is likely out of date, because the source documentation has changed. How does it work? The Source Virtual Agent is connected to NeuralSeek via the specific instructions per platform on the Integrate tab. Once connected, NeuralSeek will monitor for intents that are being used live in the Virtual Agent. Once per day NeuralSeek will search the connected KnowledgeBase and recompute the hash for the returned data. That hash will be compared to the hash of the answers stored, and if no match is found, an alert will be generated notifying that the source documentation has changed compared to the last Answer generation completed by the seek endpoint.","title":"Round Trip Logging"},{"location":"main_features/round_trip_logging/round_trip_logging/#overview","text":"","title":"Overview"},{"location":"main_features/round_trip_logging/round_trip_logging/#what-is-it","text":"Round-trip logging is a process that involves recording and storing all interactions between a user and a Virtual Agent. NeuralSeek supports receiving logs from Virtual Agents in order to monitor curated responses. This includes the user\u2019s question, the Virtual Agent\u2019s response, and any follow-up questions or clarifications.","title":"What is it?"},{"location":"main_features/round_trip_logging/round_trip_logging/#why-is-it-important","text":"The purpose of round-trip logging is to improve the Virtual Agent\u2019s performance by alerting to content in the Virtual Agent that is likely out of date, because the source documentation has changed.","title":"Why is it important?"},{"location":"main_features/round_trip_logging/round_trip_logging/#how-does-it-work","text":"The Source Virtual Agent is connected to NeuralSeek via the specific instructions per platform on the Integrate tab. Once connected, NeuralSeek will monitor for intents that are being used live in the Virtual Agent. Once per day NeuralSeek will search the connected KnowledgeBase and recompute the hash for the returned data. That hash will be compared to the hash of the answers stored, and if no match is found, an alert will be generated notifying that the source documentation has changed compared to the last Answer generation completed by the seek endpoint.","title":"How does it work?"},{"location":"main_features/semantic_analytics/semantic_analytics/","text":"# Overview What is it? NeuralSeek generates responses by directly utilizing content from corporate sources. In order to ensure transparency between the sources and answers, NeuralSeek reveals the specific origin of the words and phrases that are generated. Clarity is further achieved by employing semantic match scores. These scores compare the generated response with the ground truth documentation, providing a clear understanding of the alignment between the response and the meaning conveyed in source documents. This ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek. Why is it important? By being able to analyze how the answer generated would be originated from the actual facts given by the KnowledgeBase, users can analyze from which sources the responses actually originated from, and how much of the responses are directly coming from the knowledge versus how much of them are from LLM\u2019s generated answer. This ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek. For example, NeuralSeek\u2019s Seek will provide rich semantic analytics in terms of how well the response cover for the facts found int he KnowledgeBase (or cached generated answers) by color-coding the area of it in the response, visually linking it to the sources, and providing semantic analysis result to explain the key reasons behind the semantic match score given. How does it work? When NeuralSeek receives a question, it will first try to match for existing intents and answers, it will also try to search the underlying corporate KnowledgeBase and return any relevant passages from number of sources. NeuralSeek will then either use these answers as-is directly, or use parts of the information to form a response using LLM\u2019s generative AI capability. Configuring Semantic Analytics Configuration option for Semantic analysis is found under Configure > Confidence & Warning Thresholds . The semantic score model is enabled by default, but you can also disable it. You can also enable whether the semantic analysis should be used for confidence, and for reranking the search results from the knowledge base according to how much they semantically match. There are also sections for controlling how the analysis can apply penalties for missing key terms, search terms, or how frequent the sources are jumped (fragmented in the generated answer). \u2753 How re-ranking the search result using semantic analysis can be helpful? Having an option to re-rank the resulting KnowledgeBase search results can ensure the list of search results to appear in the order that corresponds better to the answer provided. That is because sometimes the search results returned from the KnowledgeBase does not align perfectly with the answer, and thus provided URL of the resulting document can be misleading. Using Semantic Analysis In the \u2018Seek\u2019 tab of NeuralSeek, you can provide a question, and be given an answer from NeuralSeek. When enabling the \u2018Provenance\u2019, this will give you the color-coded portion of the response that were directly originated from those results. Below the answer, you will see some of the key insights related to the answer, such as Semantic Match score (in %) , Semantic Analysis , as well as results coming from KnowledgeBase in terms of KB Confidence , KB Coverage , KB Response Time , and KB Results . Semantic Match % is the overall match \u2018score\u2019 that indicates how much NeuralSeek believes that the responses are well aligned with the underlying ground truth (from KnowledgeBase). The higher the % is, the more accurate and relevant the answer is based on the truth. Semantic Analysis summarizes in easy to understand way to explain why NeuralSeek calculated the matching score. Just by reading this would give users a good understanding why the answer was given either a high or low score. Knowledge confidence, coverage, response time, and results, are all coming from the KnowledgeBase itself, and represents how much confidence and coverage that the KnowledgeBase thinks the retrieved sources are related to the given question. KnowledgeBase contexts are the \u2018snippets\u2019 of sources that the KnowledgeBase, based on the relevance of what it found within its data. Clicking one of them would reveal the found passage, and the color code matching to that of the generated answer would be used to highlight the parts that were used. Lastly, the stump speech that is defined in NeuralSeek\u2019s configuration, is shown and also color-code highlighted based on how much of it also got used in the answer. If you are wondering where the Stump Speech is stored, you can find it in Configure > Company / Organization Preferences section: Setting the Date Penalty or Score Range The resulting KnowledgeBase result does get affected by the configuration that you set for the corporate KnowledgeBase you are using with NeuralSeek. You can find these settings in Configure > Corporate KnowledgeBase Details section: Document score range dictates the range of possible \u2018relevancy scores\u2019 that it will return as the result. For example, if the score range is 80%, the results will be of relevancy score higher than 20% and equal or lower than 100%. If the score range is 20%, the relevancy score range would then be anything between 80% ~ 100%, respectively. Document Date Penalty, if specified higher than 0%, will start to impose penalty scores to reduce the relevancy based on how old the information is coming from. KnowledgeBase will try to find any time related information in the document and would reduce the score based on how old the time is, relative to current time. When the results says, 4 filtered by date penalty or score range, it means these settings came to play when retrieving relevant information from the KnowledgeBase. Examples of Semantic Analysis High score example Medium score example Low score example","title":"Semantic Analytics"},{"location":"main_features/semantic_analytics/semantic_analytics/#overview","text":"","title":"# Overview"},{"location":"main_features/semantic_analytics/semantic_analytics/#what-is-it","text":"NeuralSeek generates responses by directly utilizing content from corporate sources. In order to ensure transparency between the sources and answers, NeuralSeek reveals the specific origin of the words and phrases that are generated. Clarity is further achieved by employing semantic match scores. These scores compare the generated response with the ground truth documentation, providing a clear understanding of the alignment between the response and the meaning conveyed in source documents. This ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek.","title":"What is it?"},{"location":"main_features/semantic_analytics/semantic_analytics/#why-is-it-important","text":"By being able to analyze how the answer generated would be originated from the actual facts given by the KnowledgeBase, users can analyze from which sources the responses actually originated from, and how much of the responses are directly coming from the knowledge versus how much of them are from LLM\u2019s generated answer. This ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek. For example, NeuralSeek\u2019s Seek will provide rich semantic analytics in terms of how well the response cover for the facts found int he KnowledgeBase (or cached generated answers) by color-coding the area of it in the response, visually linking it to the sources, and providing semantic analysis result to explain the key reasons behind the semantic match score given.","title":"Why is it important?"},{"location":"main_features/semantic_analytics/semantic_analytics/#how-does-it-work","text":"When NeuralSeek receives a question, it will first try to match for existing intents and answers, it will also try to search the underlying corporate KnowledgeBase and return any relevant passages from number of sources. NeuralSeek will then either use these answers as-is directly, or use parts of the information to form a response using LLM\u2019s generative AI capability.","title":"How does it work?"},{"location":"main_features/semantic_analytics/semantic_analytics/#configuring-semantic-analytics","text":"Configuration option for Semantic analysis is found under Configure > Confidence & Warning Thresholds . The semantic score model is enabled by default, but you can also disable it. You can also enable whether the semantic analysis should be used for confidence, and for reranking the search results from the knowledge base according to how much they semantically match. There are also sections for controlling how the analysis can apply penalties for missing key terms, search terms, or how frequent the sources are jumped (fragmented in the generated answer). \u2753 How re-ranking the search result using semantic analysis can be helpful? Having an option to re-rank the resulting KnowledgeBase search results can ensure the list of search results to appear in the order that corresponds better to the answer provided. That is because sometimes the search results returned from the KnowledgeBase does not align perfectly with the answer, and thus provided URL of the resulting document can be misleading.","title":"Configuring Semantic Analytics"},{"location":"main_features/semantic_analytics/semantic_analytics/#using-semantic-analysis","text":"In the \u2018Seek\u2019 tab of NeuralSeek, you can provide a question, and be given an answer from NeuralSeek. When enabling the \u2018Provenance\u2019, this will give you the color-coded portion of the response that were directly originated from those results. Below the answer, you will see some of the key insights related to the answer, such as Semantic Match score (in %) , Semantic Analysis , as well as results coming from KnowledgeBase in terms of KB Confidence , KB Coverage , KB Response Time , and KB Results . Semantic Match % is the overall match \u2018score\u2019 that indicates how much NeuralSeek believes that the responses are well aligned with the underlying ground truth (from KnowledgeBase). The higher the % is, the more accurate and relevant the answer is based on the truth. Semantic Analysis summarizes in easy to understand way to explain why NeuralSeek calculated the matching score. Just by reading this would give users a good understanding why the answer was given either a high or low score. Knowledge confidence, coverage, response time, and results, are all coming from the KnowledgeBase itself, and represents how much confidence and coverage that the KnowledgeBase thinks the retrieved sources are related to the given question. KnowledgeBase contexts are the \u2018snippets\u2019 of sources that the KnowledgeBase, based on the relevance of what it found within its data. Clicking one of them would reveal the found passage, and the color code matching to that of the generated answer would be used to highlight the parts that were used. Lastly, the stump speech that is defined in NeuralSeek\u2019s configuration, is shown and also color-code highlighted based on how much of it also got used in the answer. If you are wondering where the Stump Speech is stored, you can find it in Configure > Company / Organization Preferences section:","title":"Using Semantic Analysis"},{"location":"main_features/semantic_analytics/semantic_analytics/#setting-the-date-penalty-or-score-range","text":"The resulting KnowledgeBase result does get affected by the configuration that you set for the corporate KnowledgeBase you are using with NeuralSeek. You can find these settings in Configure > Corporate KnowledgeBase Details section: Document score range dictates the range of possible \u2018relevancy scores\u2019 that it will return as the result. For example, if the score range is 80%, the results will be of relevancy score higher than 20% and equal or lower than 100%. If the score range is 20%, the relevancy score range would then be anything between 80% ~ 100%, respectively. Document Date Penalty, if specified higher than 0%, will start to impose penalty scores to reduce the relevancy based on how old the information is coming from. KnowledgeBase will try to find any time related information in the document and would reduce the score based on how old the time is, relative to current time. When the results says, 4 filtered by date penalty or score range, it means these settings came to play when retrieving relevant information from the KnowledgeBase.","title":"Setting the Date Penalty or Score Range"},{"location":"main_features/semantic_analytics/semantic_analytics/#examples-of-semantic-analysis","text":"","title":"Examples of Semantic Analysis"},{"location":"main_features/semantic_analytics/semantic_analytics/#high-score-example","text":"","title":"High score example"},{"location":"main_features/semantic_analytics/semantic_analytics/#medium-score-example","text":"","title":"Medium score example"},{"location":"main_features/semantic_analytics/semantic_analytics/#low-score-example","text":"","title":"Low score example"},{"location":"main_features/sentiment_analysis/sentiment_analysis/","text":"Overview What is it? NeuralSeek's sentiment analysis is a feature that allows users to analyze the sentiment or emotional tone of a piece of text. It can determine whether the sentiment expressed in the text is positive, negative, or neutral. NeuralSeek's sentiment analysis is based on advanced natural language processing techniques and can provide valuable insights for businesses and organizations. Why is it important? By being able to detect whether user is negative or positive about certain questions, you can let the virtual agent use this information to provide more tailored services. For example, for a user who expresses negative sentiment, virtual agent might forward the session to human agents or assign higher priority so that more attention could be provided. How does it work? NeuralSeek will run sentiment analysis on the user\u2019s input text. Sentiment is returned as an integer between zero (0) and nine (9), with zero (0) being the most negative, nine (9) being the most positive, and five (5) being neutral. Sentiment Analysis REST API When using REST API, for example, providing a negative comments could trigger a low sentiment analysis score. { \"question\": \"I don't like NeuralSeek\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, Would yield a response with low sentiment score: { \"answer\": \"Stringi'm sorry to hear that you don't like NeuralSeek. If you have any specific concerns or feedback, please let me know and I'll do my best to assist you.\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 3, \"totalCount\": 9, \"KBscore\": 3, \"score\": 3, \"url\": \"https://neuralseek.com/faq\", \"document\": \"FAQ - NeuralSeek\", \"kbTime\": 454, \"kbCoverage\": 24, \"time\": 2688 } Notice the sentiment score of 3, which is in the low range of 0 - 10. On the other hand, if you express a positive sentiment as such: { \"question\": \"I really love NeuralSeek. It's the best software in the world.\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, The response will have a higher sentiment score: { \"answer\": \"Thank you for sharing your positive feedback about NeuralSeek. I cannot have personal opinions, but I'm glad to hear that you find NeuralSeek to be the best software in the world.\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 9, \"totalCount\": 9, \"KBscore\": 15, \"score\": 15, \"url\": \"https://neuralseek.com/faq\", \"document\": \"FAQ - NeuralSeek\", \"kbTime\": 5385, \"kbCoverage\": 8, \"time\": 7094 }","title":"Sentiment Analysis"},{"location":"main_features/sentiment_analysis/sentiment_analysis/#overview","text":"","title":"Overview"},{"location":"main_features/sentiment_analysis/sentiment_analysis/#what-is-it","text":"NeuralSeek's sentiment analysis is a feature that allows users to analyze the sentiment or emotional tone of a piece of text. It can determine whether the sentiment expressed in the text is positive, negative, or neutral. NeuralSeek's sentiment analysis is based on advanced natural language processing techniques and can provide valuable insights for businesses and organizations.","title":"What is it?"},{"location":"main_features/sentiment_analysis/sentiment_analysis/#why-is-it-important","text":"By being able to detect whether user is negative or positive about certain questions, you can let the virtual agent use this information to provide more tailored services. For example, for a user who expresses negative sentiment, virtual agent might forward the session to human agents or assign higher priority so that more attention could be provided.","title":"Why is it important?"},{"location":"main_features/sentiment_analysis/sentiment_analysis/#how-does-it-work","text":"NeuralSeek will run sentiment analysis on the user\u2019s input text. Sentiment is returned as an integer between zero (0) and nine (9), with zero (0) being the most negative, nine (9) being the most positive, and five (5) being neutral.","title":"How does it work?"},{"location":"main_features/sentiment_analysis/sentiment_analysis/#sentiment-analysis-rest-api","text":"When using REST API, for example, providing a negative comments could trigger a low sentiment analysis score. { \"question\": \"I don't like NeuralSeek\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, Would yield a response with low sentiment score: { \"answer\": \"Stringi'm sorry to hear that you don't like NeuralSeek. If you have any specific concerns or feedback, please let me know and I'll do my best to assist you.\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 3, \"totalCount\": 9, \"KBscore\": 3, \"score\": 3, \"url\": \"https://neuralseek.com/faq\", \"document\": \"FAQ - NeuralSeek\", \"kbTime\": 454, \"kbCoverage\": 24, \"time\": 2688 } Notice the sentiment score of 3, which is in the low range of 0 - 10. On the other hand, if you express a positive sentiment as such: { \"question\": \"I really love NeuralSeek. It's the best software in the world.\", \"context\": {}, \"user_session\": { \"metadata\": { \"user_id\": \"string\" }, \"system\": { \"session_id\": \"string\" } }, The response will have a higher sentiment score: { \"answer\": \"Thank you for sharing your positive feedback about NeuralSeek. I cannot have personal opinions, but I'm glad to hear that you find NeuralSeek to be the best software in the world.\", \"cachedResult\": false, \"langCode\": \"string\", \"sentiment\": 9, \"totalCount\": 9, \"KBscore\": 15, \"score\": 15, \"url\": \"https://neuralseek.com/faq\", \"document\": \"FAQ - NeuralSeek\", \"kbTime\": 5385, \"kbCoverage\": 8, \"time\": 7094 }","title":"Sentiment Analysis REST API"},{"location":"main_features/table_understanding/table_understanding/","text":"Overview What is it? Table Extraction, also known as Table understanding , pre-processes your documents to extract and parse table data into a format suitable for conversational queries. Since this preparation process is both costly and time-consuming , this feature is opt-in and will consume 1 seek query for every table preprocessed. Also, it should be noted that Web Crawl Collections are not eligible for table understanding, as the recrawl interval will cause excessive compute usage. Table preparation time takes several minutes per page. Why is it important? Being able to understand data in tabular structure in documents and generating answers is an important capability for NeuralSeek in order to better find the relevant data for answering. How does it work? To find table extraction, open up your instance of NeuralSeek and head over to the Configure . Select Table understanding \u26a0\ufe0f Note for users of lite/trial plans - to be able to access and use this feature you will have to contact cloud@cerebralblue.com with details of your opportunity and use case to be eligible. Once you have everything set, go over to Watson Discovery , and if you don\u2019t already, create a project and import a pdf file that contains some tables. Once you have the project copy the API information and go back to the Configure in NeuralSeek. Scroll down to Table Understanding, paste the project id, hit save, and proceed to the Seek tab. With everything set, ask some questions related to the data inside the table in the PDF file. What were the GHG emissions for business travel in 2021? You can also ask questions about a specific place or name and if there is multiple tables with data. NeuralSeek will take from each table and provide you with everything.","title":"Table Understanding"},{"location":"main_features/table_understanding/table_understanding/#overview","text":"","title":"Overview"},{"location":"main_features/table_understanding/table_understanding/#what-is-it","text":"Table Extraction, also known as Table understanding , pre-processes your documents to extract and parse table data into a format suitable for conversational queries. Since this preparation process is both costly and time-consuming , this feature is opt-in and will consume 1 seek query for every table preprocessed. Also, it should be noted that Web Crawl Collections are not eligible for table understanding, as the recrawl interval will cause excessive compute usage. Table preparation time takes several minutes per page.","title":"What is it?"},{"location":"main_features/table_understanding/table_understanding/#why-is-it-important","text":"Being able to understand data in tabular structure in documents and generating answers is an important capability for NeuralSeek in order to better find the relevant data for answering.","title":"Why is it important?"},{"location":"main_features/table_understanding/table_understanding/#how-does-it-work","text":"To find table extraction, open up your instance of NeuralSeek and head over to the Configure . Select Table understanding \u26a0\ufe0f Note for users of lite/trial plans - to be able to access and use this feature you will have to contact cloud@cerebralblue.com with details of your opportunity and use case to be eligible. Once you have everything set, go over to Watson Discovery , and if you don\u2019t already, create a project and import a pdf file that contains some tables. Once you have the project copy the API information and go back to the Configure in NeuralSeek. Scroll down to Table Understanding, paste the project id, hit save, and proceed to the Seek tab. With everything set, ask some questions related to the data inside the table in the PDF file. What were the GHG emissions for business travel in 2021? You can also ask questions about a specific place or name and if there is multiple tables with data. NeuralSeek will take from each table and provide you with everything.","title":"How does it work?"},{"location":"user_interface/user_interface/","text":"User Interface NeuralSeek's UI capabilities include a clickable path to fact-check AI responses, data analytics to enhance AI natural language capabilities, and step-by-step instructions for maintaining accurate resource data. It comes with a very intuitive UI that can be used by business users with no coding or development required. The UI also includes an Analytics tab to track confidence and coverage scores of different actions, and configuration pages to control its behavior.","title":"Overview"},{"location":"user_interface/user_interface/#user-interface","text":"NeuralSeek's UI capabilities include a clickable path to fact-check AI responses, data analytics to enhance AI natural language capabilities, and step-by-step instructions for maintaining accurate resource data. It comes with a very intuitive UI that can be used by business users with no coding or development required. The UI also includes an Analytics tab to track confidence and coverage scores of different actions, and configuration pages to control its behavior.","title":"User Interface"},{"location":"user_interface/analytics/analytics/","text":"Overview Users can work backwards from the Analytics tab to see and understand where the confidence and coverage scores of different actions are coming from. The analytics feature reveals the relationship between each action\u2019s coverage and confidence scores, and can direct users to each specific action found in the \u201cCurate\u201d section of the UI. You can see the changes up to 30 days, and also see the trend of whether the coverage or confidence has gone up or down in the past. For auto-categorization of intents, check out Intent Categorization","title":"Analytics"},{"location":"user_interface/analytics/analytics/#overview","text":"Users can work backwards from the Analytics tab to see and understand where the confidence and coverage scores of different actions are coming from. The analytics feature reveals the relationship between each action\u2019s coverage and confidence scores, and can direct users to each specific action found in the \u201cCurate\u201d section of the UI. You can see the changes up to 30 days, and also see the trend of whether the coverage or confidence has gone up or down in the past. For auto-categorization of intents, check out Intent Categorization","title":"Overview"},{"location":"user_interface/configure/configure/","text":"Overview This location of NeuralSeek is where users can edit the configurations for NeuralSeek\u2019s features. There are two types of configurations: Default Configurations and Advanced Configurations. Default Configurations KnowledgeBase Connection: Users can change their KnowledgeBase type along with the associated url, API keys, project ID, and other relevant information. Use the drop down arrows to manually configure the fields of the schema in the connected KnowledgeBase. Curation data field: select where your FAQ content/document body is located. Document name field: select what is displayed for the title. Link field: select the URL below the title, or what will be served in the Virtual Agent chat bubble as a link. Filter field: select the field you want to filter by. For example, you can filter for a specfic document type in your KnowledgeBase and only return PDF's. Users have the option to enable or diasble attributing sources inside LLM context by document name. For example, when disabled the output will be formatted with only the document contents. When enabled, the output will be formatted with an introductory sentance that attritubes the 'document content' to the appropriate document 'name' (e.g. \"The document 'name' states that: 'document content'). Users are able to enter a prioritized list of values that you want to re-rank above other results, regardless of KB score. This can be modified under the Re-Sort values list menu. See our supported KnowledgeBase page for more information on supported KnowledgeBases. LLM Details: This is where users can add a LLM of choice, and set or modify their LLM model settings. By clicking \"Add an LLM\", users will be prompted to select an LLM from their platform of choice and enter the relevant information such as API keys, endpoint URLs, project ID's, etc. Use the drop down arrow to enable languages of choice: there are a total of 56 supported LLM languages. Users can also modify which NeuralSeek functions to enable for the added LLM. Note that you must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled. This section is only available if you are using BYOLLM (bring your own LLM) plan of NeuralSeek . Company/Organization Preferences: This is where you can enter your company name, and description of what the company primarily focuses on. Note that the description is also used as a stump speech which is a block of text to be used to help generate a response when all else fails, and the answer needs to fall back to it. Users can also select whether to add or not add Company Response Affinity which adds affinity to the company on top of any affinity that may be already present in your KnowledgeBase and Stump Speeches. Platform Preferences: This is where you can set up your primary default language (e.g. English), whether the response would contain the embedded link into returned responses, a list of custom stopwords if you want to override the NeuralSeek default stopwords, and also a virtual agent type that NeuralSeek can generate its questions and answers to. See our supported Virtual Agent platforms page . Intent Categorization: This is where you can create type of categories and their descriptions to control how some of the intents in user question can be categorized into. Usually a question would be automatically categorized as FAQ , but you can provide additional ones here. Governance & Guardrails: Users are able to adjust the following: Attribute Checking: Adjust misinformation tolerance for generating text about the company, or associating people or things that lack specific references in the KnowledgeBase material by using the sliding scale from \"Rigid\" to \"Standard\". The more rigid your setting, the higher the chances of occasionally blocking legitimate questions that use alternate wording or are poorly documented in your KnowledgeBase. Semantic Score: The Semantic Score model checks the generated answer against the KnowledgeBase sources, and rates the answer based on the quantity and focus of the KnowledgeBase and Stump Speech source documentation (e.g. is the answer primarily formed from a single source or many?) Toggle the icons to enable or disable the Semantic Score Model, using Semantic Score as the basis for Warning & Minimum confidence (e.g. Do NOT Enable for usecases requiring language translation), reranking the search results based on the Semantic Match, and checking the document titles and URL's as part of the Semantic Match. Note that semantic scoring will not be accurate when getting answers in a diferent language than your KnowledgeBase source docs. Semantic Model Tuning: use the sliding scales to further tune the Semantic Match. Missing key search term penalty: After scoring, this penalty is applied for answers that are missing KnowledgeBase attribution of proper nouns that were included in the search. Missing search term penalty: After scoring, this penalty is applied for answers that are missing KnowledgeBase attribution of other nouns that were included in the search. Source Jump penalty: When anwers join across many source documents it can be an indication of lost meaning or intent, depending on your source documentation. Total Coverage weight: Looking at the answer, how much weight should be given to the total coverage alone, regardless of other penalty. Increasing this helps prevent abnormally low scores from long, highly-stitched answers. Decreasing will better catch hallucination in short answers. ReRanK Min Coverage %: What is the minimum coverage of the total answer that the top used source document needs to be reranked over the top KB-scored document. Profanity Filter: Users are able to toggle the icon to enable or disable the profanity filter, as well as add a text to reply with for sensitive questions that are blocked. (e.g. \"That seems like a sensitive question. Maybe I'm not understanding you, so try rephrasing.\"). Warning Confidence: Use the sliding scale to increase the confidence percentage for warning. Add a text to prepend a warning on low confidence results. (e.g. \"I'm not an expert in this, but...\"). Minimum Confidence: Use the sliding scale to increase the minimum confidence percentage and the minimum confidence percentage to display a URL. Add a text to reply with for questions not meeting the minimum confidence, and select whether to add a URL fallback on minimum. (e.g. \"There is nothing in our knowledge base about that.\"). Minimum Text: Use the sliding scale to set a desired minimum amount of words in a question. Add a text to reply with for questions not meeting the minimum input text length. (e.g. \"Give me a bit more to go on...\"). Maximum Length: Use the sliding scale to set a desired maximum amount of words in a question. Use a low limit to help mitigate adversarial questions designed to generate inappropriate answers. Set to 100 to remove the limit. Add a text to reply with for questions over the input word limit. (e.g. \"Can you please summarize your question for me? Questions should be limited to 20 words.\"). Advanced Configurations KnowledgeBase Tuning: Tuning your Knowledgebase is an important part of creating a well performing system. Start by entering a seek query on the Seek tab to review the answer provided, as well as look at the documentation in the accordions below the answer. If the answer is not suitable for your needs, first check if the top document is correct and complete. If not, adjust snippet size by using the slider bars to improve KB training. Secondly, check if your answers are bringing back more irrelevant documentation than you need. If so, use the slider bars to set a max documents per seek or use the sliding scale to set a lower document score upper range. Further, you can adjust the document date penalty to ensure relevancy, and adjust the cache timeout in minutes but using the KnowledgeBase Query Cache (minutes) sliding scale. Prompt Engineering: Prompt Engineering allows expert users to inject specific instructions into the LLM prompt. Most use cases will not need this and should not use this. (e.g. do not enter \"provide factual information\" or \"act as a helpful customer support agent\".) NeuralSeek's extensive prompting already does this. Do not casually enable, as you can weaken the safeguards and extensive prompting that NeuralSeek provides out-of-the-box. Do not use any language other than English in prompt engineering. Answer Engineering & Preferences: Users are able to use the sliding scale to set whether the answer generation would stick to being concise, or can have more freedom to be flexible. Toggle the icon to True or False to force answers from the Knowledgebase if desired. Also, as part of answer engineering, you can provide regular expression and replacement pair so that certain pattern of data or sensitive information can be replace or removed as necessary. Answer Engineering uses Javascript Regular Expressions to selectivley replace text in both the KnowledgeBase training data and the live generated answer. (e.g. Use this to remove or swap phone numbers, emails, etc.). Intent Matching & Cache Configuration: Here is where you can configure strategies to perform intent matching. The following types of intent match's are available: Exact Match, Vector Similarity, Fuzzy Match, Keyword Match, and Fuzzy Keyword Match. Users can also configure how the answer caching is to be done for edited answers, and normal answers. You can control the number of answers that would trigger the cache, as well as their individual matching methods by using the sliding scales. Toggle the icon to Yes or No if desired to require the cache to follow context as well. Table Understanding: Table understanding pre-processes your documents to extract and parse tabular data into a format suitable for conversational query. Since this preparation process is both costly and time consuming, this feature is opt-in and will consume 1 seek query for every table preprocessed. Web Crawl Collections are not eligible for table understanding, as the re-crawl interval will cause excessive compute usage. Table preparation time takes several minutes per page. Please contact cloud@cerebralblue.com with details of your opportunity and use case to be considered for access. Note that Table Understanding requires a compatible LLM with Table Understanding Enabled. Not all LLM's are capable of Table Understanding. Personal Identifiable Information (PII) Handling: Users can define how to handle any detected PII data that was included in the question. The following options are available: Mask, Flag, No Action, Hide (retain for Analytics), and Delete (including from Analytics). The configuration also lets you add any particular examples of PII data, so it can be better detected, or set as No PII so that it can be ignored. Corporate Document Filter: Connect NeuralSeek to an external corporate rules engine to filter allowed documentation by user. Each request will send the id's of the found documentation to a endpoint you set here. Any ID's not returned back from the corporate filter will be blocked. Toggle the icon to Enable to Disable this feature. If enabled, fill out all relevant information including: Base URL for the corporate filter (get), URL paramenter for the UserName, URL paramenter for the KB field, and the Knowledgebase field to send. Corporate Logging: Connect NeuralSeek to a corporate audit logging endpoint. When connected and enabled, all requests and responses to the Seek api endpoint, as well as the Curate tab will be logged to your elasticSearch instance. Toggle the icon to Enable to Disable this feature. If enabled, fill out all relevant information including: ElasticSearch Endpoint and ElasticSearch API Key. Advanced Setting Options Download Settings: Click this to download a .dat copy of all settings in the configure tab to your local machine. Upload Settings: Click this to upload and restore settings from a .dat copy backup of all settings in the configure tab from your local machine. Change Logs: Click to view the change log history of all settings changed in this instance for auditing and debugging purposes.","title":"Configure"},{"location":"user_interface/configure/configure/#overview","text":"This location of NeuralSeek is where users can edit the configurations for NeuralSeek\u2019s features. There are two types of configurations: Default Configurations and Advanced Configurations.","title":"Overview"},{"location":"user_interface/configure/configure/#default-configurations","text":"KnowledgeBase Connection: Users can change their KnowledgeBase type along with the associated url, API keys, project ID, and other relevant information. Use the drop down arrows to manually configure the fields of the schema in the connected KnowledgeBase. Curation data field: select where your FAQ content/document body is located. Document name field: select what is displayed for the title. Link field: select the URL below the title, or what will be served in the Virtual Agent chat bubble as a link. Filter field: select the field you want to filter by. For example, you can filter for a specfic document type in your KnowledgeBase and only return PDF's. Users have the option to enable or diasble attributing sources inside LLM context by document name. For example, when disabled the output will be formatted with only the document contents. When enabled, the output will be formatted with an introductory sentance that attritubes the 'document content' to the appropriate document 'name' (e.g. \"The document 'name' states that: 'document content'). Users are able to enter a prioritized list of values that you want to re-rank above other results, regardless of KB score. This can be modified under the Re-Sort values list menu. See our supported KnowledgeBase page for more information on supported KnowledgeBases. LLM Details: This is where users can add a LLM of choice, and set or modify their LLM model settings. By clicking \"Add an LLM\", users will be prompted to select an LLM from their platform of choice and enter the relevant information such as API keys, endpoint URLs, project ID's, etc. Use the drop down arrow to enable languages of choice: there are a total of 56 supported LLM languages. Users can also modify which NeuralSeek functions to enable for the added LLM. Note that you must add at least one LLM. If you add multiple, NeuralSeek will load-balance across them for the selected functions that have multiple LLM's. Features that an LLM are not capable of will be unselectable. If you do not provide an LLM for a function, there is no fallback and that function of NeuralSeek will be disabled. This section is only available if you are using BYOLLM (bring your own LLM) plan of NeuralSeek . Company/Organization Preferences: This is where you can enter your company name, and description of what the company primarily focuses on. Note that the description is also used as a stump speech which is a block of text to be used to help generate a response when all else fails, and the answer needs to fall back to it. Users can also select whether to add or not add Company Response Affinity which adds affinity to the company on top of any affinity that may be already present in your KnowledgeBase and Stump Speeches. Platform Preferences: This is where you can set up your primary default language (e.g. English), whether the response would contain the embedded link into returned responses, a list of custom stopwords if you want to override the NeuralSeek default stopwords, and also a virtual agent type that NeuralSeek can generate its questions and answers to. See our supported Virtual Agent platforms page . Intent Categorization: This is where you can create type of categories and their descriptions to control how some of the intents in user question can be categorized into. Usually a question would be automatically categorized as FAQ , but you can provide additional ones here. Governance & Guardrails: Users are able to adjust the following: Attribute Checking: Adjust misinformation tolerance for generating text about the company, or associating people or things that lack specific references in the KnowledgeBase material by using the sliding scale from \"Rigid\" to \"Standard\". The more rigid your setting, the higher the chances of occasionally blocking legitimate questions that use alternate wording or are poorly documented in your KnowledgeBase. Semantic Score: The Semantic Score model checks the generated answer against the KnowledgeBase sources, and rates the answer based on the quantity and focus of the KnowledgeBase and Stump Speech source documentation (e.g. is the answer primarily formed from a single source or many?) Toggle the icons to enable or disable the Semantic Score Model, using Semantic Score as the basis for Warning & Minimum confidence (e.g. Do NOT Enable for usecases requiring language translation), reranking the search results based on the Semantic Match, and checking the document titles and URL's as part of the Semantic Match. Note that semantic scoring will not be accurate when getting answers in a diferent language than your KnowledgeBase source docs. Semantic Model Tuning: use the sliding scales to further tune the Semantic Match. Missing key search term penalty: After scoring, this penalty is applied for answers that are missing KnowledgeBase attribution of proper nouns that were included in the search. Missing search term penalty: After scoring, this penalty is applied for answers that are missing KnowledgeBase attribution of other nouns that were included in the search. Source Jump penalty: When anwers join across many source documents it can be an indication of lost meaning or intent, depending on your source documentation. Total Coverage weight: Looking at the answer, how much weight should be given to the total coverage alone, regardless of other penalty. Increasing this helps prevent abnormally low scores from long, highly-stitched answers. Decreasing will better catch hallucination in short answers. ReRanK Min Coverage %: What is the minimum coverage of the total answer that the top used source document needs to be reranked over the top KB-scored document. Profanity Filter: Users are able to toggle the icon to enable or disable the profanity filter, as well as add a text to reply with for sensitive questions that are blocked. (e.g. \"That seems like a sensitive question. Maybe I'm not understanding you, so try rephrasing.\"). Warning Confidence: Use the sliding scale to increase the confidence percentage for warning. Add a text to prepend a warning on low confidence results. (e.g. \"I'm not an expert in this, but...\"). Minimum Confidence: Use the sliding scale to increase the minimum confidence percentage and the minimum confidence percentage to display a URL. Add a text to reply with for questions not meeting the minimum confidence, and select whether to add a URL fallback on minimum. (e.g. \"There is nothing in our knowledge base about that.\"). Minimum Text: Use the sliding scale to set a desired minimum amount of words in a question. Add a text to reply with for questions not meeting the minimum input text length. (e.g. \"Give me a bit more to go on...\"). Maximum Length: Use the sliding scale to set a desired maximum amount of words in a question. Use a low limit to help mitigate adversarial questions designed to generate inappropriate answers. Set to 100 to remove the limit. Add a text to reply with for questions over the input word limit. (e.g. \"Can you please summarize your question for me? Questions should be limited to 20 words.\").","title":"Default Configurations"},{"location":"user_interface/configure/configure/#advanced-configurations","text":"KnowledgeBase Tuning: Tuning your Knowledgebase is an important part of creating a well performing system. Start by entering a seek query on the Seek tab to review the answer provided, as well as look at the documentation in the accordions below the answer. If the answer is not suitable for your needs, first check if the top document is correct and complete. If not, adjust snippet size by using the slider bars to improve KB training. Secondly, check if your answers are bringing back more irrelevant documentation than you need. If so, use the slider bars to set a max documents per seek or use the sliding scale to set a lower document score upper range. Further, you can adjust the document date penalty to ensure relevancy, and adjust the cache timeout in minutes but using the KnowledgeBase Query Cache (minutes) sliding scale. Prompt Engineering: Prompt Engineering allows expert users to inject specific instructions into the LLM prompt. Most use cases will not need this and should not use this. (e.g. do not enter \"provide factual information\" or \"act as a helpful customer support agent\".) NeuralSeek's extensive prompting already does this. Do not casually enable, as you can weaken the safeguards and extensive prompting that NeuralSeek provides out-of-the-box. Do not use any language other than English in prompt engineering. Answer Engineering & Preferences: Users are able to use the sliding scale to set whether the answer generation would stick to being concise, or can have more freedom to be flexible. Toggle the icon to True or False to force answers from the Knowledgebase if desired. Also, as part of answer engineering, you can provide regular expression and replacement pair so that certain pattern of data or sensitive information can be replace or removed as necessary. Answer Engineering uses Javascript Regular Expressions to selectivley replace text in both the KnowledgeBase training data and the live generated answer. (e.g. Use this to remove or swap phone numbers, emails, etc.). Intent Matching & Cache Configuration: Here is where you can configure strategies to perform intent matching. The following types of intent match's are available: Exact Match, Vector Similarity, Fuzzy Match, Keyword Match, and Fuzzy Keyword Match. Users can also configure how the answer caching is to be done for edited answers, and normal answers. You can control the number of answers that would trigger the cache, as well as their individual matching methods by using the sliding scales. Toggle the icon to Yes or No if desired to require the cache to follow context as well. Table Understanding: Table understanding pre-processes your documents to extract and parse tabular data into a format suitable for conversational query. Since this preparation process is both costly and time consuming, this feature is opt-in and will consume 1 seek query for every table preprocessed. Web Crawl Collections are not eligible for table understanding, as the re-crawl interval will cause excessive compute usage. Table preparation time takes several minutes per page. Please contact cloud@cerebralblue.com with details of your opportunity and use case to be considered for access. Note that Table Understanding requires a compatible LLM with Table Understanding Enabled. Not all LLM's are capable of Table Understanding. Personal Identifiable Information (PII) Handling: Users can define how to handle any detected PII data that was included in the question. The following options are available: Mask, Flag, No Action, Hide (retain for Analytics), and Delete (including from Analytics). The configuration also lets you add any particular examples of PII data, so it can be better detected, or set as No PII so that it can be ignored. Corporate Document Filter: Connect NeuralSeek to an external corporate rules engine to filter allowed documentation by user. Each request will send the id's of the found documentation to a endpoint you set here. Any ID's not returned back from the corporate filter will be blocked. Toggle the icon to Enable to Disable this feature. If enabled, fill out all relevant information including: Base URL for the corporate filter (get), URL paramenter for the UserName, URL paramenter for the KB field, and the Knowledgebase field to send. Corporate Logging: Connect NeuralSeek to a corporate audit logging endpoint. When connected and enabled, all requests and responses to the Seek api endpoint, as well as the Curate tab will be logged to your elasticSearch instance. Toggle the icon to Enable to Disable this feature. If enabled, fill out all relevant information including: ElasticSearch Endpoint and ElasticSearch API Key.","title":"Advanced Configurations"},{"location":"user_interface/configure/configure/#advanced-setting-options","text":"Download Settings: Click this to download a .dat copy of all settings in the configure tab to your local machine. Upload Settings: Click this to upload and restore settings from a .dat copy backup of all settings in the configure tab from your local machine. Change Logs: Click to view the change log history of all settings changed in this instance for auditing and debugging purposes.","title":"Advanced Setting Options"},{"location":"user_interface/curate/curate/","text":"Overview NeuralSeek's \"Curate\" features allows users to view intents generated from the KnowledgeBase, import and export intents into the assistant, and manage example questions and answers. The intent content and parameters can be adapted and adjusted to accommodate employee and customer needs. Users can also view the results of other features as well, such as round trip logging, merge/unmerge actions, whether the intent contains any Personally Identifiable Information (P.I.I.), and whether the source knowledge base information has changed so that user can easily detect whether the answer that were generated needs to be updated or not. Sometimes, it is easier to curate all the questions and answers outside of NeuralSeek, and upload them in batch. Use the \"Curate\" feature to upload and update the curated Q&A (supports CSV format). A template CSV file is given for you to use it. See Curation of Answers for more info.","title":"Curate"},{"location":"user_interface/curate/curate/#overview","text":"NeuralSeek's \"Curate\" features allows users to view intents generated from the KnowledgeBase, import and export intents into the assistant, and manage example questions and answers. The intent content and parameters can be adapted and adjusted to accommodate employee and customer needs. Users can also view the results of other features as well, such as round trip logging, merge/unmerge actions, whether the intent contains any Personally Identifiable Information (P.I.I.), and whether the source knowledge base information has changed so that user can easily detect whether the answer that were generated needs to be updated or not. Sometimes, it is easier to curate all the questions and answers outside of NeuralSeek, and upload them in batch. Use the \"Curate\" feature to upload and update the curated Q&A (supports CSV format). A template CSV file is given for you to use it. See Curation of Answers for more info.","title":"Overview"},{"location":"user_interface/explore/explore/","text":"Overview Introducing \"Explore\" - an open-ended retrieval augmented generation playground designed to enhance your experience with Large Language Models (LLMs). Explore is a practical tool that provides you with the following capabilities: Choice of LLM : Select your preferred LLM, and seamlessly integrate it with Explore.[^1] [^1]: \u2755 Disclaimer: This feature is for users on a Bring You Own Large Language Model plan (BYOLLM). Utilize NeuralSeek Template Language (NTL) : Craft dynamic prompts using a combination of regular words and NTL markup to retrieve content from different sources. User-Friendly Visual Editor : Create custom prompts with an easy-to-use point-and-click visual editor, without the use of NTL markup. Utilize NeuralSeek's Seek Feature : Seek a query through the Explore platform and generate an answer based on a corporate KnowledgeBase content. Versatile Content Retrieval : Retrieve data from various sources, including Knowledge Bases, websites, local files (PDFs, Docs, CSVs, XLS, TXT), or input your own text. Content Enhancement : Improve your data with features like summarization, stopwords removal, keyword extraction, and PII removal to ensure your content is refined and valuable. Guided Prompts : Explore provides guidance on LLM prompt syntax and optimal weights to achieve the best results. Table Understanding : Conduct searches and generate answers to non-SQL queries based on structured data such as a CSV or Excel file. Effortless Output : Easily view your generated content within the built-in editor or export it directly to a Word document, offering convenient control over your output. Precision Semantic Scoring : Importantly, all these operations are assessed using our Semantic Scoring model. This allows you to measure content generation based on your specific needs, providing insight into the content's scope tailored to your preferences. For more information, see Explore Platform .","title":"Explore"},{"location":"user_interface/explore/explore/#overview","text":"Introducing \"Explore\" - an open-ended retrieval augmented generation playground designed to enhance your experience with Large Language Models (LLMs). Explore is a practical tool that provides you with the following capabilities: Choice of LLM : Select your preferred LLM, and seamlessly integrate it with Explore.[^1] [^1]: \u2755 Disclaimer: This feature is for users on a Bring You Own Large Language Model plan (BYOLLM). Utilize NeuralSeek Template Language (NTL) : Craft dynamic prompts using a combination of regular words and NTL markup to retrieve content from different sources. User-Friendly Visual Editor : Create custom prompts with an easy-to-use point-and-click visual editor, without the use of NTL markup. Utilize NeuralSeek's Seek Feature : Seek a query through the Explore platform and generate an answer based on a corporate KnowledgeBase content. Versatile Content Retrieval : Retrieve data from various sources, including Knowledge Bases, websites, local files (PDFs, Docs, CSVs, XLS, TXT), or input your own text. Content Enhancement : Improve your data with features like summarization, stopwords removal, keyword extraction, and PII removal to ensure your content is refined and valuable. Guided Prompts : Explore provides guidance on LLM prompt syntax and optimal weights to achieve the best results. Table Understanding : Conduct searches and generate answers to non-SQL queries based on structured data such as a CSV or Excel file. Effortless Output : Easily view your generated content within the built-in editor or export it directly to a Word document, offering convenient control over your output. Precision Semantic Scoring : Importantly, all these operations are assessed using our Semantic Scoring model. This allows you to measure content generation based on your specific needs, providing insight into the content's scope tailored to your preferences. For more information, see Explore Platform .","title":"Overview"},{"location":"user_interface/extract/extract/","text":"Overview Extract lets user extract detected entities found inside a user provided text. The interface would let user enter texts, and from there it will automatically try to extract found entities and provide its list. You can also add, update, or delete any number of custom entities that you may want to specify certain entities better or create a new type of entity. For more information, see entity extraction .","title":"Extract"},{"location":"user_interface/extract/extract/#overview","text":"Extract lets user extract detected entities found inside a user provided text. The interface would let user enter texts, and from there it will automatically try to extract found entities and provide its list. You can also add, update, or delete any number of custom entities that you may want to specify certain entities better or create a new type of entity. For more information, see entity extraction .","title":"Overview"},{"location":"user_interface/home/home/","text":"Overview The home page is where users can get started interacting with NeuralSeek and quickly set up a chatbot in a matter of moments (see NeuralSeek onboarding). Basics : User provides general information about their organization with NeuralSeek. Data : Where users connect to their KnowledgeBase (can also be done under the \u201cConfigure\u201d tab). LLM : (only available with Bring Your Own LLM plan) User can select their preferred LLM (Large Language Model) of choice. User is required to enter the appropriate integration information (e.g. API key of their LLM) in order to continue. About: Describing the organization and use case preferences. Tune: Provide information about the documentation/KnowledgeBase. Q&A: Auto-generate a list of actions to set up a virtual agent in minutes. User can also perform the following actions through the home page: Auto-generate questions : Virtual Agents would typically require an adequate number of questions for creating meaningful intentions and dialogs. This process would typically involve having to manually create them. However, NeuralSeek can generate, based on the contents of your KnowledgeBase, a good set of commonly asked questions for you. Manually Input questions : If you already have a good set of questions that your user frequently asks, you can upload them into NeuralSeek and NeuralSeek could categorize and create their answers for you to later review and curate them. Upload Test questions : If you want to test out your set of questions, and validate whether their coverage or confidence score is good enough, or find out how their analytics look like, use this. A template CSV file is given for you to use it.","title":"Home"},{"location":"user_interface/home/home/#overview","text":"The home page is where users can get started interacting with NeuralSeek and quickly set up a chatbot in a matter of moments (see NeuralSeek onboarding). Basics : User provides general information about their organization with NeuralSeek. Data : Where users connect to their KnowledgeBase (can also be done under the \u201cConfigure\u201d tab). LLM : (only available with Bring Your Own LLM plan) User can select their preferred LLM (Large Language Model) of choice. User is required to enter the appropriate integration information (e.g. API key of their LLM) in order to continue. About: Describing the organization and use case preferences. Tune: Provide information about the documentation/KnowledgeBase. Q&A: Auto-generate a list of actions to set up a virtual agent in minutes. User can also perform the following actions through the home page: Auto-generate questions : Virtual Agents would typically require an adequate number of questions for creating meaningful intentions and dialogs. This process would typically involve having to manually create them. However, NeuralSeek can generate, based on the contents of your KnowledgeBase, a good set of commonly asked questions for you. Manually Input questions : If you already have a good set of questions that your user frequently asks, you can upload them into NeuralSeek and NeuralSeek could categorize and create their answers for you to later review and curate them. Upload Test questions : If you want to test out your set of questions, and validate whether their coverage or confidence score is good enough, or find out how their analytics look like, use this. A template CSV file is given for you to use it.","title":"Overview"},{"location":"user_interface/integrate/integrate/","text":"Overview Custom Extension: This contains the information to build a custom NeuralSeek extension within Watson Assistant. LexV2 Lambda: Use the AWS Lambda to send user input that routes to the Lex FallbackIntent to NeuralSeek. Used in conjunction with AWS LexV2. LexV2 Logs: How to enable Round-Trip Logging using LexV2 Logs, to monitor the usage of curated intents. The purpose of round-trip logging is to improve the virtual agent\u2019s performance by analyzing the data and identifying areas for improvement. Watson Logs : How to enable Round-Trip Logging using Watson Logs, to monitor the usage of curated intents. The purpose of round-trip logging is to improve the virtual agent\u2019s performance by analyzing the data and identifying areas for improvement. WebHook : This is the backbone of NeuralSeek, how users connect and communicate with the solution. One can make a call to this WebHook from any applications (e.g. slack, servicenow, etc.) that can foward its question to it and receive answers from. API (REST) : Where to find necessary information regarding how to invoke NeuralSeek\u2019s REST API, and navigate and test it right on its openAPI generated page. You can get examples of JSON message requests and responses, as well as JSON schema of the message payloads. KoreAI : Activate Round-Trip monitoring for deployed NeuralSeek Intents. This feature enables NeuralSeek to continuously monitor the usage of its curated intents through KoreAI event Tasks. It will promptly alert you if any curated intents require updates due to changes in the associated KnowledgeBase documents. Console API : This integration allows users to access debugging and monitoring features conveniently from within the NeuralSeek application, simplifying tasks such as error identification, performance analysis, and data insights without the need to switch between different tools or interfaces. It enhances the user experience by providing seamless access to the Console API's functionality within NeuralSeek's interface, streamlining development and monitoring tasks","title":"Integrate"},{"location":"user_interface/integrate/integrate/#overview","text":"Custom Extension: This contains the information to build a custom NeuralSeek extension within Watson Assistant. LexV2 Lambda: Use the AWS Lambda to send user input that routes to the Lex FallbackIntent to NeuralSeek. Used in conjunction with AWS LexV2. LexV2 Logs: How to enable Round-Trip Logging using LexV2 Logs, to monitor the usage of curated intents. The purpose of round-trip logging is to improve the virtual agent\u2019s performance by analyzing the data and identifying areas for improvement. Watson Logs : How to enable Round-Trip Logging using Watson Logs, to monitor the usage of curated intents. The purpose of round-trip logging is to improve the virtual agent\u2019s performance by analyzing the data and identifying areas for improvement. WebHook : This is the backbone of NeuralSeek, how users connect and communicate with the solution. One can make a call to this WebHook from any applications (e.g. slack, servicenow, etc.) that can foward its question to it and receive answers from. API (REST) : Where to find necessary information regarding how to invoke NeuralSeek\u2019s REST API, and navigate and test it right on its openAPI generated page. You can get examples of JSON message requests and responses, as well as JSON schema of the message payloads. KoreAI : Activate Round-Trip monitoring for deployed NeuralSeek Intents. This feature enables NeuralSeek to continuously monitor the usage of its curated intents through KoreAI event Tasks. It will promptly alert you if any curated intents require updates due to changes in the associated KnowledgeBase documents. Console API : This integration allows users to access debugging and monitoring features conveniently from within the NeuralSeek application, simplifying tasks such as error identification, performance analysis, and data insights without the need to switch between different tools or interfaces. It enhances the user experience by providing seamless access to the Console API's functionality within NeuralSeek's interface, streamlining development and monitoring tasks","title":"Overview"},{"location":"user_interface/logs/logs/","text":"Overview In NeuralSeek, users can access the usage log generated from interactions with the \"Seek\" feature through the \"Logs\" tab. This feature allows users to efficiently filter their log history by date, session ID, question, and answer for a more streamlined and informative experience.","title":"Logs"},{"location":"user_interface/logs/logs/#overview","text":"In NeuralSeek, users can access the usage log generated from interactions with the \"Seek\" feature through the \"Logs\" tab. This feature allows users to efficiently filter their log history by date, session ID, question, and answer for a more streamlined and informative experience.","title":"Overview"},{"location":"user_interface/seek/seek/","text":"Overview NeuralSeek\u2019s \"Seek\" feature enables users to test questions and generate answers using content from their connected KnowledgeBase. To ensure transparency between the sources and answers, NeuralSeek highlights where the answers are coming from within the KnowledgeBase. Semantic match scores are employed to compare the generated responses with the original documentation, providing a clear understanding of the alignment between the response and the meaning conveyed in source documents. This process ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek. Features User ID : Users are able to view and set a User ID to test conversations. Session ID : A unique \"Session ID\" number is generated. Users are able to revert to a new session with a unique \"Session ID\" number by clicking the red arrow next to \"Session Turns\". Session Turns : A \"Session Turns\" number is generated, which allow the user to view how many turns were generated in the corresponding Session ID. Highlight Answer Provenance : Enabling this feature reveals how the majority of responses are formed by the trained answer and additional components that came in from the KnowledgeBase itself. Users are able to enable or disable. Answer Streaming : Streaming is available to enable or disable. Enabling this feature allows for the reponse to be generated word-by-word. Disabling this feature allows for the whole response to be generated at once. Information Output | Total Response Time | This number indicates the total amount of time for a response to generate in seconds. | | Semantic Match % | This percentage is the overall match score that indicates how much NeuralSeek believes that the responses are well aligned with the underlying ground truth from the KnowledgeBase. The higher the percentage is, the more accurate and relevant the answer is based on the truth. | Semantic Analysis | A summary describing why NeuralSeek calculated the matching score in an easy-to-understand syntax. This gives users a good understanding why the answer was given either a high or low score. | KnowledgeBase Confidence % | This percentage indicates how confident the KnowledgeBase thinks the retrieved sources are related to the given question. | KnowledgeBase Coverage % | This percentage indicates how much coverage the KnowledgeBase thinks the retrieved sources are related to the given question. | KnowledgeBase Response Time | This number indicates the amount of time for the KnowledgeBase to generate a response in seconds. | KnowledgeBase Results | This number indicates the amount of retrieved sources the KnowledgeBase thinks are related to the given question. Other Uses Users are able to provide feedback on answers by clicking the \"Thumbs Up\" or \"Thumbs Down\" icons. Users can personalize and filter through their KnowledgeBase documents on the Seek tab as well. Users are able to see what an answer would have been when using the \"minimum confidence\" icon on low semantic match scored Seek's. For more information, see Semantic Analytics .","title":"Seek"},{"location":"user_interface/seek/seek/#overview","text":"NeuralSeek\u2019s \"Seek\" feature enables users to test questions and generate answers using content from their connected KnowledgeBase. To ensure transparency between the sources and answers, NeuralSeek highlights where the answers are coming from within the KnowledgeBase. Semantic match scores are employed to compare the generated responses with the original documentation, providing a clear understanding of the alignment between the response and the meaning conveyed in source documents. This process ensures accuracy and instills confidence in the reliability of the responses generated by NeuralSeek.","title":"Overview"},{"location":"user_interface/seek/seek/#features","text":"User ID : Users are able to view and set a User ID to test conversations. Session ID : A unique \"Session ID\" number is generated. Users are able to revert to a new session with a unique \"Session ID\" number by clicking the red arrow next to \"Session Turns\". Session Turns : A \"Session Turns\" number is generated, which allow the user to view how many turns were generated in the corresponding Session ID. Highlight Answer Provenance : Enabling this feature reveals how the majority of responses are formed by the trained answer and additional components that came in from the KnowledgeBase itself. Users are able to enable or disable. Answer Streaming : Streaming is available to enable or disable. Enabling this feature allows for the reponse to be generated word-by-word. Disabling this feature allows for the whole response to be generated at once.","title":"Features"},{"location":"user_interface/seek/seek/#information-output","text":"| Total Response Time | This number indicates the total amount of time for a response to generate in seconds. | | Semantic Match % | This percentage is the overall match score that indicates how much NeuralSeek believes that the responses are well aligned with the underlying ground truth from the KnowledgeBase. The higher the percentage is, the more accurate and relevant the answer is based on the truth. | Semantic Analysis | A summary describing why NeuralSeek calculated the matching score in an easy-to-understand syntax. This gives users a good understanding why the answer was given either a high or low score. | KnowledgeBase Confidence % | This percentage indicates how confident the KnowledgeBase thinks the retrieved sources are related to the given question. | KnowledgeBase Coverage % | This percentage indicates how much coverage the KnowledgeBase thinks the retrieved sources are related to the given question. | KnowledgeBase Response Time | This number indicates the amount of time for the KnowledgeBase to generate a response in seconds. | KnowledgeBase Results | This number indicates the amount of retrieved sources the KnowledgeBase thinks are related to the given question.","title":"Information Output"},{"location":"user_interface/seek/seek/#other-uses","text":"Users are able to provide feedback on answers by clicking the \"Thumbs Up\" or \"Thumbs Down\" icons. Users can personalize and filter through their KnowledgeBase documents on the Seek tab as well. Users are able to see what an answer would have been when using the \"minimum confidence\" icon on low semantic match scored Seek's. For more information, see Semantic Analytics .","title":"Other Uses"}]}